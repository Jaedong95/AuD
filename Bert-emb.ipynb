{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3fc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from attrdict import AttrDict\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c008d70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\AuD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9480baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, 'data')\n",
    "base_model = os.path.join(default_path, 'base-model')\n",
    "model_path = os.path.join(default_path, 'models')\n",
    "config_path = os.path.join(default_path, 'config')\n",
    "log_path = os.path.join(default_path, 'log')\n",
    "config_file = \"bert-base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467eb122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50gph3</td>\n",
       "      <td>every little insult even if it's online just h...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_wfhxs</td>\n",
       "      <td>do you know why you're feeling depressed, or i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58580</td>\n",
       "      <td>So I'm just gonna live in the countryside</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  label\n",
       "0    50gph3  every little insult even if it's online just h...      8\n",
       "1  t3_wfhxs  do you know why you're feeling depressed, or i...      0\n",
       "2     58580          So I'm just gonna live in the countryside      9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm_samp = pd.read_csv(os.path.join(data_path, 'dsm_samp_test.csv'))\n",
    "dsm_samp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7fd8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611a2a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19869"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsm_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2406f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path, 'training_config.json')) as f:\n",
    "    training_config = AttrDict(json.load(f))\n",
    "\n",
    "training_config.pad = 'max_length'\n",
    "training_config.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40ee94",
   "metadata": {},
   "source": [
    "### 1. Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037e996",
   "metadata": {},
   "source": [
    "#### 1.1 non-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f597120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at F:\\AuD\\base-model\\bert-small were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(base_model, 'bert-small'), model_max_length=128)\n",
    "config = BertConfig.from_pretrained(os.path.join(base_model, 'bert-small', 'bert_config.json'), output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(os.path.join(base_model, 'bert-small'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df1acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b7adf90",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 512)\n",
       "    (token_type_embeddings): Embedding(2, 512)\n",
       "    (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_emb = []\n",
    "\n",
    "for idx in range(len(dsm_samp)):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text=dsm_samp.text[idx],  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        max_length = 64,  # maximum length of a sentence\n",
    "        pad_to_max_length=True,  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "    # print(encoded)\n",
    "    input_ids = torch.tensor(encoded['input_ids']).to(training_config.device)\n",
    "    attn_mask = torch.tensor(encoded['attention_mask']).to(training_config.device)\n",
    "    token_type_ids = torch.tensor(encoded['token_type_ids']).to(training_config.device)\n",
    "    \n",
    "    outputs = model(input_ids, attn_mask, token_type_ids)\n",
    "    hidden_states = outputs[2]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    dsm_emb.append(list(sentence_embedding.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_X = pd.DataFrame(dsm_emb, columns=range(len(dsm_emb[0])))\n",
    "dsm_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de19c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_y = dsm_samp.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "dsm_X = pca.fit_transform(dsm_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(dsm_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"bright\", 10)\n",
    "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=dsm_y, legend='full', palette=palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565f550",
   "metadata": {},
   "source": [
    "#### 1.2 Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57ef1b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at F:\\AuD\\base-model\\bert-mini were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at F:\\AuD\\base-model\\bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(base_model, 'bert-mini'), model_max_length=128)\n",
    "config = BertConfig.from_pretrained(os.path.join(base_model, 'bert-mini', 'bert_config.json'), num_labels=10, output_hidden_states=True)\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(base_model, 'bert-mini'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d11d779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c94e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.path.join(model_path, 'DSM-5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b92bef59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(model_name))\n",
    "model.load_state_dict(torch.load(model_name, map_location=torch.device('cpu')))\n",
    "model.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d97e8652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "<ipython-input-27-1a4eef9ba610>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(encoded['input_ids']).to(training_config.device)\n",
      "<ipython-input-27-1a4eef9ba610>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn_mask = torch.tensor(encoded['attention_mask']).to(training_config.device)\n",
      "<ipython-input-27-1a4eef9ba610>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  token_type_ids = torch.tensor(encoded['token_type_ids']).to(training_config.device)\n"
     ]
    }
   ],
   "source": [
    "dsm_emb = []\n",
    "\n",
    "for idx in range(len(dsm_samp)):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text=dsm_samp.text[idx],  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        max_length = 128,  # maximum length of a sentence\n",
    "        pad_to_max_length=True,  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "    input_ids = torch.tensor(encoded['input_ids']).to(training_config.device)\n",
    "    attn_mask = torch.tensor(encoded['attention_mask']).to(training_config.device)\n",
    "    token_type_ids = torch.tensor(encoded['token_type_ids']).to(training_config.device)\n",
    "    \n",
    "    outputs = model(input_ids, attn_mask, token_type_ids)\n",
    "    # last_hidden_state = outputs.last_hidden_state\n",
    "    # print(len(last_hidden_state), len(last_hidden_state[0]), last_hidden_state[0])\n",
    "    hidden_states = outputs[1]\n",
    "    # print(len(hidden_states), len(hidden_states[0]), len(hidden_states[0][0]))\n",
    "    # print(hidden_states[-1][0][0])\n",
    "    # token_vecs = hidden_states[-2][0]\n",
    "    # sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    # dsm_emb.append(list(sentence_embedding.detach().cpu().numpy()))\n",
    "    dsm_emb.append(hidden_states[-1][0][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6143b18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.486853</td>\n",
       "      <td>-0.193270</td>\n",
       "      <td>0.151135</td>\n",
       "      <td>0.146182</td>\n",
       "      <td>0.427602</td>\n",
       "      <td>-0.362908</td>\n",
       "      <td>0.400432</td>\n",
       "      <td>0.097074</td>\n",
       "      <td>-0.102784</td>\n",
       "      <td>0.368796</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.179803</td>\n",
       "      <td>-0.065481</td>\n",
       "      <td>-0.909711</td>\n",
       "      <td>0.803173</td>\n",
       "      <td>-0.347903</td>\n",
       "      <td>-0.200656</td>\n",
       "      <td>-0.810690</td>\n",
       "      <td>-1.159843</td>\n",
       "      <td>-0.862530</td>\n",
       "      <td>-1.357067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024803</td>\n",
       "      <td>-0.813630</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>1.542021</td>\n",
       "      <td>0.242508</td>\n",
       "      <td>0.156137</td>\n",
       "      <td>-1.340185</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>0.019987</td>\n",
       "      <td>-1.743095</td>\n",
       "      <td>...</td>\n",
       "      <td>1.029202</td>\n",
       "      <td>-0.774437</td>\n",
       "      <td>-0.238063</td>\n",
       "      <td>-0.627681</td>\n",
       "      <td>1.745501</td>\n",
       "      <td>-0.110484</td>\n",
       "      <td>0.751730</td>\n",
       "      <td>0.960535</td>\n",
       "      <td>-0.767883</td>\n",
       "      <td>0.144055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.411822</td>\n",
       "      <td>1.214387</td>\n",
       "      <td>-0.142876</td>\n",
       "      <td>-0.874122</td>\n",
       "      <td>1.236772</td>\n",
       "      <td>0.329782</td>\n",
       "      <td>-0.971316</td>\n",
       "      <td>-0.770505</td>\n",
       "      <td>-0.271521</td>\n",
       "      <td>1.710381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055863</td>\n",
       "      <td>-1.427106</td>\n",
       "      <td>-0.851285</td>\n",
       "      <td>1.050934</td>\n",
       "      <td>-0.015948</td>\n",
       "      <td>2.187469</td>\n",
       "      <td>-0.181016</td>\n",
       "      <td>-0.389234</td>\n",
       "      <td>-0.190878</td>\n",
       "      <td>-0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.062845</td>\n",
       "      <td>-0.826382</td>\n",
       "      <td>0.527836</td>\n",
       "      <td>1.536701</td>\n",
       "      <td>0.197945</td>\n",
       "      <td>0.121790</td>\n",
       "      <td>-1.343516</td>\n",
       "      <td>-0.041023</td>\n",
       "      <td>0.025586</td>\n",
       "      <td>-1.695547</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019377</td>\n",
       "      <td>-0.819648</td>\n",
       "      <td>-0.175562</td>\n",
       "      <td>-0.640132</td>\n",
       "      <td>1.820292</td>\n",
       "      <td>-0.040953</td>\n",
       "      <td>0.630163</td>\n",
       "      <td>1.051598</td>\n",
       "      <td>-0.888924</td>\n",
       "      <td>0.124368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037301</td>\n",
       "      <td>-0.790814</td>\n",
       "      <td>0.619905</td>\n",
       "      <td>1.597858</td>\n",
       "      <td>0.248374</td>\n",
       "      <td>0.074239</td>\n",
       "      <td>-1.391596</td>\n",
       "      <td>-0.084606</td>\n",
       "      <td>-0.075381</td>\n",
       "      <td>-1.832390</td>\n",
       "      <td>...</td>\n",
       "      <td>1.064606</td>\n",
       "      <td>-0.738650</td>\n",
       "      <td>-0.155709</td>\n",
       "      <td>-0.703536</td>\n",
       "      <td>1.506873</td>\n",
       "      <td>-0.068177</td>\n",
       "      <td>0.690584</td>\n",
       "      <td>0.992289</td>\n",
       "      <td>-0.767494</td>\n",
       "      <td>0.192331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19864</th>\n",
       "      <td>-1.448124</td>\n",
       "      <td>-0.345054</td>\n",
       "      <td>0.128609</td>\n",
       "      <td>0.329477</td>\n",
       "      <td>0.312694</td>\n",
       "      <td>-0.542881</td>\n",
       "      <td>0.578909</td>\n",
       "      <td>0.459581</td>\n",
       "      <td>0.058677</td>\n",
       "      <td>0.189265</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.122634</td>\n",
       "      <td>-0.195104</td>\n",
       "      <td>-0.789639</td>\n",
       "      <td>0.713112</td>\n",
       "      <td>-0.216910</td>\n",
       "      <td>-0.328524</td>\n",
       "      <td>-0.948527</td>\n",
       "      <td>-1.070781</td>\n",
       "      <td>-0.685449</td>\n",
       "      <td>-1.133018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19865</th>\n",
       "      <td>0.245643</td>\n",
       "      <td>-0.703774</td>\n",
       "      <td>0.331653</td>\n",
       "      <td>1.529896</td>\n",
       "      <td>0.557994</td>\n",
       "      <td>-0.131982</td>\n",
       "      <td>-1.296736</td>\n",
       "      <td>0.092015</td>\n",
       "      <td>-0.208416</td>\n",
       "      <td>-1.717511</td>\n",
       "      <td>...</td>\n",
       "      <td>1.154158</td>\n",
       "      <td>-0.490438</td>\n",
       "      <td>-0.150028</td>\n",
       "      <td>-0.618349</td>\n",
       "      <td>1.545427</td>\n",
       "      <td>-0.136561</td>\n",
       "      <td>0.735391</td>\n",
       "      <td>0.542333</td>\n",
       "      <td>-0.461790</td>\n",
       "      <td>0.080199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19866</th>\n",
       "      <td>-0.406430</td>\n",
       "      <td>1.418849</td>\n",
       "      <td>-0.025132</td>\n",
       "      <td>-0.802424</td>\n",
       "      <td>1.109272</td>\n",
       "      <td>0.414215</td>\n",
       "      <td>-0.683377</td>\n",
       "      <td>-0.712544</td>\n",
       "      <td>-0.225217</td>\n",
       "      <td>1.743096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086616</td>\n",
       "      <td>-1.464859</td>\n",
       "      <td>-0.903901</td>\n",
       "      <td>1.120344</td>\n",
       "      <td>-0.080405</td>\n",
       "      <td>2.149774</td>\n",
       "      <td>-0.323670</td>\n",
       "      <td>-0.299840</td>\n",
       "      <td>-0.325195</td>\n",
       "      <td>-0.167066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19867</th>\n",
       "      <td>0.105223</td>\n",
       "      <td>-0.802513</td>\n",
       "      <td>0.561354</td>\n",
       "      <td>1.569160</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>0.187522</td>\n",
       "      <td>-1.309656</td>\n",
       "      <td>-0.053082</td>\n",
       "      <td>-0.058957</td>\n",
       "      <td>-1.693745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.066048</td>\n",
       "      <td>-0.775402</td>\n",
       "      <td>-0.232960</td>\n",
       "      <td>-0.596793</td>\n",
       "      <td>1.799102</td>\n",
       "      <td>-0.100361</td>\n",
       "      <td>0.663616</td>\n",
       "      <td>1.068503</td>\n",
       "      <td>-0.854274</td>\n",
       "      <td>0.134337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19868</th>\n",
       "      <td>0.250672</td>\n",
       "      <td>-0.738862</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>1.297927</td>\n",
       "      <td>0.150930</td>\n",
       "      <td>0.189713</td>\n",
       "      <td>-1.485037</td>\n",
       "      <td>0.334913</td>\n",
       "      <td>-0.014500</td>\n",
       "      <td>-1.701741</td>\n",
       "      <td>...</td>\n",
       "      <td>1.135922</td>\n",
       "      <td>-0.396166</td>\n",
       "      <td>-0.098104</td>\n",
       "      <td>-0.920081</td>\n",
       "      <td>1.412104</td>\n",
       "      <td>-0.043530</td>\n",
       "      <td>0.803868</td>\n",
       "      <td>0.724715</td>\n",
       "      <td>-0.398402</td>\n",
       "      <td>0.289202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19869 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -1.486853 -0.193270  0.151135  0.146182  0.427602 -0.362908  0.400432   \n",
       "1      0.024803 -0.813630  0.518797  1.542021  0.242508  0.156137 -1.340185   \n",
       "2     -0.411822  1.214387 -0.142876 -0.874122  1.236772  0.329782 -0.971316   \n",
       "3      0.062845 -0.826382  0.527836  1.536701  0.197945  0.121790 -1.343516   \n",
       "4      0.037301 -0.790814  0.619905  1.597858  0.248374  0.074239 -1.391596   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19864 -1.448124 -0.345054  0.128609  0.329477  0.312694 -0.542881  0.578909   \n",
       "19865  0.245643 -0.703774  0.331653  1.529896  0.557994 -0.131982 -1.296736   \n",
       "19866 -0.406430  1.418849 -0.025132 -0.802424  1.109272  0.414215 -0.683377   \n",
       "19867  0.105223 -0.802513  0.561354  1.569160  0.192706  0.187522 -1.309656   \n",
       "19868  0.250672 -0.738862  0.336842  1.297927  0.150930  0.189713 -1.485037   \n",
       "\n",
       "            7         8         9    ...       246       247       248  \\\n",
       "0      0.097074 -0.102784  0.368796  ... -1.179803 -0.065481 -0.909711   \n",
       "1     -0.003623  0.019987 -1.743095  ...  1.029202 -0.774437 -0.238063   \n",
       "2     -0.770505 -0.271521  1.710381  ...  0.055863 -1.427106 -0.851285   \n",
       "3     -0.041023  0.025586 -1.695547  ...  1.019377 -0.819648 -0.175562   \n",
       "4     -0.084606 -0.075381 -1.832390  ...  1.064606 -0.738650 -0.155709   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19864  0.459581  0.058677  0.189265  ... -1.122634 -0.195104 -0.789639   \n",
       "19865  0.092015 -0.208416 -1.717511  ...  1.154158 -0.490438 -0.150028   \n",
       "19866 -0.712544 -0.225217  1.743096  ...  0.086616 -1.464859 -0.903901   \n",
       "19867 -0.053082 -0.058957 -1.693745  ...  1.066048 -0.775402 -0.232960   \n",
       "19868  0.334913 -0.014500 -1.701741  ...  1.135922 -0.396166 -0.098104   \n",
       "\n",
       "            249       250       251       252       253       254       255  \n",
       "0      0.803173 -0.347903 -0.200656 -0.810690 -1.159843 -0.862530 -1.357067  \n",
       "1     -0.627681  1.745501 -0.110484  0.751730  0.960535 -0.767883  0.144055  \n",
       "2      1.050934 -0.015948  2.187469 -0.181016 -0.389234 -0.190878 -0.217400  \n",
       "3     -0.640132  1.820292 -0.040953  0.630163  1.051598 -0.888924  0.124368  \n",
       "4     -0.703536  1.506873 -0.068177  0.690584  0.992289 -0.767494  0.192331  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19864  0.713112 -0.216910 -0.328524 -0.948527 -1.070781 -0.685449 -1.133018  \n",
       "19865 -0.618349  1.545427 -0.136561  0.735391  0.542333 -0.461790  0.080199  \n",
       "19866  1.120344 -0.080405  2.149774 -0.323670 -0.299840 -0.325195 -0.167066  \n",
       "19867 -0.596793  1.799102 -0.100361  0.663616  1.068503 -0.854274  0.134337  \n",
       "19868 -0.920081  1.412104 -0.043530  0.803868  0.724715 -0.398402  0.289202  \n",
       "\n",
       "[19869 rows x 256 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm_X = pd.DataFrame(dsm_emb, columns=range(len(dsm_emb[0])))\n",
    "dsm_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1eeeba39",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\AuD\\\\data\\\\emb\\\\mini-10-emb.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-5d308ef91fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdsm_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'emb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mini-10-emb.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\AuD\\\\data\\\\emb\\\\mini-10-emb.csv'"
     ]
    }
   ],
   "source": [
    "dsm_X.to_csv(os.path.join(data_path, 'emb', 'mini-10-emb.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c65421",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "dsm_X = pca.fit_transform(dsm_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_y = dsm_samp.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c90201",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(dsm_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'emb', 'tiny-9-embedded.pickle'), 'wb') as f:\n",
    "    pickle.dump(X_embedded, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f496c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'emb', 'tiny-9-embedded.pickle'), 'rb') as f:\n",
    "    X_embedded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02494cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"bright\", 10)\n",
    "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=dsm_y, legend='full', palette=palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda74f8f",
   "metadata": {},
   "source": [
    "### 2. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am depressed'\n",
    "text2 = 'she is angry to me'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode_plus(\n",
    "    text=text2,  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length = 64,  # maximum length of a sentence\n",
    "    pad_to_max_length=True,  # Add [PAD]s\n",
    "    return_attention_mask = True,  # Generate the attention mask\n",
    "    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    ")\n",
    "input_ids = torch.tensor(encoded['input_ids']).to(training_config.device)\n",
    "attn_mask = torch.tensor(encoded['attention_mask']).to(training_config.device)\n",
    "token_type_ids = torch.tensor(encoded['token_type_ids']).to(training_config.device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b643bcf",
   "metadata": {},
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efeffd76",
   "metadata": {},
   "source": [
    "outputs = model(input_ids, attn_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2401ba7a",
   "metadata": {},
   "source": [
    "hidden_states = outputs[1]   # hidden state of text "
   ]
  },
  {
   "cell_type": "raw",
   "id": "02da1452",
   "metadata": {},
   "source": [
    "len(hidden_states), len(hidden_states[-2][0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9511b91",
   "metadata": {},
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e555970",
   "metadata": {},
   "source": [
    "token_vecs = hidden_states[-2][0]\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f977d758",
   "metadata": {},
   "source": [
    "print('Tensor shape for each layer: ', hidden_states1[1].size())\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)   \n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)   # (13, 1, 64, 768) -> (13, 64, 768)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)   # (13, 64, 768)  -> (64, 13, 768)   # 각 토큰 별 hidden states \n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e916113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
