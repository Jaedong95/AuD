{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c9dbf2",
   "metadata": {},
   "source": [
    "### 1. Environment Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e256a",
   "metadata": {},
   "source": [
    "#### 1.1 Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33e218b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from attrdict import AttrDict\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401885e",
   "metadata": {},
   "source": [
    "#### 1.2 Setting Default Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2275e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, 'data')\n",
    "base_model = os.path.join(default_path, 'base-model')\n",
    "config_path = os.path.join(default_path, 'config')\n",
    "log_path = os.path.join(default_path, 'log')\n",
    "config_file = \"bert-base.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f1b19",
   "metadata": {},
   "source": [
    "#### 1.3 Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c10afcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join(data_path, 'bws_score_train.csv'))\n",
    "X_dev = pd.read_csv(os.path.join(data_path, 'bws_score_val.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671f6c4",
   "metadata": {},
   "source": [
    "#### 1.4 Load Pretrained model & tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b046bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at F:\\AuD\\base-model\\bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(base_model, 'bert-tiny'), model_max_length=128)\n",
    "config = BertConfig.from_pretrained(os.path.join(base_model, 'bert-tiny', 'bert_config.json'))\n",
    "model = BertModel.from_pretrained(os.path.join(base_model, 'bert-tiny'), config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ee345",
   "metadata": {},
   "source": [
    "#### 1.5 setting training args & config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e440926",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path, 'training_config.json')) as f:\n",
    "    training_config = AttrDict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "affe617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "234fbee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrDict({'default_path': '../', 'data_path': 'data', 'log_path': 'log', 'model_path': 'model', 'config_path': 'config', 'seed': 42, 'train_batch_size': 8, 'device': 'cpu', 'eval_batch_size': 8, 'num_epochs': 500, 'gradient_accumulation_steps': 1, 'warmup_proportion': 0, 'adam_epsilon': 1e-08, 'learning_rate': 5e-05, 'do_lower_case': False, 'no_cuda': False, 'max_steps': -1, 'logging_steps': 100})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf7ea26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c79e9",
   "metadata": {},
   "source": [
    "### 2. Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e338dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.pad = 'max_length'\n",
    "training_config.num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d9d6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = data_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.label)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return text, label\n",
    "        '''\n",
    "        self.reset_index()\n",
    "        text = self.data.text[idx]\n",
    "        label = self.data.label[idx]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7213136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertProcessor():\n",
    "    def __init__(self, config, training_config, tokenizer, truncation=True):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = config.max_position_embeddings\n",
    "        self.pad = training_config.pad\n",
    "        self.batch_size = training_config.train_batch_size\n",
    "        self.truncation = truncation\n",
    "    \n",
    "    def convert_data(self, data_file):\n",
    "        context2 = None    # single sentence classification\n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(data_file[idx][0], context2) for idx in range(len(data_file))],   # text, \n",
    "            max_length = self.max_len,\n",
    "            padding = self.pad,\n",
    "            truncation = self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        for i in range(len(data_file)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "            try:\n",
    "                inputs['label'] = data_file[i][1] \n",
    "            except:\n",
    "                # print('input label 오류')\n",
    "                inputs['label'] = 0 \n",
    "            features.append(inputs)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        all_labels = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "        return dataset\n",
    "    \n",
    "    def shuffle_data(self, dataset, data_type):\n",
    "        if data_type == 'train':\n",
    "            return RandomSampler(dataset)\n",
    "        elif data_type == 'eval' or data_type == 'test':\n",
    "            return SequentialSampler(dataset)\n",
    "        \n",
    "    def load_data(self, dataset, sampler):\n",
    "        return DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07618eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c513d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.model = model\n",
    "        self.linear = nn.Linear(config.hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.last_hidden_state[:, 0, :]\n",
    "        # print(f'logits: {len(logits)}, {len(logits[0])}')\n",
    "        x = self.linear(logits)\n",
    "        x = self.relu(x)\n",
    "        score = self.out(x)\n",
    "        # print(f'score: {score}')\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8cb7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer():\n",
    "    def __init__(self, config, training_config, model, train_dataloader, eval_dataloader):\n",
    "        self.config = config\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        \n",
    "    def set_seed(self):\n",
    "        random.seed(self.training_config.seed)\n",
    "        np.random.seed(self.training_config.seed)\n",
    "        torch.manual_seed(self.training_config.seed)\n",
    "        if not self.training_config.no_cuda and torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.training_config.seed)\n",
    "    \n",
    "    def train(self):\n",
    "        global_step = 0; nb_eval_steps = 0\n",
    "        train_rmse = []; eval_rmse = []\n",
    "        t_total = len(self.train_dataloader) // self.training_config.gradient_accumulation_steps * self.training_config.num_epochs\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.training_config.learning_rate, eps=self.training_config.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * self.training_config.warmup_proportion), \\\n",
    "                                                    num_training_steps=t_total)\n",
    "        \n",
    "        criterion = RMSELoss\n",
    "        # criterion = nn.MSELoss()\n",
    "        best_loss = 9999 \n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        for epoch in range(int(self.training_config.num_epochs)):\n",
    "            train_loss = 0.0; eval_loss = 0.0 \n",
    "            \n",
    "            for step, batch in enumerate(self.train_dataloader):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.training_config.device) for t in batch)\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                # print(f'output: {type(outputs)}, {outputs.squeeze}')\n",
    "                label = batch[3]\n",
    "                # print(f'label: {label}')\n",
    "                # print(f'output: {outputs}, {outputs.squeeze()}')\n",
    "                loss = criterion(outputs.squeeze(), batch[3].type_as(outputs))\n",
    "                loss.backward()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.training_config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                self.model.zero_grad()\n",
    "            \n",
    "            print(f'epoch: {epoch + 1} done, train_loss: {train_loss / len(self.train_dataloader)}')\n",
    "            train_rmse.append(train_loss / len(self.train_dataloader))\n",
    "\n",
    "            for step2, batch2 in enumerate(self.eval_dataloader):\n",
    "                self.model.eval()\n",
    "                batch2 = tuple(t.to(self.training_config.device) for t in batch2)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inputs = {\n",
    "                        \"input_ids\": batch2[0],\n",
    "                        \"attention_mask\": batch2[1],\n",
    "                        \"token_type_ids\": batch2[2],\n",
    "                    }\n",
    "                    label2 = batch2[3]\n",
    "                    outputs = self.model(**inputs)\n",
    "                    tmp_eval_loss = criterion(outputs.squeeze(), label2.type_as(outputs))\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                    \n",
    "                nb_eval_steps += 1\n",
    "\n",
    "            eval_loss = eval_loss / nb_eval_steps\n",
    "            eval_rmse.append(eval_loss)\n",
    "            if eval_loss < best_loss:\n",
    "                best_loss = eval_loss\n",
    "                es = 0\n",
    "                print(f'save best loss state model & log(epoch {epoch + 1})')\n",
    "                self.save_model(os.path.join(self.training_config.default_path, self.training_config.model_path, f'bert_bws_{epoch}.pt'))\n",
    "            else:\n",
    "                es += 1\n",
    "                print(\"Counter {} of 5\".format(es))\n",
    "\n",
    "            if es > 4:\n",
    "                print(\"Early stopping with best_loss: \", best_loss, \"and val_loss for this epoch: \", eval_loss, \"...\")\n",
    "                self.save_model(os.path.join(self.training_config.default_path, self.training_config.model_path, f'bert_bws_best.pt'))\n",
    "                break\n",
    "        self.save_log(train_rmse, eval_rmse, epoch+1)\n",
    "        return train_rmse, eval_rmse\n",
    "            \n",
    "    def save_log(self, train_mse, eval_mse, epoch):\n",
    "        with open(os.path.join(self.training_config.default_path, self.training_config.log_path, f'train_{epoch}_mse.pickle'), 'wb') as f:\n",
    "            pickle.dump(train_mse, f, pickle.HIGHEST_PROTOCOL)  \n",
    "        \n",
    "        with open(os.path.join(self.training_config.default_path, self.training_config.log_path, f'eval_{epoch}_mse.pickle'), 'wb') as f:\n",
    "            pickle.dump(eval_mse, f, pickle.HIGHEST_PROTOCOL)  \n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        torch.save(self.model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fbc7d",
   "metadata": {},
   "source": [
    "### 3. Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0207c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>translated</th>\n",
       "      <th>cnt</th>\n",
       "      <th>weakest_cnt</th>\n",
       "      <th>strongest_cnt</th>\n",
       "      <th>score</th>\n",
       "      <th>minmax_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>i've been super resistant to the idea that i a...</td>\n",
       "      <td>나는 최근까지 내가 우울하다는 생각에 매우 저항해왔다. 나는 너무 압도당해서 아무것...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>i feel sad but i don't if i am really actually...</td>\n",
       "      <td>나는 슬프지만 내가 정말로 우울한지는 모르겠다</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>i just feel like i don't deserve to say that i...</td>\n",
       "      <td>나는 단지 내가 우울하다고 말할 자격이 없다고 느낀다. 왜냐하면 나는 나에게 그렇게...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>they also don't belive that i am depressed eve...</td>\n",
       "      <td>그들은 또한 내 의사가 몇 달 전에 나를 MDD로 진단했음에도 불구하고 내가 우울하...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>But I think it's chilly now</td>\n",
       "      <td>하지만 지금은 쌀쌀한 것 같아요</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>any comments suggestions if i am depressed wha...</td>\n",
       "      <td>어떤 댓글이든 내가 우울하면 어떻게 해야 하는지 솔직히 모르겠어</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>and it made me angry because i was paying some...</td>\n",
       "      <td>그리고 그것은 내가 누군가에게 돈을 주고 다음에 우울할 때 일어나서 산책하는 것처럼...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>if i feel lonely and isolated and want to find...</td>\n",
       "      <td>만약 내가 외롭고 고립되었다고 느끼고 책에서 위안을 찾고 싶다면 나는 J를 읽는다</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>as i just said i am unhappy regardless of context</td>\n",
       "      <td>내가 방금 말했듯이 나는 문맥에 상관없이 불행하다</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>When I learned how to play the piano, I took a...</td>\n",
       "      <td>피아노를 배울 때 시험을 봤는데 키키의 글 키키를 공부하는 것처럼 싫었다.</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "542   i've been super resistant to the idea that i a...   \n",
       "102   i feel sad but i don't if i am really actually...   \n",
       "637   i just feel like i don't deserve to say that i...   \n",
       "238   they also don't belive that i am depressed eve...   \n",
       "1338                        But I think it's chilly now   \n",
       "...                                                 ...   \n",
       "463   any comments suggestions if i am depressed wha...   \n",
       "632   and it made me angry because i was paying some...   \n",
       "1038  if i feel lonely and isolated and want to find...   \n",
       "1081  as i just said i am unhappy regardless of context   \n",
       "1264  When I learned how to play the piano, I took a...   \n",
       "\n",
       "                                             translated  cnt  weakest_cnt  \\\n",
       "542   나는 최근까지 내가 우울하다는 생각에 매우 저항해왔다. 나는 너무 압도당해서 아무것...    8            0   \n",
       "102                           나는 슬프지만 내가 정말로 우울한지는 모르겠다    8            0   \n",
       "637   나는 단지 내가 우울하다고 말할 자격이 없다고 느낀다. 왜냐하면 나는 나에게 그렇게...    8            0   \n",
       "238   그들은 또한 내 의사가 몇 달 전에 나를 MDD로 진단했음에도 불구하고 내가 우울하...    8            0   \n",
       "1338                                  하지만 지금은 쌀쌀한 것 같아요    8            8   \n",
       "...                                                 ...  ...          ...   \n",
       "463                 어떤 댓글이든 내가 우울하면 어떻게 해야 하는지 솔직히 모르겠어    8            0   \n",
       "632   그리고 그것은 내가 누군가에게 돈을 주고 다음에 우울할 때 일어나서 산책하는 것처럼...    8            0   \n",
       "1038      만약 내가 외롭고 고립되었다고 느끼고 책에서 위안을 찾고 싶다면 나는 J를 읽는다    8            0   \n",
       "1081                        내가 방금 말했듯이 나는 문맥에 상관없이 불행하다    8            0   \n",
       "1264          피아노를 배울 때 시험을 봤는데 키키의 글 키키를 공부하는 것처럼 싫었다.    8            8   \n",
       "\n",
       "      strongest_cnt  score  minmax_score  \n",
       "542               5  0.625        0.8125  \n",
       "102               2  0.250        0.6250  \n",
       "637               1  0.125        0.5625  \n",
       "238               1  0.125        0.5625  \n",
       "1338              0 -1.000        0.0000  \n",
       "...             ...    ...           ...  \n",
       "463               3  0.375        0.6875  \n",
       "632               0  0.000        0.5000  \n",
       "1038              0  0.000        0.5000  \n",
       "1081              6  0.750        0.8750  \n",
       "1264              0 -1.000        0.0000  \n",
       "\n",
       "[1152 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7539a64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10,  9,  0,  8,  1, 15, 12, 14, 16, 11,  3,  7,  6,  2,  4,  5],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d1a95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = BertDataset(X_train)\n",
    "val_file = BertDataset(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bacfeb4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 128)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file), len(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "beaac774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_position_embeddings = 32\n",
    "config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4417358",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_processor = BertProcessor(config, training_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a727f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = bert_processor.convert_data(train_file)\n",
    "val_dataset = bert_processor.convert_data(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01232724",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = bert_processor.shuffle_data(train_dataset, 'train')\n",
    "val_sampler = bert_processor.shuffle_data(val_dataset, 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d4dd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = bert_processor.load_data(train_dataset, train_sampler)\n",
    "val_dataloader = bert_processor.load_data(val_dataset, val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bad48641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15e18f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = BertRegressor(config, model).to(training_config.device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1b972c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trainer = BertTrainer(config, training_config, model_reg, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5b6e9d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 done, train_loss: 8.730203549067179\n",
      "save best loss state model & log(epoch 1)\n",
      "epoch: 2 done, train_loss: 7.522732986344232\n",
      "save best loss state model & log(epoch 2)\n",
      "epoch: 3 done, train_loss: 6.752065645323859\n",
      "save best loss state model & log(epoch 3)\n",
      "epoch: 4 done, train_loss: 6.193689187367757\n",
      "save best loss state model & log(epoch 4)\n",
      "epoch: 5 done, train_loss: 5.542160458034939\n",
      "save best loss state model & log(epoch 5)\n",
      "epoch: 6 done, train_loss: 4.758662270175086\n",
      "save best loss state model & log(epoch 6)\n",
      "epoch: 7 done, train_loss: 4.044101489914788\n",
      "save best loss state model & log(epoch 7)\n",
      "epoch: 8 done, train_loss: 3.4134022262361317\n",
      "save best loss state model & log(epoch 8)\n",
      "epoch: 9 done, train_loss: 2.9464251034789615\n",
      "save best loss state model & log(epoch 9)\n",
      "epoch: 10 done, train_loss: 2.680649767319361\n",
      "save best loss state model & log(epoch 10)\n",
      "epoch: 11 done, train_loss: 2.4707671966817646\n",
      "save best loss state model & log(epoch 11)\n",
      "epoch: 12 done, train_loss: 2.4411909646458096\n",
      "save best loss state model & log(epoch 12)\n",
      "epoch: 13 done, train_loss: 2.392093254460229\n",
      "save best loss state model & log(epoch 13)\n",
      "epoch: 14 done, train_loss: 2.3343455294768014\n",
      "save best loss state model & log(epoch 14)\n",
      "epoch: 15 done, train_loss: 2.2914741006162433\n",
      "save best loss state model & log(epoch 15)\n",
      "epoch: 16 done, train_loss: 2.1978971660137177\n",
      "save best loss state model & log(epoch 16)\n",
      "epoch: 17 done, train_loss: 2.1620991163783603\n",
      "save best loss state model & log(epoch 17)\n",
      "epoch: 18 done, train_loss: 2.1502069234848022\n",
      "save best loss state model & log(epoch 18)\n",
      "epoch: 19 done, train_loss: 2.061085330115424\n",
      "save best loss state model & log(epoch 19)\n",
      "epoch: 20 done, train_loss: 2.0777602394421897\n",
      "save best loss state model & log(epoch 20)\n",
      "epoch: 21 done, train_loss: 2.0145674679014416\n",
      "save best loss state model & log(epoch 21)\n",
      "epoch: 22 done, train_loss: 1.9167665143807728\n",
      "save best loss state model & log(epoch 22)\n",
      "epoch: 23 done, train_loss: 1.9244449105527666\n",
      "save best loss state model & log(epoch 23)\n",
      "epoch: 24 done, train_loss: 1.7545457118087344\n",
      "save best loss state model & log(epoch 24)\n",
      "epoch: 25 done, train_loss: 1.791217264201906\n",
      "save best loss state model & log(epoch 25)\n",
      "epoch: 26 done, train_loss: 1.7429228292571173\n",
      "Counter 1 of 5\n",
      "epoch: 27 done, train_loss: 1.7053400344318814\n",
      "save best loss state model & log(epoch 27)\n",
      "epoch: 28 done, train_loss: 1.6964282393455505\n",
      "save best loss state model & log(epoch 28)\n",
      "epoch: 29 done, train_loss: 1.6031549407376184\n",
      "save best loss state model & log(epoch 29)\n",
      "epoch: 30 done, train_loss: 1.5050986210505168\n",
      "save best loss state model & log(epoch 30)\n",
      "epoch: 31 done, train_loss: 1.5180597371525235\n",
      "save best loss state model & log(epoch 31)\n",
      "epoch: 32 done, train_loss: 1.5392038888401456\n",
      "save best loss state model & log(epoch 32)\n",
      "epoch: 33 done, train_loss: 1.4384040120575163\n",
      "Counter 1 of 5\n",
      "epoch: 34 done, train_loss: 1.4116727047496371\n",
      "save best loss state model & log(epoch 34)\n",
      "epoch: 35 done, train_loss: 1.3669238835573196\n",
      "Counter 1 of 5\n",
      "epoch: 36 done, train_loss: 1.2932784921593137\n",
      "save best loss state model & log(epoch 36)\n",
      "epoch: 37 done, train_loss: 1.265561721391148\n",
      "save best loss state model & log(epoch 37)\n",
      "epoch: 38 done, train_loss: 1.2354350404606924\n",
      "Counter 1 of 5\n",
      "epoch: 39 done, train_loss: 1.2465168601936765\n",
      "save best loss state model & log(epoch 39)\n",
      "epoch: 40 done, train_loss: 1.2331692030032475\n",
      "save best loss state model & log(epoch 40)\n",
      "epoch: 41 done, train_loss: 1.229280584388309\n",
      "save best loss state model & log(epoch 41)\n",
      "epoch: 42 done, train_loss: 1.1239345338609483\n",
      "Counter 1 of 5\n",
      "epoch: 43 done, train_loss: 1.1669878678189383\n",
      "save best loss state model & log(epoch 43)\n",
      "epoch: 44 done, train_loss: 1.0812258687284257\n",
      "save best loss state model & log(epoch 44)\n",
      "epoch: 45 done, train_loss: 1.1171642574999068\n",
      "save best loss state model & log(epoch 45)\n",
      "epoch: 46 done, train_loss: 1.0936804562807083\n",
      "Counter 1 of 5\n",
      "epoch: 47 done, train_loss: 1.0971166888872783\n",
      "save best loss state model & log(epoch 47)\n",
      "epoch: 48 done, train_loss: 1.0636741320292156\n",
      "save best loss state model & log(epoch 48)\n",
      "epoch: 49 done, train_loss: 1.025414953629176\n",
      "save best loss state model & log(epoch 49)\n",
      "epoch: 50 done, train_loss: 1.0179044008255005\n",
      "save best loss state model & log(epoch 50)\n",
      "epoch: 51 done, train_loss: 0.973261715637313\n",
      "save best loss state model & log(epoch 51)\n",
      "epoch: 52 done, train_loss: 0.992485319574674\n",
      "Counter 1 of 5\n",
      "epoch: 53 done, train_loss: 0.9852820518943999\n",
      "save best loss state model & log(epoch 53)\n",
      "epoch: 54 done, train_loss: 0.9795574595530828\n",
      "save best loss state model & log(epoch 54)\n",
      "epoch: 55 done, train_loss: 0.9573268956608243\n",
      "save best loss state model & log(epoch 55)\n",
      "epoch: 56 done, train_loss: 1.0292803280883365\n",
      "Counter 1 of 5\n",
      "epoch: 57 done, train_loss: 0.9431038300196329\n",
      "save best loss state model & log(epoch 57)\n",
      "epoch: 58 done, train_loss: 0.9442304737038083\n",
      "Counter 1 of 5\n",
      "epoch: 59 done, train_loss: 0.970860111216704\n",
      "Counter 2 of 5\n",
      "epoch: 60 done, train_loss: 0.9303584860430824\n",
      "save best loss state model & log(epoch 60)\n",
      "epoch: 61 done, train_loss: 0.8734724173943201\n",
      "save best loss state model & log(epoch 61)\n",
      "epoch: 62 done, train_loss: 0.892101428574986\n",
      "Counter 1 of 5\n",
      "epoch: 63 done, train_loss: 0.9037377503183153\n",
      "save best loss state model & log(epoch 63)\n",
      "epoch: 64 done, train_loss: 0.869901105761528\n",
      "save best loss state model & log(epoch 64)\n",
      "epoch: 65 done, train_loss: 0.8717094096872542\n",
      "Counter 1 of 5\n",
      "epoch: 66 done, train_loss: 0.8859814753135046\n",
      "save best loss state model & log(epoch 66)\n",
      "epoch: 67 done, train_loss: 0.8634946743647257\n",
      "Counter 1 of 5\n",
      "epoch: 68 done, train_loss: 0.8744109521309534\n",
      "save best loss state model & log(epoch 68)\n",
      "epoch: 69 done, train_loss: 0.8538775030109618\n",
      "Counter 1 of 5\n",
      "epoch: 70 done, train_loss: 0.8304954237408109\n",
      "Counter 2 of 5\n",
      "epoch: 71 done, train_loss: 0.8453846557272805\n",
      "save best loss state model & log(epoch 71)\n",
      "epoch: 72 done, train_loss: 0.8227451021472613\n",
      "Counter 1 of 5\n",
      "epoch: 73 done, train_loss: 0.8511419097582499\n",
      "save best loss state model & log(epoch 73)\n",
      "epoch: 74 done, train_loss: 0.7817802859677209\n",
      "save best loss state model & log(epoch 74)\n",
      "epoch: 75 done, train_loss: 0.8062302453650368\n",
      "save best loss state model & log(epoch 75)\n",
      "epoch: 76 done, train_loss: 0.7914536794026693\n",
      "save best loss state model & log(epoch 76)\n",
      "epoch: 77 done, train_loss: 0.780499012933837\n",
      "save best loss state model & log(epoch 77)\n",
      "epoch: 78 done, train_loss: 0.7759721312257979\n",
      "save best loss state model & log(epoch 78)\n",
      "epoch: 79 done, train_loss: 0.7597652359141244\n",
      "save best loss state model & log(epoch 79)\n",
      "epoch: 80 done, train_loss: 0.7574742345346345\n",
      "save best loss state model & log(epoch 80)\n",
      "epoch: 81 done, train_loss: 0.7538632485601637\n",
      "save best loss state model & log(epoch 81)\n",
      "epoch: 82 done, train_loss: 0.7339284320672353\n",
      "save best loss state model & log(epoch 82)\n",
      "epoch: 83 done, train_loss: 0.737619979514016\n",
      "save best loss state model & log(epoch 83)\n",
      "epoch: 84 done, train_loss: 0.7694659431775411\n",
      "Counter 1 of 5\n",
      "epoch: 85 done, train_loss: 0.7439868946870168\n",
      "save best loss state model & log(epoch 85)\n",
      "epoch: 86 done, train_loss: 0.7380971395307117\n",
      "Counter 1 of 5\n",
      "epoch: 87 done, train_loss: 0.7078172788023949\n",
      "save best loss state model & log(epoch 87)\n",
      "epoch: 88 done, train_loss: 0.7344864275720384\n",
      "save best loss state model & log(epoch 88)\n",
      "epoch: 89 done, train_loss: 0.6821786711613337\n",
      "save best loss state model & log(epoch 89)\n",
      "epoch: 90 done, train_loss: 0.7056111751331223\n",
      "save best loss state model & log(epoch 90)\n",
      "epoch: 91 done, train_loss: 0.7031576285759608\n",
      "save best loss state model & log(epoch 91)\n",
      "epoch: 92 done, train_loss: 0.6961911544203758\n",
      "save best loss state model & log(epoch 92)\n",
      "epoch: 93 done, train_loss: 0.7006458052330546\n",
      "save best loss state model & log(epoch 93)\n",
      "epoch: 94 done, train_loss: 0.7695966536800066\n",
      "save best loss state model & log(epoch 94)\n",
      "epoch: 95 done, train_loss: 0.6987821973032422\n",
      "save best loss state model & log(epoch 95)\n",
      "epoch: 96 done, train_loss: 0.6926813862389989\n",
      "save best loss state model & log(epoch 96)\n",
      "epoch: 97 done, train_loss: 0.704484786424372\n",
      "save best loss state model & log(epoch 97)\n",
      "epoch: 98 done, train_loss: 0.6726817074749205\n",
      "save best loss state model & log(epoch 98)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 99 done, train_loss: 0.6764412017332183\n",
      "save best loss state model & log(epoch 99)\n",
      "epoch: 100 done, train_loss: 0.6300304209192594\n",
      "Counter 1 of 5\n",
      "epoch: 101 done, train_loss: 0.6813581710060438\n",
      "save best loss state model & log(epoch 101)\n",
      "epoch: 102 done, train_loss: 0.636998159190019\n",
      "save best loss state model & log(epoch 102)\n",
      "epoch: 103 done, train_loss: 0.6548252147105005\n",
      "save best loss state model & log(epoch 103)\n",
      "epoch: 104 done, train_loss: 0.6984443813562393\n",
      "Counter 1 of 5\n",
      "epoch: 105 done, train_loss: 0.6510748101605309\n",
      "save best loss state model & log(epoch 105)\n",
      "epoch: 106 done, train_loss: 0.6431237492296431\n",
      "save best loss state model & log(epoch 106)\n",
      "epoch: 107 done, train_loss: 0.6486574593517516\n",
      "Counter 1 of 5\n",
      "epoch: 108 done, train_loss: 0.6421058550477028\n",
      "save best loss state model & log(epoch 108)\n",
      "epoch: 109 done, train_loss: 0.6274502260817422\n",
      "save best loss state model & log(epoch 109)\n",
      "epoch: 110 done, train_loss: 0.6622166418366962\n",
      "save best loss state model & log(epoch 110)\n",
      "epoch: 111 done, train_loss: 0.5954509567883279\n",
      "save best loss state model & log(epoch 111)\n",
      "epoch: 112 done, train_loss: 0.6273024843798743\n",
      "save best loss state model & log(epoch 112)\n",
      "epoch: 113 done, train_loss: 0.6161213094989458\n",
      "save best loss state model & log(epoch 113)\n",
      "epoch: 114 done, train_loss: 0.6068684458732605\n",
      "Counter 1 of 5\n",
      "epoch: 115 done, train_loss: 0.6217903147141138\n",
      "save best loss state model & log(epoch 115)\n",
      "epoch: 116 done, train_loss: 0.6115665725535817\n",
      "save best loss state model & log(epoch 116)\n",
      "epoch: 117 done, train_loss: 0.5858764557374848\n",
      "Counter 1 of 5\n",
      "epoch: 118 done, train_loss: 0.6441481047206454\n",
      "save best loss state model & log(epoch 118)\n",
      "epoch: 119 done, train_loss: 0.598087825708919\n",
      "save best loss state model & log(epoch 119)\n",
      "epoch: 120 done, train_loss: 0.5922297090291977\n",
      "Counter 1 of 5\n",
      "epoch: 121 done, train_loss: 0.5958073486884435\n",
      "save best loss state model & log(epoch 121)\n",
      "epoch: 122 done, train_loss: 0.6000811176167594\n",
      "Counter 1 of 5\n",
      "epoch: 123 done, train_loss: 0.5929192958606614\n",
      "Counter 2 of 5\n",
      "epoch: 124 done, train_loss: 0.6524850933088197\n",
      "save best loss state model & log(epoch 124)\n",
      "epoch: 125 done, train_loss: 0.6079423775275549\n",
      "Counter 1 of 5\n",
      "epoch: 126 done, train_loss: 0.575496514638265\n",
      "Counter 2 of 5\n",
      "epoch: 127 done, train_loss: 0.5826625087194972\n",
      "Counter 3 of 5\n",
      "epoch: 128 done, train_loss: 0.5466647255751822\n",
      "save best loss state model & log(epoch 128)\n",
      "epoch: 129 done, train_loss: 0.5615755700402789\n",
      "Counter 1 of 5\n",
      "epoch: 130 done, train_loss: 0.5652126901679568\n",
      "save best loss state model & log(epoch 130)\n",
      "epoch: 131 done, train_loss: 0.5547808955113093\n",
      "save best loss state model & log(epoch 131)\n",
      "epoch: 132 done, train_loss: 0.551796205341816\n",
      "Counter 1 of 5\n",
      "epoch: 133 done, train_loss: 0.5782269636789957\n",
      "save best loss state model & log(epoch 133)\n",
      "epoch: 134 done, train_loss: 0.578894225259622\n",
      "save best loss state model & log(epoch 134)\n",
      "epoch: 135 done, train_loss: 0.5940205181638399\n",
      "Counter 1 of 5\n",
      "epoch: 136 done, train_loss: 0.569989473455482\n",
      "save best loss state model & log(epoch 136)\n",
      "epoch: 137 done, train_loss: 0.5290232780906889\n",
      "save best loss state model & log(epoch 137)\n",
      "epoch: 138 done, train_loss: 0.5704075346390406\n",
      "Counter 1 of 5\n",
      "epoch: 139 done, train_loss: 0.5851035209165679\n",
      "Counter 2 of 5\n",
      "epoch: 140 done, train_loss: 0.5327835919128524\n",
      "Counter 3 of 5\n",
      "epoch: 141 done, train_loss: 0.5654014804297023\n",
      "save best loss state model & log(epoch 141)\n",
      "epoch: 142 done, train_loss: 0.5628974686066309\n",
      "Counter 1 of 5\n",
      "epoch: 143 done, train_loss: 0.5414242934849527\n",
      "save best loss state model & log(epoch 143)\n",
      "epoch: 144 done, train_loss: 0.5124450946847597\n",
      "save best loss state model & log(epoch 144)\n",
      "epoch: 145 done, train_loss: 0.5201540655559964\n",
      "Counter 1 of 5\n",
      "epoch: 146 done, train_loss: 0.5288568826185333\n",
      "save best loss state model & log(epoch 146)\n",
      "epoch: 147 done, train_loss: 0.534243279033237\n",
      "save best loss state model & log(epoch 147)\n",
      "epoch: 148 done, train_loss: 0.5856448320878876\n",
      "save best loss state model & log(epoch 148)\n",
      "epoch: 149 done, train_loss: 0.5059717098871866\n",
      "Counter 1 of 5\n",
      "epoch: 150 done, train_loss: 0.559996147122648\n",
      "save best loss state model & log(epoch 150)\n",
      "epoch: 151 done, train_loss: 0.5303750435511271\n",
      "Counter 1 of 5\n",
      "epoch: 152 done, train_loss: 0.4950246372156673\n",
      "Counter 2 of 5\n",
      "epoch: 153 done, train_loss: 0.5545436027977202\n",
      "save best loss state model & log(epoch 153)\n",
      "epoch: 154 done, train_loss: 0.516363435321384\n",
      "save best loss state model & log(epoch 154)\n",
      "epoch: 155 done, train_loss: 0.5723369982507494\n",
      "save best loss state model & log(epoch 155)\n",
      "epoch: 156 done, train_loss: 0.49104615383678013\n",
      "save best loss state model & log(epoch 156)\n",
      "epoch: 157 done, train_loss: 0.5381638573275672\n",
      "Counter 1 of 5\n",
      "epoch: 158 done, train_loss: 0.5031593806213803\n",
      "Counter 2 of 5\n",
      "epoch: 159 done, train_loss: 0.5353448192278544\n",
      "save best loss state model & log(epoch 159)\n",
      "epoch: 160 done, train_loss: 0.5380833405587409\n",
      "save best loss state model & log(epoch 160)\n",
      "epoch: 161 done, train_loss: 0.5133583247661591\n",
      "Counter 1 of 5\n",
      "epoch: 162 done, train_loss: 0.5113553346859084\n",
      "save best loss state model & log(epoch 162)\n",
      "epoch: 163 done, train_loss: 0.5175817501213815\n",
      "Counter 1 of 5\n",
      "epoch: 164 done, train_loss: 0.5189700689580705\n",
      "Counter 2 of 5\n",
      "epoch: 165 done, train_loss: 0.5318531543016434\n",
      "Counter 3 of 5\n",
      "epoch: 166 done, train_loss: 0.49590512696239686\n",
      "save best loss state model & log(epoch 166)\n",
      "epoch: 167 done, train_loss: 0.5328853378693262\n",
      "Counter 1 of 5\n",
      "epoch: 168 done, train_loss: 0.4795951628022724\n",
      "Counter 2 of 5\n",
      "epoch: 169 done, train_loss: 0.4868960637185309\n",
      "save best loss state model & log(epoch 169)\n",
      "epoch: 170 done, train_loss: 0.49193190617693794\n",
      "save best loss state model & log(epoch 170)\n",
      "epoch: 171 done, train_loss: 0.4857844163974126\n",
      "save best loss state model & log(epoch 171)\n",
      "epoch: 172 done, train_loss: 0.46198636210627025\n",
      "Counter 1 of 5\n",
      "epoch: 173 done, train_loss: 0.5008216400941213\n",
      "save best loss state model & log(epoch 173)\n",
      "epoch: 174 done, train_loss: 0.48585231188270783\n",
      "save best loss state model & log(epoch 174)\n",
      "epoch: 175 done, train_loss: 0.4871538546350267\n",
      "save best loss state model & log(epoch 175)\n",
      "epoch: 176 done, train_loss: 0.48247194704082275\n",
      "save best loss state model & log(epoch 176)\n",
      "epoch: 177 done, train_loss: 0.4758821957641178\n",
      "Counter 1 of 5\n",
      "epoch: 178 done, train_loss: 0.4947074204683304\n",
      "Counter 2 of 5\n",
      "epoch: 179 done, train_loss: 0.496968870361646\n",
      "Counter 3 of 5\n",
      "epoch: 180 done, train_loss: 0.4908686363034778\n",
      "save best loss state model & log(epoch 180)\n",
      "epoch: 181 done, train_loss: 0.5064436710543103\n",
      "Counter 1 of 5\n",
      "epoch: 182 done, train_loss: 0.4797718541489707\n",
      "save best loss state model & log(epoch 182)\n",
      "epoch: 183 done, train_loss: 0.4851042611731423\n",
      "save best loss state model & log(epoch 183)\n",
      "epoch: 184 done, train_loss: 0.48085929205020267\n",
      "save best loss state model & log(epoch 184)\n",
      "epoch: 185 done, train_loss: 0.47159864670700496\n",
      "Counter 1 of 5\n",
      "epoch: 186 done, train_loss: 0.4880084784494506\n",
      "Counter 2 of 5\n",
      "epoch: 187 done, train_loss: 0.4735709743367301\n",
      "save best loss state model & log(epoch 187)\n",
      "epoch: 188 done, train_loss: 0.4963465589616034\n",
      "Counter 1 of 5\n",
      "epoch: 189 done, train_loss: 0.4876607673035728\n",
      "save best loss state model & log(epoch 189)\n",
      "epoch: 190 done, train_loss: 0.4752170749836498\n",
      "Counter 1 of 5\n",
      "epoch: 191 done, train_loss: 0.47016163998179966\n",
      "save best loss state model & log(epoch 191)\n",
      "epoch: 192 done, train_loss: 0.4669373515579436\n",
      "Counter 1 of 5\n",
      "epoch: 193 done, train_loss: 0.46852537327342564\n",
      "save best loss state model & log(epoch 193)\n",
      "epoch: 194 done, train_loss: 0.4419751796457503\n",
      "save best loss state model & log(epoch 194)\n",
      "epoch: 195 done, train_loss: 0.4961048323247168\n",
      "save best loss state model & log(epoch 195)\n",
      "epoch: 196 done, train_loss: 0.47053486274348366\n",
      "Counter 1 of 5\n",
      "epoch: 197 done, train_loss: 0.4687338363793161\n",
      "save best loss state model & log(epoch 197)\n",
      "epoch: 198 done, train_loss: 0.44713328488998944\n",
      "save best loss state model & log(epoch 198)\n",
      "epoch: 199 done, train_loss: 0.43886393308639526\n",
      "Counter 1 of 5\n",
      "epoch: 200 done, train_loss: 0.4820430775483449\n",
      "Counter 2 of 5\n",
      "epoch: 201 done, train_loss: 0.469020856751336\n",
      "Counter 3 of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 202 done, train_loss: 0.457380344470342\n",
      "save best loss state model & log(epoch 202)\n",
      "epoch: 203 done, train_loss: 0.44554927365647423\n",
      "Counter 1 of 5\n",
      "epoch: 204 done, train_loss: 0.4592753541138437\n",
      "Counter 2 of 5\n",
      "epoch: 205 done, train_loss: 0.4511765291293462\n",
      "save best loss state model & log(epoch 205)\n",
      "epoch: 206 done, train_loss: 0.4554980562792884\n",
      "save best loss state model & log(epoch 206)\n",
      "epoch: 207 done, train_loss: 0.4566250716646512\n",
      "save best loss state model & log(epoch 207)\n",
      "epoch: 208 done, train_loss: 0.4588734399941232\n",
      "save best loss state model & log(epoch 208)\n",
      "epoch: 209 done, train_loss: 0.4268709404600991\n",
      "Counter 1 of 5\n",
      "epoch: 210 done, train_loss: 0.4559331031309234\n",
      "save best loss state model & log(epoch 210)\n",
      "epoch: 211 done, train_loss: 0.42838386197884876\n",
      "Counter 1 of 5\n",
      "epoch: 212 done, train_loss: 0.4763445349203216\n",
      "Counter 2 of 5\n",
      "epoch: 213 done, train_loss: 0.4453582829899258\n",
      "save best loss state model & log(epoch 213)\n",
      "epoch: 214 done, train_loss: 0.43203549252616036\n",
      "Counter 1 of 5\n",
      "epoch: 215 done, train_loss: 0.4262330142988099\n",
      "Counter 2 of 5\n",
      "epoch: 216 done, train_loss: 0.44398295051521725\n",
      "Counter 3 of 5\n",
      "epoch: 217 done, train_loss: 0.42630721628665924\n",
      "save best loss state model & log(epoch 217)\n",
      "epoch: 218 done, train_loss: 0.4150235280394554\n",
      "save best loss state model & log(epoch 218)\n",
      "epoch: 219 done, train_loss: 0.45279987156391144\n",
      "save best loss state model & log(epoch 219)\n",
      "epoch: 220 done, train_loss: 0.4263148506482442\n",
      "Counter 1 of 5\n",
      "epoch: 221 done, train_loss: 0.40625421164764297\n",
      "Counter 2 of 5\n",
      "epoch: 222 done, train_loss: 0.43059323231379193\n",
      "Counter 3 of 5\n",
      "epoch: 223 done, train_loss: 0.413414357850949\n",
      "save best loss state model & log(epoch 223)\n",
      "epoch: 224 done, train_loss: 0.4152515795495775\n",
      "save best loss state model & log(epoch 224)\n",
      "epoch: 225 done, train_loss: 0.4462904358903567\n",
      "Counter 1 of 5\n",
      "epoch: 226 done, train_loss: 0.4217801218231519\n",
      "save best loss state model & log(epoch 226)\n",
      "epoch: 227 done, train_loss: 0.4220862214763959\n",
      "Counter 1 of 5\n",
      "epoch: 228 done, train_loss: 0.42658058057228726\n",
      "save best loss state model & log(epoch 228)\n",
      "epoch: 229 done, train_loss: 0.41686341332064736\n",
      "save best loss state model & log(epoch 229)\n",
      "epoch: 230 done, train_loss: 0.4021156852444013\n",
      "save best loss state model & log(epoch 230)\n",
      "epoch: 231 done, train_loss: 0.4127124059531424\n",
      "save best loss state model & log(epoch 231)\n",
      "epoch: 232 done, train_loss: 0.43481801119115615\n",
      "save best loss state model & log(epoch 232)\n",
      "epoch: 233 done, train_loss: 0.41981514501902795\n",
      "save best loss state model & log(epoch 233)\n",
      "epoch: 234 done, train_loss: 0.41608787328004837\n",
      "Counter 1 of 5\n",
      "epoch: 235 done, train_loss: 0.41684412956237793\n",
      "Counter 2 of 5\n",
      "epoch: 236 done, train_loss: 0.44070270160833996\n",
      "save best loss state model & log(epoch 236)\n",
      "epoch: 237 done, train_loss: 0.44681188960870105\n",
      "save best loss state model & log(epoch 237)\n",
      "epoch: 238 done, train_loss: 0.3855852025250594\n",
      "Counter 1 of 5\n",
      "epoch: 239 done, train_loss: 0.40094360295269227\n",
      "Counter 2 of 5\n",
      "epoch: 240 done, train_loss: 0.39418735603491467\n",
      "save best loss state model & log(epoch 240)\n",
      "epoch: 241 done, train_loss: 0.4612601904405488\n",
      "save best loss state model & log(epoch 241)\n",
      "epoch: 242 done, train_loss: 0.401627575357755\n",
      "Counter 1 of 5\n",
      "epoch: 243 done, train_loss: 0.39749787582291496\n",
      "Counter 2 of 5\n",
      "epoch: 244 done, train_loss: 0.40975049055284923\n",
      "save best loss state model & log(epoch 244)\n",
      "epoch: 245 done, train_loss: 0.421914230618212\n",
      "Counter 1 of 5\n",
      "epoch: 246 done, train_loss: 0.4004180481036504\n",
      "Counter 2 of 5\n",
      "epoch: 247 done, train_loss: 0.40367935804857147\n",
      "Counter 3 of 5\n",
      "epoch: 248 done, train_loss: 0.381211433145735\n",
      "Counter 4 of 5\n",
      "epoch: 249 done, train_loss: 0.3923797528776858\n",
      "save best loss state model & log(epoch 249)\n",
      "epoch: 250 done, train_loss: 0.4051384888589382\n",
      "Counter 1 of 5\n",
      "epoch: 251 done, train_loss: 0.3778770731555091\n",
      "Counter 2 of 5\n",
      "epoch: 252 done, train_loss: 0.41543986317184234\n",
      "save best loss state model & log(epoch 252)\n",
      "epoch: 253 done, train_loss: 0.3967982563707564\n",
      "Counter 1 of 5\n",
      "epoch: 254 done, train_loss: 0.3872770696050591\n",
      "Counter 2 of 5\n",
      "epoch: 255 done, train_loss: 0.3746196606920825\n",
      "save best loss state model & log(epoch 255)\n",
      "epoch: 256 done, train_loss: 0.3737946276863416\n",
      "save best loss state model & log(epoch 256)\n",
      "epoch: 257 done, train_loss: 0.3913752105500963\n",
      "Counter 1 of 5\n",
      "epoch: 258 done, train_loss: 0.3835066859092977\n",
      "Counter 2 of 5\n",
      "epoch: 259 done, train_loss: 0.3959372184342808\n",
      "save best loss state model & log(epoch 259)\n",
      "epoch: 260 done, train_loss: 0.4027007731298606\n",
      "save best loss state model & log(epoch 260)\n",
      "epoch: 261 done, train_loss: 0.3815988269117143\n",
      "save best loss state model & log(epoch 261)\n",
      "epoch: 262 done, train_loss: 0.3785129984219869\n",
      "save best loss state model & log(epoch 262)\n",
      "epoch: 263 done, train_loss: 0.3780040095249812\n",
      "Counter 1 of 5\n",
      "epoch: 264 done, train_loss: 0.4138510897755623\n",
      "Counter 2 of 5\n",
      "epoch: 265 done, train_loss: 0.3927331864833832\n",
      "save best loss state model & log(epoch 265)\n",
      "epoch: 266 done, train_loss: 0.38040560649500954\n",
      "save best loss state model & log(epoch 266)\n",
      "epoch: 267 done, train_loss: 0.40134269785549903\n",
      "save best loss state model & log(epoch 267)\n",
      "epoch: 268 done, train_loss: 0.38553227070305085\n",
      "Counter 1 of 5\n",
      "epoch: 269 done, train_loss: 0.39283931959006524\n",
      "save best loss state model & log(epoch 269)\n",
      "epoch: 270 done, train_loss: 0.37978043945299256\n",
      "Counter 1 of 5\n",
      "epoch: 271 done, train_loss: 0.3638443586726983\n",
      "save best loss state model & log(epoch 271)\n",
      "epoch: 272 done, train_loss: 0.3911755395432313\n",
      "save best loss state model & log(epoch 272)\n",
      "epoch: 273 done, train_loss: 0.3558642471002208\n",
      "Counter 1 of 5\n",
      "epoch: 274 done, train_loss: 0.3932324027021726\n",
      "Counter 2 of 5\n",
      "epoch: 275 done, train_loss: 0.41152044137318927\n",
      "Counter 3 of 5\n",
      "epoch: 276 done, train_loss: 0.39253201170100105\n",
      "save best loss state model & log(epoch 276)\n",
      "epoch: 277 done, train_loss: 0.3756161824696594\n",
      "Counter 1 of 5\n",
      "epoch: 278 done, train_loss: 0.3963660117652681\n",
      "save best loss state model & log(epoch 278)\n",
      "epoch: 279 done, train_loss: 0.3704479622344176\n",
      "save best loss state model & log(epoch 279)\n",
      "epoch: 280 done, train_loss: 0.36560038063261246\n",
      "Counter 1 of 5\n",
      "epoch: 281 done, train_loss: 0.3647535526090198\n",
      "save best loss state model & log(epoch 281)\n",
      "epoch: 282 done, train_loss: 0.38736047099033993\n",
      "save best loss state model & log(epoch 282)\n",
      "epoch: 283 done, train_loss: 0.398454745610555\n",
      "Counter 1 of 5\n",
      "epoch: 284 done, train_loss: 0.368573646992445\n",
      "Counter 2 of 5\n",
      "epoch: 285 done, train_loss: 0.3861934261189567\n",
      "save best loss state model & log(epoch 285)\n",
      "epoch: 286 done, train_loss: 0.3934738122754627\n",
      "Counter 1 of 5\n",
      "epoch: 287 done, train_loss: 0.37961769104003906\n",
      "Counter 2 of 5\n",
      "epoch: 288 done, train_loss: 0.40517985075712204\n",
      "Counter 3 of 5\n",
      "epoch: 289 done, train_loss: 0.3947661452823215\n",
      "Counter 4 of 5\n",
      "epoch: 290 done, train_loss: 0.37924469427929985\n",
      "save best loss state model & log(epoch 290)\n",
      "epoch: 291 done, train_loss: 0.39282925551136333\n",
      "Counter 1 of 5\n",
      "epoch: 292 done, train_loss: 0.3894825726747513\n",
      "Counter 2 of 5\n",
      "epoch: 293 done, train_loss: 0.35615065321326256\n",
      "save best loss state model & log(epoch 293)\n",
      "epoch: 294 done, train_loss: 0.37360504145423573\n",
      "Counter 1 of 5\n",
      "epoch: 295 done, train_loss: 0.36359694972634315\n",
      "save best loss state model & log(epoch 295)\n",
      "epoch: 296 done, train_loss: 0.40569911524653435\n",
      "Counter 1 of 5\n",
      "epoch: 297 done, train_loss: 0.37073345937662655\n",
      "Counter 2 of 5\n",
      "epoch: 298 done, train_loss: 0.3904130454692576\n",
      "Counter 3 of 5\n",
      "epoch: 299 done, train_loss: 0.3632754033638371\n",
      "save best loss state model & log(epoch 299)\n",
      "epoch: 300 done, train_loss: 0.34862903671132195\n",
      "Counter 1 of 5\n",
      "epoch: 301 done, train_loss: 0.3760792323284679\n",
      "save best loss state model & log(epoch 301)\n",
      "epoch: 302 done, train_loss: 0.3866896765927474\n",
      "save best loss state model & log(epoch 302)\n",
      "epoch: 303 done, train_loss: 0.3581124097108841\n",
      "save best loss state model & log(epoch 303)\n",
      "epoch: 304 done, train_loss: 0.3622010201215744\n",
      "save best loss state model & log(epoch 304)\n",
      "epoch: 305 done, train_loss: 0.3637876771390438\n",
      "save best loss state model & log(epoch 305)\n",
      "epoch: 306 done, train_loss: 0.3620835691690445\n",
      "Counter 1 of 5\n",
      "epoch: 307 done, train_loss: 0.37447700566715664\n",
      "Counter 2 of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 308 done, train_loss: 0.3538840123348766\n",
      "Counter 3 of 5\n",
      "epoch: 309 done, train_loss: 0.3826797923280133\n",
      "save best loss state model & log(epoch 309)\n",
      "epoch: 310 done, train_loss: 0.3336207614176803\n",
      "save best loss state model & log(epoch 310)\n",
      "epoch: 311 done, train_loss: 0.36390185521708596\n",
      "Counter 1 of 5\n",
      "epoch: 312 done, train_loss: 0.34973593594299424\n",
      "save best loss state model & log(epoch 312)\n",
      "epoch: 313 done, train_loss: 0.3774471580982208\n",
      "Counter 1 of 5\n",
      "epoch: 314 done, train_loss: 0.35882802431782085\n",
      "Counter 2 of 5\n",
      "epoch: 315 done, train_loss: 0.35836147020260495\n",
      "Counter 3 of 5\n",
      "epoch: 316 done, train_loss: 0.35151588833994335\n",
      "save best loss state model & log(epoch 316)\n",
      "epoch: 317 done, train_loss: 0.3854202495680915\n",
      "save best loss state model & log(epoch 317)\n",
      "epoch: 318 done, train_loss: 0.3921714992158943\n",
      "Counter 1 of 5\n",
      "epoch: 319 done, train_loss: 0.36496928830941516\n",
      "Counter 2 of 5\n",
      "epoch: 320 done, train_loss: 0.3385783988568518\n",
      "Counter 3 of 5\n",
      "epoch: 321 done, train_loss: 0.355765536841419\n",
      "save best loss state model & log(epoch 321)\n",
      "epoch: 322 done, train_loss: 0.38539449829194283\n",
      "save best loss state model & log(epoch 322)\n",
      "epoch: 323 done, train_loss: 0.3493947597841422\n",
      "Counter 1 of 5\n",
      "epoch: 324 done, train_loss: 0.3534574579033587\n",
      "Counter 2 of 5\n",
      "epoch: 325 done, train_loss: 0.36942439319358933\n",
      "Counter 3 of 5\n",
      "epoch: 326 done, train_loss: 0.3668499357170529\n",
      "save best loss state model & log(epoch 326)\n",
      "epoch: 327 done, train_loss: 0.3686223025951121\n",
      "save best loss state model & log(epoch 327)\n",
      "epoch: 328 done, train_loss: 0.3568677310314443\n",
      "save best loss state model & log(epoch 328)\n",
      "epoch: 329 done, train_loss: 0.35718706167406505\n",
      "save best loss state model & log(epoch 329)\n",
      "epoch: 330 done, train_loss: 0.36636549565527177\n",
      "save best loss state model & log(epoch 330)\n",
      "epoch: 331 done, train_loss: 0.3565231007006433\n",
      "Counter 1 of 5\n",
      "epoch: 332 done, train_loss: 0.3459116402599547\n",
      "Counter 2 of 5\n",
      "epoch: 333 done, train_loss: 0.33197973171869916\n",
      "save best loss state model & log(epoch 333)\n",
      "epoch: 334 done, train_loss: 0.35537326832612354\n",
      "Counter 1 of 5\n",
      "epoch: 335 done, train_loss: 0.35154767168892753\n",
      "save best loss state model & log(epoch 335)\n",
      "epoch: 336 done, train_loss: 0.3587652374472883\n",
      "Counter 1 of 5\n",
      "epoch: 337 done, train_loss: 0.3475327288938893\n",
      "Counter 2 of 5\n",
      "epoch: 338 done, train_loss: 0.3636704720556736\n",
      "save best loss state model & log(epoch 338)\n",
      "epoch: 339 done, train_loss: 0.3748312368988991\n",
      "Counter 1 of 5\n",
      "epoch: 340 done, train_loss: 0.34706182156999904\n",
      "save best loss state model & log(epoch 340)\n",
      "epoch: 341 done, train_loss: 0.36433947334686917\n",
      "Counter 1 of 5\n",
      "epoch: 342 done, train_loss: 0.3500346408949958\n",
      "save best loss state model & log(epoch 342)\n",
      "epoch: 343 done, train_loss: 0.3220890706612004\n",
      "save best loss state model & log(epoch 343)\n",
      "epoch: 344 done, train_loss: 0.3329894029431873\n",
      "save best loss state model & log(epoch 344)\n",
      "epoch: 345 done, train_loss: 0.37646371374527615\n",
      "save best loss state model & log(epoch 345)\n",
      "epoch: 346 done, train_loss: 0.3531513487299283\n",
      "Counter 1 of 5\n",
      "epoch: 347 done, train_loss: 0.33827899561987984\n",
      "save best loss state model & log(epoch 347)\n",
      "epoch: 348 done, train_loss: 0.34561286121606827\n",
      "save best loss state model & log(epoch 348)\n",
      "epoch: 349 done, train_loss: 0.3553440525299973\n",
      "Counter 1 of 5\n",
      "epoch: 350 done, train_loss: 0.3491639950209194\n",
      "Counter 2 of 5\n",
      "epoch: 351 done, train_loss: 0.3285232215291924\n",
      "Counter 3 of 5\n",
      "epoch: 352 done, train_loss: 0.3309308836857478\n",
      "save best loss state model & log(epoch 352)\n",
      "epoch: 353 done, train_loss: 0.34672991144988274\n",
      "Counter 1 of 5\n",
      "epoch: 354 done, train_loss: 0.3301652425693141\n",
      "save best loss state model & log(epoch 354)\n",
      "epoch: 355 done, train_loss: 0.33093514665961266\n",
      "Counter 1 of 5\n",
      "epoch: 356 done, train_loss: 0.33759304881095886\n",
      "save best loss state model & log(epoch 356)\n",
      "epoch: 357 done, train_loss: 0.3400534734957748\n",
      "save best loss state model & log(epoch 357)\n",
      "epoch: 358 done, train_loss: 0.33402229183250004\n",
      "Counter 1 of 5\n",
      "epoch: 359 done, train_loss: 0.3515343430141608\n",
      "save best loss state model & log(epoch 359)\n",
      "epoch: 360 done, train_loss: 0.33796582329604363\n",
      "Counter 1 of 5\n",
      "epoch: 361 done, train_loss: 0.34027548092934823\n",
      "save best loss state model & log(epoch 361)\n",
      "epoch: 362 done, train_loss: 0.3455866157180733\n",
      "save best loss state model & log(epoch 362)\n",
      "epoch: 363 done, train_loss: 0.3298138446278042\n",
      "Counter 1 of 5\n",
      "epoch: 364 done, train_loss: 0.3313904085920917\n",
      "Counter 2 of 5\n",
      "epoch: 365 done, train_loss: 0.3176015106340249\n",
      "Counter 3 of 5\n",
      "epoch: 366 done, train_loss: 0.3162341850499312\n",
      "Counter 4 of 5\n",
      "epoch: 367 done, train_loss: 0.32649844512343407\n",
      "Counter 5 of 5\n",
      "Early stopping with best_loss:  0.006907077784037722 and val_loss for this epoch:  0.0069556858299213795 ...\n"
     ]
    }
   ],
   "source": [
    "train_mse, eval_mse = bert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "868b31cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bert_trainer.save_model(os.path.join(model_path, 'bert_class.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7dfeb754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = list(range(len(train_mse)))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(list(range(len(train_mse))), train_mse, label='train_rmse')\n",
    "plt.plot(list(range(len(eval_mse))), eval_mse, label='val_rmse')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18b783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
