{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04363d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from attrdict import AttrDict\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertConfig, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1f1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, '../data')\n",
    "base_model = os.path.join(default_path, '../base-model')\n",
    "model_path = os.path.join(default_path, '../models')\n",
    "config_path = os.path.join(default_path, '../config')\n",
    "log_path = os.path.join(default_path, '../log')\n",
    "config_file = \"bert-base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36df50eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '우울',\n",
       " 1: '무기력',\n",
       " 2: '급격한 체중(식욕)변화',\n",
       " 3: '수면장애',\n",
       " 4: '정서불안',\n",
       " 5: '피로',\n",
       " 6: '과도한 죄책감 및 무가치함',\n",
       " 7: '인지기능저하',\n",
       " 8: '자살충동',\n",
       " 9: '일상'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = dict()\n",
    "\n",
    "label[0] = '우울'\n",
    "label[1] = '무기력'\n",
    "label[2] = '급격한 체중(식욕)변화'\n",
    "label[3] = '수면장애'\n",
    "label[4] = '정서불안'\n",
    "label[5] = '피로'\n",
    "label[6] = '과도한 죄책감 및 무가치함'\n",
    "label[7] = '인지기능저하'\n",
    "label[8] = '자살충동'\n",
    "label[9] = '일상'\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13db85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path, 'training_config.json')) as f:\n",
    "    training_config = AttrDict(json.load(f))\n",
    "    \n",
    "training_config.pad = 'max_length'\n",
    "training_config.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a3887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = data_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.label)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return text, label\n",
    "        '''\n",
    "        self.reset_index()\n",
    "        text = self.data.text[idx]\n",
    "        label = self.data.label[idx]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac2af702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertProcessor():\n",
    "    def __init__(self, training_config, tokenizer, truncation=True):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = 128\n",
    "        self.pad = training_config.pad\n",
    "        self.batch_size = training_config.train_batch_size\n",
    "        self.truncation = truncation\n",
    "    \n",
    "    def convert_data(self, data_file):\n",
    "        context2 = None    # single sentence classification\n",
    "\n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(data_file[idx][0], context2) for idx in range(len(data_file))],   # text, \n",
    "            max_length = self.max_len,\n",
    "            padding = self.pad,\n",
    "            truncation = self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        for i in range(len(data_file)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "            try:\n",
    "                inputs['label'] = data_file[i][1] \n",
    "            except:\n",
    "                inputs['label'] = 0 \n",
    "            features.append(inputs)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        all_labels = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "        return dataset\n",
    "    \n",
    "    def convert_sentence(self, sent_list):   # 사용자 입력 문장 1개 -> 입력 형태 변환\n",
    "        context2 = None \n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(sent_list, context2)], max_length=self.max_len, padding=self.pad, truncation=self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        inputs = {k: batch_encoding[k][0] for k in batch_encoding}\n",
    "        inputs['label'] = 0 \n",
    "        features.append(inputs)\n",
    "\n",
    "        input_id = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        input_am = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        input_tts = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        input_lb = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "        dataset = TensorDataset(input_id, input_am, input_tts, input_lb)\n",
    "        return dataset\n",
    "    \n",
    "    def shuffle_data(self, dataset, data_type):\n",
    "        if data_type == 'train':\n",
    "            return RandomSampler(dataset)\n",
    "        elif data_type == 'eval' or data_type == 'test':\n",
    "            return SequentialSampler(dataset)\n",
    "        \n",
    "    def load_data(self, dataset, sampler):\n",
    "        return DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab92923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.model = model\n",
    "        self.linear = nn.Linear(config.hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.linear(logits)\n",
    "        x = self.relu(x)\n",
    "        score = self.out(x)\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fbefff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegTester():\n",
    "    def __init__(self, training_config, model):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "\n",
    "    def get_label(self, test_dataloader, test_type):\n",
    "        '''\n",
    "        test_type: 0  -> Test dataset \n",
    "        test_type: 1  -> Test sentence\n",
    "        '''\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            self.model.eval()   # self 안 붙이면 이상한 Output (BaseModelOutputWithPoolingAndCrossAttentions) 출력 \n",
    "            batch = tuple(t.to(training_config.device) for t in batch)   # args.device: cuda \n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                if test_type == 0:\n",
    "                    preds.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                elif test_type == 1:\n",
    "                    preds.extend(outputs[0].detach().cpu().numpy())            \n",
    "            label = batch[3].detach().cpu().numpy()\n",
    "            labels.extend(label)\n",
    "        return preds, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e90e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClsTester():\n",
    "    def __init__(self, training_config, model):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "\n",
    "    def get_label(self, test_dataloader, test_type):\n",
    "        '''\n",
    "        test_type: 0  -> Test dataset \n",
    "        test_type: 1  -> Test sentence\n",
    "        '''\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            self.model.eval()\n",
    "            batch = tuple(t.to(self.training_config.device) for t in batch)   # args.device: cuda \n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": batch[3]\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                test_loss, logits = outputs[:2] \n",
    "                pred = logits.detach().cpu().numpy()\n",
    "                if test_type == 0:\n",
    "                    preds.extend(np.argmax(pred, axis=1))\n",
    "                elif test_type == 1:\n",
    "                    preds.append(np.argmax(pred))  \n",
    "            label = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            labels.extend(label)\n",
    "        return preds, labels \n",
    "    \n",
    "    def get_f1_score(self, test_dataloader):\n",
    "        y_pred, y_true = self.get_label(test_dataloader)\n",
    "        return round(f1_score(y_true, y_pred, average='micro'), 3) \n",
    "     \n",
    "    def get_cl_report(self, test_dataloader):\n",
    "        y_pred, y_true = self.get_label(test_dataloader, 0)\n",
    "        cr = classification_report(y_true, y_pred).split('\\n')\n",
    "        clr_df = []\n",
    "\n",
    "        for idx, line in enumerate(cr):\n",
    "            clr_df.append([])\n",
    "            if line == '':\n",
    "                continue\n",
    "\n",
    "            word_list = line.strip().split(' ')\n",
    "\n",
    "            for word in word_list:\n",
    "                if word != '':\n",
    "                    clr_df[idx].append(word)\n",
    "\n",
    "        clr_df[-2][0] = ' '.join([clr_df[-2][0], clr_df[-2][1]])\n",
    "        clr_df[-3][0] = ' '.join([clr_df[-3][0], clr_df[-3][1]])\n",
    "        clr_df[-4].insert(1, ' ')\n",
    "        clr_df[-4].insert(2, ' ')\n",
    "        clr_df[0].insert(0, 'index')\n",
    "\n",
    "        clr_df[-2].pop(1)\n",
    "        clr_df[-3].pop(1)\n",
    "        clr_df.pop(1)\n",
    "        clr_df.pop(-1)\n",
    "        clr_df.pop(-4)\n",
    "        clr_df = pd.DataFrame(clr_df[1:], columns=clr_df[0])\n",
    "        clr_df.index = clr_df['index']\n",
    "\n",
    "        del clr_df['index']\n",
    "        return clr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39cde6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at F:\\AuD\\jupyter notebook\\../base-model\\bert-small were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bws_tokenizer = BertTokenizer.from_pretrained(os.path.join(base_model, 'bert-small'), model_max_length=128)\n",
    "bws_config = BertConfig.from_pretrained(os.path.join(base_model, 'bert-small', 'bert_config.json'), output_hidden_states=True, output_attentions=True)\n",
    "bws_model = BertModel.from_pretrained(os.path.join(base_model, 'bert-small'), config=bws_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa11de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bws_config.max_position_embeddings = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807d52aa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 512)\n",
       "    (token_type_embeddings): Embedding(2, 512)\n",
       "    (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bws_model.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5645b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "bws_processor = BertProcessor(training_config, bws_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27da5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bws_reg = BertRegressor(bws_config, bws_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71076fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bws_model_name = os.path.join(model_path, 'BWS.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30e26ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertRegressor(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_reg.load_state_dict(torch.load(model_name))\n",
    "bws_reg.load_state_dict(torch.load(bws_model_name, map_location=torch.device('cpu')))\n",
    "bws_reg.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "567c36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bws_tester = BertRegTester(training_config, bws_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7be1f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at F:\\AuD\\base-model\\bert-mini were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at F:\\AuD\\base-model\\bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dsm_tokenizer = BertTokenizer.from_pretrained(os.path.join(base_model, 'bert-mini'), model_max_length=128)\n",
    "dsm_config = BertConfig.from_pretrained(os.path.join(base_model, 'bert-mini', 'bert_config.json'), num_labels=10, output_hidden_states=True, output_attentions=True)\n",
    "dsm_model = BertForSequenceClassification.from_pretrained(os.path.join(base_model, 'bert-mini'), config=dsm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e651235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_config.max_position_embeddings = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9ccd82d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm_model.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f943e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_processor = BertProcessor(training_config, dsm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6817628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_model_name = os.path.join(model_path, 'DSM-5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4453d7eb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm_model.load_state_dict(torch.load(dsm_model_name, map_location=torch.device('cpu')))\n",
    "dsm_model.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c2bd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_tester = BertClsTester(training_config, dsm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36a6765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_toks(input_text, tokenizer, model, num_words):\n",
    "    input_text = input_text.replace(\"'m\", \" am\").replace('.', ' ').replace(',', ' ')\n",
    "    print(input_text)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt').to(training_config.device)\n",
    "    outputs = model(inputs)  # Run model\n",
    "    attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "    att_metrics = outputs.attentions[-1][0]\n",
    "    att_sum = list(map(sum, att_metrics))\n",
    "    sorted_att = sum(att_sum).sort(descending=True)\n",
    "    \n",
    "    cnt = 0 \n",
    "    tok_idx = []\n",
    "    for idx in range(len(inputs[0])):\n",
    "        if inputs[0][sorted_att.indices[idx]] == 101 or inputs[0][sorted_att.indices[idx]] == 102:\n",
    "            continue\n",
    "        tok_idx.append(sorted_att.indices[idx])\n",
    "        cnt += 1\n",
    "        if cnt == num_words:\n",
    "            break \n",
    "    \n",
    "    tok_list = [tokenizer.decode(inputs[0][int(tok)]) for tok in tok_idx]\n",
    "    return tok_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae281fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"also I lost 30 pounds and I feel lethargic\"\n",
    "input_text = \"I do not feel depressed today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c99314d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bws_data = bws_processor.convert_sentence(input_text)\n",
    "bws_sampler = bws_processor.shuffle_data(bws_data, 'test')\n",
    "bws_loader = bws_processor.load_data(bws_data, bws_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4511ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_data = dsm_processor.convert_sentence(input_text)\n",
    "dsm_sampler = dsm_processor.shuffle_data(dsm_data, 'test')\n",
    "dsm_loader = dsm_processor.load_data(dsm_data, dsm_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c002f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "bws_pred, _ = bws_tester.get_label(bws_loader, 1)\n",
    "dsm_pred, _ = dsm_tester.get_label(dsm_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef573dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not feel depressed today\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f e e l', 'd e p r e s s e d', 'd o']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_att_toks(input_text, bws_tokenizer, bws_model, 3)\n",
    "# get_att_toks(input_text, dsm_tokenizer, dsm_model, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fc0b7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.110906, '우울')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bws_pred[0], label[dsm_pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206605c2",
   "metadata": {},
   "source": [
    "#### CBT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "340e73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbt = pd.read_csv(os.path.join(data_path, 'cbt.csv'), skiprows=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47324629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbt_no</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBT 1-1</td>\n",
       "      <td>안녕! 마음의 방에 온걸 환영해.</td>\n",
       "      <td>안녕</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cbt_no             chatbot user\n",
       "0  CBT 1-1  안녕! 마음의 방에 온걸 환영해.  안녕 "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbt_samp = cbt[['cbt', 'question', 'response (오재동)']]\n",
    "cbt_samp.columns = ['cbt_no', 'chatbot', 'user']\n",
    "cbt_samp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb474244",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4463: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbt_no</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBT 1-1</td>\n",
       "      <td>안녕! 마음의 방에 온걸 환영해.</td>\n",
       "      <td>안녕</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>사람은 여러 가지의 감정을 느껴. 좋은 기분도 싫은 기분도 느끼게 되는데 그것은 자...</td>\n",
       "      <td>자신의 감정을 받아드리는게 중요하구나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>울적한 기분이 들었을 때 그 후에 어떻게 행동할까, 어떻게 생각할까 하는 것은 별개...</td>\n",
       "      <td>그걸 알면서도 가끔은 감정이 행동으로 나도 모르게 이어지는거 같아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>최근에 있었던 기분 좋았던 일이나 기분 나빴던 상황을 떠올려 볼래? 예를 들어, 학...</td>\n",
       "      <td>논문 쓰는걸 드디어 끝냈어 기분 좋아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>방금 연습해 본 것처럼 앞으로 일상에서 어떤 생각을 하고 어떤 감정이 들고 어떤 행...</td>\n",
       "      <td>좋아 앞으로 잘 부탁해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>마음의 방에 와서 새롭게 배운 것들은 뭐가 있어?</td>\n",
       "      <td>내 마음을 좀 더 돌아볼 수 있었던거 같아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NaN</td>\n",
       "      <td>마음의 방에서 배운 것들 중에서 너에게 가장 도움이 되었던 건 뭐야?</td>\n",
       "      <td>부정적인 감정에 좀 더 잘 대처할 수 있을거 같아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>마음의 방에 온 이후에 생긴 삶의 변화는 어떤 것들이 있어?</td>\n",
       "      <td>큰 변화는 없어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>여기 오면서 예전에는 하지 못했거나 오랫동안 하지 않았던 것 중에서 하게 된 것도 있어?</td>\n",
       "      <td>딱히 없는거 같아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>정말 많은 것을 이루었구나! 그동안 열심히 노력해줘서 정말 고마워. 고생 많았어. ...</td>\n",
       "      <td>공백</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cbt_no                                            chatbot  \\\n",
       "0    CBT 1-1                                 안녕! 마음의 방에 온걸 환영해.   \n",
       "1        NaN  사람은 여러 가지의 감정을 느껴. 좋은 기분도 싫은 기분도 느끼게 되는데 그것은 자...   \n",
       "2        NaN  울적한 기분이 들었을 때 그 후에 어떻게 행동할까, 어떻게 생각할까 하는 것은 별개...   \n",
       "3        NaN  최근에 있었던 기분 좋았던 일이나 기분 나빴던 상황을 떠올려 볼래? 예를 들어, 학...   \n",
       "4        NaN  방금 연습해 본 것처럼 앞으로 일상에서 어떤 생각을 하고 어떤 감정이 들고 어떤 행...   \n",
       "..       ...                                                ...   \n",
       "192      NaN                        마음의 방에 와서 새롭게 배운 것들은 뭐가 있어?   \n",
       "193      NaN             마음의 방에서 배운 것들 중에서 너에게 가장 도움이 되었던 건 뭐야?   \n",
       "194      NaN                  마음의 방에 온 이후에 생긴 삶의 변화는 어떤 것들이 있어?   \n",
       "195      NaN  여기 오면서 예전에는 하지 못했거나 오랫동안 하지 않았던 것 중에서 하게 된 것도 있어?   \n",
       "196      NaN  정말 많은 것을 이루었구나! 그동안 열심히 노력해줘서 정말 고마워. 고생 많았어. ...   \n",
       "\n",
       "                                     user  \n",
       "0                                     안녕   \n",
       "1                    자신의 감정을 받아드리는게 중요하구나  \n",
       "2    그걸 알면서도 가끔은 감정이 행동으로 나도 모르게 이어지는거 같아  \n",
       "3                    논문 쓰는걸 드디어 끝냈어 기분 좋아  \n",
       "4                            좋아 앞으로 잘 부탁해  \n",
       "..                                    ...  \n",
       "192               내 마음을 좀 더 돌아볼 수 있었던거 같아  \n",
       "193           부정적인 감정에 좀 더 잘 대처할 수 있을거 같아  \n",
       "194                              큰 변화는 없어  \n",
       "195                             딱히 없는거 같아  \n",
       "196                                    공백  \n",
       "\n",
       "[197 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbt_samp.user.fillna('공백', inplace=True)\n",
    "cbt_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71180645",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbt_samp.to_csv(os.path.join(data_path, 'cbt_samp.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bcdcaf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbt_t1 = pd.read_csv(os.path.join(data_path, 'cbt_samp_t1.csv'))\n",
    "cbt_t2 = pd.read_csv(os.path.join(data_path, 'cbt_samp_t2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfcc2bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbt_no</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>user</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBT 1-1</td>\n",
       "      <td>안녕! 마음의 방에 온걸 환영해.</td>\n",
       "      <td>안녕</td>\n",
       "      <td>Hello! Welcome to the mind room.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>사람은 여러 가지의 감정을 느껴. 좋은 기분도 싫은 기분도 느끼게 되는데 그것은 자...</td>\n",
       "      <td>자신의 감정을 받아드리는게 중요하구나</td>\n",
       "      <td>People feel various emotions. It makes me feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>울적한 기분이 들었을 때 그 후에 어떻게 행동할까, 어떻게 생각할까 하는 것은 별개...</td>\n",
       "      <td>그걸 알면서도 가끔은 감정이 행동으로 나도 모르게 이어지는거 같아</td>\n",
       "      <td>It's another matter of how to act and how to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>최근에 있었던 기분 좋았던 일이나 기분 나빴던 상황을 떠올려 볼래? 예를 들어, 학...</td>\n",
       "      <td>논문 쓰는걸 드디어 끝냈어 기분 좋아</td>\n",
       "      <td>Can you think of something that made you feel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>방금 연습해 본 것처럼 앞으로 일상에서 어떤 생각을 하고 어떤 감정이 들고 어떤 행...</td>\n",
       "      <td>좋아 앞으로 잘 부탁해</td>\n",
       "      <td>As we just practiced, we will find out what th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>마음의 방에 와서 새롭게 배운 것들은 뭐가 있어?</td>\n",
       "      <td>내 마음을 좀 더 돌아볼 수 있었던거 같아</td>\n",
       "      <td>What are some new things you learned in the mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NaN</td>\n",
       "      <td>마음의 방에서 배운 것들 중에서 너에게 가장 도움이 되었던 건 뭐야?</td>\n",
       "      <td>부정적인 감정에 좀 더 잘 대처할 수 있을거 같아</td>\n",
       "      <td>What was the most helpful thing you learned in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>마음의 방에 온 이후에 생긴 삶의 변화는 어떤 것들이 있어?</td>\n",
       "      <td>큰 변화는 없어</td>\n",
       "      <td>What are some changes in your life after comin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>여기 오면서 예전에는 하지 못했거나 오랫동안 하지 않았던 것 중에서 하게 된 것도 있어?</td>\n",
       "      <td>딱히 없는거 같아</td>\n",
       "      <td>Is there anything you haven't done before or h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>정말 많은 것을 이루었구나! 그동안 열심히 노력해줘서 정말 고마워. 고생 많았어. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a lot you've achieved! Thank you so much ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cbt_no                                            chatbot  \\\n",
       "0    CBT 1-1                                 안녕! 마음의 방에 온걸 환영해.   \n",
       "1        NaN  사람은 여러 가지의 감정을 느껴. 좋은 기분도 싫은 기분도 느끼게 되는데 그것은 자...   \n",
       "2        NaN  울적한 기분이 들었을 때 그 후에 어떻게 행동할까, 어떻게 생각할까 하는 것은 별개...   \n",
       "3        NaN  최근에 있었던 기분 좋았던 일이나 기분 나빴던 상황을 떠올려 볼래? 예를 들어, 학...   \n",
       "4        NaN  방금 연습해 본 것처럼 앞으로 일상에서 어떤 생각을 하고 어떤 감정이 들고 어떤 행...   \n",
       "..       ...                                                ...   \n",
       "192      NaN                        마음의 방에 와서 새롭게 배운 것들은 뭐가 있어?   \n",
       "193      NaN             마음의 방에서 배운 것들 중에서 너에게 가장 도움이 되었던 건 뭐야?   \n",
       "194      NaN                  마음의 방에 온 이후에 생긴 삶의 변화는 어떤 것들이 있어?   \n",
       "195      NaN  여기 오면서 예전에는 하지 못했거나 오랫동안 하지 않았던 것 중에서 하게 된 것도 있어?   \n",
       "196      NaN  정말 많은 것을 이루었구나! 그동안 열심히 노력해줘서 정말 고마워. 고생 많았어. ...   \n",
       "\n",
       "                                     user  \\\n",
       "0                                     안녕    \n",
       "1                    자신의 감정을 받아드리는게 중요하구나   \n",
       "2    그걸 알면서도 가끔은 감정이 행동으로 나도 모르게 이어지는거 같아   \n",
       "3                    논문 쓰는걸 드디어 끝냈어 기분 좋아   \n",
       "4                            좋아 앞으로 잘 부탁해   \n",
       "..                                    ...   \n",
       "192               내 마음을 좀 더 돌아볼 수 있었던거 같아   \n",
       "193           부정적인 감정에 좀 더 잘 대처할 수 있을거 같아   \n",
       "194                              큰 변화는 없어   \n",
       "195                             딱히 없는거 같아   \n",
       "196                                   NaN   \n",
       "\n",
       "                                            translated  \n",
       "0                     Hello! Welcome to the mind room.  \n",
       "1    People feel various emotions. It makes me feel...  \n",
       "2    It's another matter of how to act and how to t...  \n",
       "3    Can you think of something that made you feel ...  \n",
       "4    As we just practiced, we will find out what th...  \n",
       "..                                                 ...  \n",
       "192  What are some new things you learned in the mi...  \n",
       "193  What was the most helpful thing you learned in...  \n",
       "194  What are some changes in your life after comin...  \n",
       "195  Is there anything you haven't done before or h...  \n",
       "196  What a lot you've achieved! Thank you so much ...  \n",
       "\n",
       "[197 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbt_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67265671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cbt_no</th>\n",
       "      <th>chatbot</th>\n",
       "      <th>user</th>\n",
       "      <th>chatbot_kor</th>\n",
       "      <th>user_kor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBT 1-1</td>\n",
       "      <td>안녕! 마음의 방에 온걸 환영해.</td>\n",
       "      <td>안녕</td>\n",
       "      <td>Hello! Welcome to the mind room.</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cbt_no             chatbot user                       chatbot_kor user_kor\n",
       "0  CBT 1-1  안녕! 마음의 방에 온걸 환영해.  안녕   Hello! Welcome to the mind room.      Hi."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbt_samp['chatbot_kor'] = cbt_t1.translated\n",
    "cbt_samp['user_kor'] = cbt_t2.translated\n",
    "\n",
    "cbt_samp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f37ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
