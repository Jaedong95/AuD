{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c9dbf2",
   "metadata": {},
   "source": [
    "### 1. Environment Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e256a",
   "metadata": {},
   "source": [
    "#### 1.1 Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e218b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 15:21:03.957111: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-18 15:21:04.143359: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-18 15:21:04.181233: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-18 15:21:04.944467: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-18 15:21:04.944580: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-18 15:21:04.944589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import pymysql\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from attrdict import AttrDict\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401885e",
   "metadata": {},
   "source": [
    "#### 1.2 Setting Default Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2275e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/lamda_00/Depression_paper/data/\"\n",
    "model_path = \"/home/lamda_00/Depression_paper/model/\"\n",
    "ckpt_path = \"/home/lamda_00/Depression_paper/ckpt/\"\n",
    "config_path = \"/home/lamda_00/Depression_paper/config/\"\n",
    "log_path = \"/home/lamda_00/Depression_paper/log/\"\n",
    "config_file = \"bert-base.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f1b19",
   "metadata": {},
   "source": [
    "#### 1.3 Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10afcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>translated</th>\n",
       "      <th>cnt</th>\n",
       "      <th>weakest_cnt</th>\n",
       "      <th>strongest_cnt</th>\n",
       "      <th>score</th>\n",
       "      <th>minmax_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am sad for both of them and i hope something...</td>\n",
       "      <td>나는 그들 모두에게 슬프고 나는 무언가가 더 나은 방향으로 바뀌기를 바란다</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it feels like all i ever achieve through what ...</td>\n",
       "      <td>불행함을 느끼지 않기 위해 내가 힘든 노력으로 느끼는 것을 통해 내가 성취하는 모든...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so yet again i lose the person that i tell eve...</td>\n",
       "      <td>그래서 다시 나는 내가 모든 것을 말할 수 있는 사람을 잃고 우울할 때 기분이 나아...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  i am sad for both of them and i hope something...   \n",
       "1  it feels like all i ever achieve through what ...   \n",
       "2  so yet again i lose the person that i tell eve...   \n",
       "\n",
       "                                          translated  cnt  weakest_cnt  \\\n",
       "0          나는 그들 모두에게 슬프고 나는 무언가가 더 나은 방향으로 바뀌기를 바란다    8            0   \n",
       "1  불행함을 느끼지 않기 위해 내가 힘든 노력으로 느끼는 것을 통해 내가 성취하는 모든...    8            0   \n",
       "2  그래서 다시 나는 내가 모든 것을 말할 수 있는 사람을 잃고 우울할 때 기분이 나아...    8            0   \n",
       "\n",
       "   strongest_cnt  score  minmax_score  label  \n",
       "0              3  0.375        0.6875     11  \n",
       "1              0  0.000        0.5000      8  \n",
       "2              0  0.000        0.5000      8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bws_score = pd.read_csv(os.path.join(data_path, 'bws_sim_score1.csv'))\n",
    "bws_score.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b3face5",
   "metadata": {},
   "source": [
    "bws_score['label'] = bws_score['minmax_score'] * 16\n",
    "bws_score['label'] = bws_score.label.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88845925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_kor</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am sad for both of them and i hope something...</td>\n",
       "      <td>나는 그들 모두에게 슬프고 나는 무언가가 더 나은 방향으로 바뀌기를 바란다</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  i am sad for both of them and i hope something...   \n",
       "\n",
       "                                    text_kor  label  \n",
       "0  나는 그들 모두에게 슬프고 나는 무언가가 더 나은 방향으로 바뀌기를 바란다     11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bws_score = bws_score[['text', 'translated', 'label']]\n",
    "bws_score.columns = ['text', 'text_kor', 'label']\n",
    "bws_score.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ec1a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 128, 320)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(bws_score, test_size=0.2, random_state=42)\n",
    "X_train, X_dev = train_test_split(X_train, test_size=0.1, random_state=42)\n",
    "len(X_train), len(X_dev), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77448fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join(data_path, 'bws_score_train.csv'), index=False)\n",
    "X_dev.to_csv(os.path.join(data_path, 'bws_score_val.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(data_path, 'bws_score_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671f6c4",
   "metadata": {},
   "source": [
    "#### 1.4 Load Pretrained model & tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b046bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/lamda_00/Depression_paper/model/bert-tiny were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(model_path, 'bert-tiny'), model_max_length=32)\n",
    "config = BertConfig.from_pretrained(os.path.join(model_path, 'bert-tiny', 'bert_config.json'))\n",
    "model = BertModel.from_pretrained(os.path.join(model_path, 'bert-tiny'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "88a0b9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lamda_00/Depression_paper/model/'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "562b48c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 128)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.attention_probs_dropout_prob, config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5350f54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"/home/lamda_00/Depression_paper/model/bert-tiny\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.23.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ee345",
   "metadata": {},
   "source": [
    "#### 1.5 setting training args & config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9e440926",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path, 'training_config.json')) as f:\n",
    "    training_config = AttrDict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "affe617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "234fbee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrDict({'default_path': '../', 'data_path': 'data', 'log_path': 'log', 'model_path': 'model', 'config_path': 'config', 'seed': 42, 'train_batch_size': 32, 'device': device(type='cuda'), 'eval_batch_size': 32, 'num_epochs': 500, 'gradient_accumulation_steps': 1, 'max_grad_norm': 1.0, 'adam_epsilon': 1e-08, 'warmup_proportion': 0, 'learning_rate': 5e-05, 'do_lower_case': False, 'no_cuda': False, 'max_steps': -1, 'logging_steps': 100})"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bdf7ea26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c79e9",
   "metadata": {},
   "source": [
    "### 2. Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2e338dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.pad = 'max_length'\n",
    "training_config.num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5d9d6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = data_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.label)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return text, label\n",
    "        '''\n",
    "        self.reset_index()\n",
    "        text = self.data.text[idx]\n",
    "        label = self.data.label[idx]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7213136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertProcessor():\n",
    "    def __init__(self, config, training_config, tokenizer, truncation=True):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = config.max_position_embeddings\n",
    "        self.pad = training_config.pad\n",
    "        self.batch_size = training_config.train_batch_size\n",
    "        self.truncation = truncation\n",
    "    \n",
    "    def convert_data(self, data_file):\n",
    "        context2 = None    # single sentence classification\n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(data_file[idx][0], context2) for idx in range(len(data_file))],   # text, \n",
    "            max_length = self.max_len,\n",
    "            padding = self.pad,\n",
    "            truncation = self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        for i in range(len(data_file)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "            try:\n",
    "                inputs['label'] = data_file[i][1] \n",
    "            except:\n",
    "                # print('input label 오류')\n",
    "                inputs['label'] = 0 \n",
    "            features.append(inputs)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        all_labels = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "        return dataset\n",
    "    \n",
    "    def shuffle_data(self, dataset, data_type):\n",
    "        if data_type == 'train':\n",
    "            return RandomSampler(dataset)\n",
    "        elif data_type == 'eval' or data_type == 'test':\n",
    "            return SequentialSampler(dataset)\n",
    "        \n",
    "    def load_data(self, dataset, sampler):\n",
    "        return DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdecbfd0",
   "metadata": {},
   "source": [
    "class ElectraRegressor(nn.Module):\n",
    "    '''\n",
    "    다중 분류 학습 시 선언하는 클래스 \n",
    "    '''\n",
    "    def __init__(self, electra, config):\n",
    "        # 부모 생성자 초기화, super().__init__(config) 시 오류 발생 \n",
    "        super(ElectraRegressor, self).__init__() \n",
    "        self.electra = electra\n",
    "        self.layer = nn.Linear(config.hidden_size, 128)\n",
    "        self.regressor = nn.Sequential(nn.Dropout(0.1), nn.Linear(128, 6))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.electra(input_ids=input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids)\n",
    "        logits = outputs.last_hidden_state[:, 0, :]\n",
    "        output = self.layer(logits)\n",
    "        output = self.regressor(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "89a0759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrDict({'default_path': '../', 'data_path': 'data', 'log_path': 'log', 'model_path': 'model', 'config_path': 'config', 'seed': 42, 'train_batch_size': 32, 'device': device(type='cuda'), 'eval_batch_size': 32, 'num_epochs': 500, 'gradient_accumulation_steps': 1, 'max_grad_norm': 1.0, 'adam_epsilon': 1e-08, 'warmup_proportion': 0, 'learning_rate': 5e-05, 'do_lower_case': False, 'no_cuda': False, 'max_steps': -1, 'logging_steps': 100, 'pad': 'max_length'})"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config.num_epochs\n",
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "043726f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "07618eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e3c513d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.model = model\n",
    "        self.linear = nn.Linear(config.hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.last_hidden_state[:, 0, :]\n",
    "        # print(f'logits: {len(logits)}, {len(logits[0])}')\n",
    "        x = self.linear(logits)\n",
    "        x = self.relu(x)\n",
    "        score = self.out(x)\n",
    "        # print(f'score: {score}')\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d8cb7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer():\n",
    "    def __init__(self, config, training_config, model, train_dataloader, eval_dataloader):\n",
    "        self.config = config\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        \n",
    "    def set_seed(self):\n",
    "        random.seed(self.training_config.seed)\n",
    "        np.random.seed(self.training_config.seed)\n",
    "        torch.manual_seed(self.training_config.seed)\n",
    "        if not self.training_config.no_cuda and torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.training_config.seed)\n",
    "    \n",
    "    def train(self):\n",
    "        global_step = 0; nb_eval_steps = 0\n",
    "        train_rmse = []; eval_rmse = []\n",
    "        t_total = len(self.train_dataloader) // self.training_config.gradient_accumulation_steps * self.training_config.num_epochs\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.training_config.learning_rate, eps=self.training_config.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * self.training_config.warmup_proportion), \\\n",
    "                                                    num_training_steps=t_total)\n",
    "        \n",
    "        criterion = RMSELoss\n",
    "        # criterion = nn.MSELoss()\n",
    "        best_loss = 9999 \n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        for epoch in range(int(self.training_config.num_epochs)):\n",
    "            train_loss = 0.0; eval_loss = 0.0 \n",
    "            \n",
    "            for step, batch in enumerate(self.train_dataloader):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.training_config.device) for t in batch)\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                # print(f'output: {type(outputs)}, {outputs.squeeze}')\n",
    "                label = batch[3]\n",
    "                # print(f'label: {label}')\n",
    "                # print(f'output: {outputs}, {outputs.squeeze()}')\n",
    "                loss = criterion(outputs.squeeze(), batch[3].type_as(outputs))\n",
    "                loss.backward()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.training_config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                self.model.zero_grad()\n",
    "            \n",
    "            print(f'epoch: {epoch + 1} done, train_loss: {train_loss / len(self.train_dataloader)}')\n",
    "            train_rmse.append(train_loss / len(self.train_dataloader))\n",
    "\n",
    "            for step2, batch2 in enumerate(self.eval_dataloader):\n",
    "                self.model.eval()\n",
    "                batch2 = tuple(t.to(self.training_config.device) for t in batch2)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inputs = {\n",
    "                        \"input_ids\": batch2[0],\n",
    "                        \"attention_mask\": batch2[1],\n",
    "                        \"token_type_ids\": batch2[2],\n",
    "                    }\n",
    "                    label2 = batch2[3]\n",
    "                    outputs = self.model(**inputs)\n",
    "                    tmp_eval_loss = criterion(outputs.squeeze(), label2.type_as(outputs))\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                    \n",
    "                nb_eval_steps += 1\n",
    "\n",
    "            eval_loss = eval_loss / nb_eval_steps\n",
    "            eval_rmse.append(eval_loss)\n",
    "            if eval_loss < best_loss:\n",
    "                best_loss = eval_loss\n",
    "                es = 0\n",
    "                print(f'save best loss state model & log(epoch {epoch + 1})')\n",
    "                self.save_model(os.path.join(self.training_config.default_path, self.training_config.model_path, f'bert_bws_{epoch}.pt'))\n",
    "            else:\n",
    "                es += 1\n",
    "                print(\"Counter {} of 5\".format(es))\n",
    "\n",
    "            if es > 4:\n",
    "                print(\"Early stopping with best_loss: \", best_loss, \"and val_loss for this epoch: \", eval_loss, \"...\")\n",
    "                self.save_model(os.path.join(self.training_config.default_path, self.training_config.model_path, f'bert_bws_best.pt'))\n",
    "                break\n",
    "        self.save_log(train_rmse, eval_rmse, epoch+1)\n",
    "        return train_rmse, eval_rmse\n",
    "            \n",
    "    def save_log(self, train_mse, eval_mse, epoch):\n",
    "        with open(os.path.join(self.training_config.default_path, self.training_config.log_path, f'train_{epoch}_mse.pickle'), 'wb') as f:\n",
    "            pickle.dump(train_mse, f, pickle.HIGHEST_PROTOCOL)  \n",
    "        \n",
    "        with open(os.path.join(self.training_config.default_path, self.training_config.log_path, f'eval_{epoch}_mse.pickle'), 'wb') as f:\n",
    "            pickle.dump(eval_mse, f, pickle.HIGHEST_PROTOCOL)  \n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        torch.save(self.model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fbc7d",
   "metadata": {},
   "source": [
    "### 3. Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7539a64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10,  9,  0,  8,  1, 15, 12, 14, 16, 11,  3,  7,  6,  2,  4,  5])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5d1a95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = BertDataset(X_train)\n",
    "val_file = BertDataset(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bacfeb4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 128)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file), len(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "beaac774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_position_embeddings = 32\n",
    "config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f4417358",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_processor = BertProcessor(config, training_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a727f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = bert_processor.convert_data(train_file)\n",
    "val_dataset = bert_processor.convert_data(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "01232724",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = bert_processor.shuffle_data(train_dataset, 'train')\n",
    "val_sampler = bert_processor.shuffle_data(val_dataset, 'eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4d4dd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = bert_processor.load_data(train_dataset, train_sampler)\n",
    "val_dataloader = bert_processor.load_data(val_dataset, val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bad48641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 4)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "15e18f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = BertRegressor(config, model).to(training_config.device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a1b972c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trainer = BertTrainer(config, training_config, model_reg, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5b6e9d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 done, train_loss: 8.730203549067179\n",
      "save best loss state model & log(epoch 1)\n",
      "epoch: 2 done, train_loss: 7.522732986344232\n",
      "save best loss state model & log(epoch 2)\n",
      "epoch: 3 done, train_loss: 6.752065645323859\n",
      "save best loss state model & log(epoch 3)\n",
      "epoch: 4 done, train_loss: 6.193689187367757\n",
      "save best loss state model & log(epoch 4)\n",
      "epoch: 5 done, train_loss: 5.542160458034939\n",
      "save best loss state model & log(epoch 5)\n",
      "epoch: 6 done, train_loss: 4.758662270175086\n",
      "save best loss state model & log(epoch 6)\n",
      "epoch: 7 done, train_loss: 4.044101489914788\n",
      "save best loss state model & log(epoch 7)\n",
      "epoch: 8 done, train_loss: 3.4134022262361317\n",
      "save best loss state model & log(epoch 8)\n",
      "epoch: 9 done, train_loss: 2.9464251034789615\n",
      "save best loss state model & log(epoch 9)\n",
      "epoch: 10 done, train_loss: 2.680649767319361\n",
      "save best loss state model & log(epoch 10)\n",
      "epoch: 11 done, train_loss: 2.4707671966817646\n",
      "save best loss state model & log(epoch 11)\n",
      "epoch: 12 done, train_loss: 2.4411909646458096\n",
      "save best loss state model & log(epoch 12)\n",
      "epoch: 13 done, train_loss: 2.392093254460229\n",
      "save best loss state model & log(epoch 13)\n",
      "epoch: 14 done, train_loss: 2.3343455294768014\n",
      "save best loss state model & log(epoch 14)\n",
      "epoch: 15 done, train_loss: 2.2914741006162433\n",
      "save best loss state model & log(epoch 15)\n",
      "epoch: 16 done, train_loss: 2.1978971660137177\n",
      "save best loss state model & log(epoch 16)\n",
      "epoch: 17 done, train_loss: 2.1620991163783603\n",
      "save best loss state model & log(epoch 17)\n",
      "epoch: 18 done, train_loss: 2.1502069234848022\n",
      "save best loss state model & log(epoch 18)\n",
      "epoch: 19 done, train_loss: 2.061085330115424\n",
      "save best loss state model & log(epoch 19)\n",
      "epoch: 20 done, train_loss: 2.0777602394421897\n",
      "save best loss state model & log(epoch 20)\n",
      "epoch: 21 done, train_loss: 2.0145674679014416\n",
      "save best loss state model & log(epoch 21)\n",
      "epoch: 22 done, train_loss: 1.9167665143807728\n",
      "save best loss state model & log(epoch 22)\n",
      "epoch: 23 done, train_loss: 1.9244449105527666\n",
      "save best loss state model & log(epoch 23)\n",
      "epoch: 24 done, train_loss: 1.7545457118087344\n",
      "save best loss state model & log(epoch 24)\n",
      "epoch: 25 done, train_loss: 1.791217264201906\n",
      "save best loss state model & log(epoch 25)\n",
      "epoch: 26 done, train_loss: 1.7429228292571173\n",
      "Counter 1 of 5\n",
      "epoch: 27 done, train_loss: 1.7053400344318814\n",
      "save best loss state model & log(epoch 27)\n",
      "epoch: 28 done, train_loss: 1.6964282393455505\n",
      "save best loss state model & log(epoch 28)\n",
      "epoch: 29 done, train_loss: 1.6031549407376184\n",
      "save best loss state model & log(epoch 29)\n",
      "epoch: 30 done, train_loss: 1.5050986210505168\n",
      "save best loss state model & log(epoch 30)\n",
      "epoch: 31 done, train_loss: 1.5180597371525235\n",
      "save best loss state model & log(epoch 31)\n",
      "epoch: 32 done, train_loss: 1.5392038888401456\n",
      "save best loss state model & log(epoch 32)\n",
      "epoch: 33 done, train_loss: 1.4384040120575163\n",
      "Counter 1 of 5\n",
      "epoch: 34 done, train_loss: 1.4116727047496371\n",
      "save best loss state model & log(epoch 34)\n",
      "epoch: 35 done, train_loss: 1.3669238835573196\n",
      "Counter 1 of 5\n",
      "epoch: 36 done, train_loss: 1.2932784921593137\n",
      "save best loss state model & log(epoch 36)\n",
      "epoch: 37 done, train_loss: 1.265561721391148\n",
      "save best loss state model & log(epoch 37)\n",
      "epoch: 38 done, train_loss: 1.2354350404606924\n",
      "Counter 1 of 5\n",
      "epoch: 39 done, train_loss: 1.2465168601936765\n",
      "save best loss state model & log(epoch 39)\n",
      "epoch: 40 done, train_loss: 1.2331692030032475\n",
      "save best loss state model & log(epoch 40)\n",
      "epoch: 41 done, train_loss: 1.229280584388309\n",
      "save best loss state model & log(epoch 41)\n",
      "epoch: 42 done, train_loss: 1.1239345338609483\n",
      "Counter 1 of 5\n",
      "epoch: 43 done, train_loss: 1.1669878678189383\n",
      "save best loss state model & log(epoch 43)\n",
      "epoch: 44 done, train_loss: 1.0812258687284257\n",
      "save best loss state model & log(epoch 44)\n",
      "epoch: 45 done, train_loss: 1.1171642574999068\n",
      "save best loss state model & log(epoch 45)\n",
      "epoch: 46 done, train_loss: 1.0936804562807083\n",
      "Counter 1 of 5\n",
      "epoch: 47 done, train_loss: 1.0971166888872783\n",
      "save best loss state model & log(epoch 47)\n",
      "epoch: 48 done, train_loss: 1.0636741320292156\n",
      "save best loss state model & log(epoch 48)\n",
      "epoch: 49 done, train_loss: 1.025414953629176\n",
      "save best loss state model & log(epoch 49)\n",
      "epoch: 50 done, train_loss: 1.0179044008255005\n",
      "save best loss state model & log(epoch 50)\n",
      "epoch: 51 done, train_loss: 0.973261715637313\n",
      "save best loss state model & log(epoch 51)\n",
      "epoch: 52 done, train_loss: 0.992485319574674\n",
      "Counter 1 of 5\n",
      "epoch: 53 done, train_loss: 0.9852820518943999\n",
      "save best loss state model & log(epoch 53)\n",
      "epoch: 54 done, train_loss: 0.9795574595530828\n",
      "save best loss state model & log(epoch 54)\n",
      "epoch: 55 done, train_loss: 0.9573268956608243\n",
      "save best loss state model & log(epoch 55)\n",
      "epoch: 56 done, train_loss: 1.0292803280883365\n",
      "Counter 1 of 5\n",
      "epoch: 57 done, train_loss: 0.9431038300196329\n",
      "save best loss state model & log(epoch 57)\n",
      "epoch: 58 done, train_loss: 0.9442304737038083\n",
      "Counter 1 of 5\n",
      "epoch: 59 done, train_loss: 0.970860111216704\n",
      "Counter 2 of 5\n",
      "epoch: 60 done, train_loss: 0.9303584860430824\n",
      "save best loss state model & log(epoch 60)\n",
      "epoch: 61 done, train_loss: 0.8734724173943201\n",
      "save best loss state model & log(epoch 61)\n",
      "epoch: 62 done, train_loss: 0.892101428574986\n",
      "Counter 1 of 5\n",
      "epoch: 63 done, train_loss: 0.9037377503183153\n",
      "save best loss state model & log(epoch 63)\n",
      "epoch: 64 done, train_loss: 0.869901105761528\n",
      "save best loss state model & log(epoch 64)\n",
      "epoch: 65 done, train_loss: 0.8717094096872542\n",
      "Counter 1 of 5\n",
      "epoch: 66 done, train_loss: 0.8859814753135046\n",
      "save best loss state model & log(epoch 66)\n",
      "epoch: 67 done, train_loss: 0.8634946743647257\n",
      "Counter 1 of 5\n",
      "epoch: 68 done, train_loss: 0.8744109521309534\n",
      "save best loss state model & log(epoch 68)\n",
      "epoch: 69 done, train_loss: 0.8538775030109618\n",
      "Counter 1 of 5\n",
      "epoch: 70 done, train_loss: 0.8304954237408109\n",
      "Counter 2 of 5\n",
      "epoch: 71 done, train_loss: 0.8453846557272805\n",
      "save best loss state model & log(epoch 71)\n",
      "epoch: 72 done, train_loss: 0.8227451021472613\n",
      "Counter 1 of 5\n",
      "epoch: 73 done, train_loss: 0.8511419097582499\n",
      "save best loss state model & log(epoch 73)\n",
      "epoch: 74 done, train_loss: 0.7817802859677209\n",
      "save best loss state model & log(epoch 74)\n",
      "epoch: 75 done, train_loss: 0.8062302453650368\n",
      "save best loss state model & log(epoch 75)\n",
      "epoch: 76 done, train_loss: 0.7914536794026693\n",
      "save best loss state model & log(epoch 76)\n",
      "epoch: 77 done, train_loss: 0.780499012933837\n",
      "save best loss state model & log(epoch 77)\n",
      "epoch: 78 done, train_loss: 0.7759721312257979\n",
      "save best loss state model & log(epoch 78)\n",
      "epoch: 79 done, train_loss: 0.7597652359141244\n",
      "save best loss state model & log(epoch 79)\n",
      "epoch: 80 done, train_loss: 0.7574742345346345\n",
      "save best loss state model & log(epoch 80)\n",
      "epoch: 81 done, train_loss: 0.7538632485601637\n",
      "save best loss state model & log(epoch 81)\n",
      "epoch: 82 done, train_loss: 0.7339284320672353\n",
      "save best loss state model & log(epoch 82)\n",
      "epoch: 83 done, train_loss: 0.737619979514016\n",
      "save best loss state model & log(epoch 83)\n",
      "epoch: 84 done, train_loss: 0.7694659431775411\n",
      "Counter 1 of 5\n",
      "epoch: 85 done, train_loss: 0.7439868946870168\n",
      "save best loss state model & log(epoch 85)\n",
      "epoch: 86 done, train_loss: 0.7380971395307117\n",
      "Counter 1 of 5\n",
      "epoch: 87 done, train_loss: 0.7078172788023949\n",
      "save best loss state model & log(epoch 87)\n",
      "epoch: 88 done, train_loss: 0.7344864275720384\n",
      "save best loss state model & log(epoch 88)\n",
      "epoch: 89 done, train_loss: 0.6821786711613337\n",
      "save best loss state model & log(epoch 89)\n",
      "epoch: 90 done, train_loss: 0.7056111751331223\n",
      "save best loss state model & log(epoch 90)\n",
      "epoch: 91 done, train_loss: 0.7031576285759608\n",
      "save best loss state model & log(epoch 91)\n",
      "epoch: 92 done, train_loss: 0.6961911544203758\n",
      "save best loss state model & log(epoch 92)\n",
      "epoch: 93 done, train_loss: 0.7006458052330546\n",
      "save best loss state model & log(epoch 93)\n",
      "epoch: 94 done, train_loss: 0.7695966536800066\n",
      "save best loss state model & log(epoch 94)\n",
      "epoch: 95 done, train_loss: 0.6987821973032422\n",
      "save best loss state model & log(epoch 95)\n",
      "epoch: 96 done, train_loss: 0.6926813862389989\n",
      "save best loss state model & log(epoch 96)\n",
      "epoch: 97 done, train_loss: 0.704484786424372\n",
      "save best loss state model & log(epoch 97)\n",
      "epoch: 98 done, train_loss: 0.6726817074749205\n",
      "save best loss state model & log(epoch 98)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 99 done, train_loss: 0.6764412017332183\n",
      "save best loss state model & log(epoch 99)\n",
      "epoch: 100 done, train_loss: 0.6300304209192594\n",
      "Counter 1 of 5\n",
      "epoch: 101 done, train_loss: 0.6813581710060438\n",
      "save best loss state model & log(epoch 101)\n",
      "epoch: 102 done, train_loss: 0.636998159190019\n",
      "save best loss state model & log(epoch 102)\n",
      "epoch: 103 done, train_loss: 0.6548252147105005\n",
      "save best loss state model & log(epoch 103)\n",
      "epoch: 104 done, train_loss: 0.6984443813562393\n",
      "Counter 1 of 5\n",
      "epoch: 105 done, train_loss: 0.6510748101605309\n",
      "save best loss state model & log(epoch 105)\n",
      "epoch: 106 done, train_loss: 0.6431237492296431\n",
      "save best loss state model & log(epoch 106)\n",
      "epoch: 107 done, train_loss: 0.6486574593517516\n",
      "Counter 1 of 5\n",
      "epoch: 108 done, train_loss: 0.6421058550477028\n",
      "save best loss state model & log(epoch 108)\n",
      "epoch: 109 done, train_loss: 0.6274502260817422\n",
      "save best loss state model & log(epoch 109)\n",
      "epoch: 110 done, train_loss: 0.6622166418366962\n",
      "save best loss state model & log(epoch 110)\n",
      "epoch: 111 done, train_loss: 0.5954509567883279\n",
      "save best loss state model & log(epoch 111)\n",
      "epoch: 112 done, train_loss: 0.6273024843798743\n",
      "save best loss state model & log(epoch 112)\n",
      "epoch: 113 done, train_loss: 0.6161213094989458\n",
      "save best loss state model & log(epoch 113)\n",
      "epoch: 114 done, train_loss: 0.6068684458732605\n",
      "Counter 1 of 5\n",
      "epoch: 115 done, train_loss: 0.6217903147141138\n",
      "save best loss state model & log(epoch 115)\n",
      "epoch: 116 done, train_loss: 0.6115665725535817\n",
      "save best loss state model & log(epoch 116)\n",
      "epoch: 117 done, train_loss: 0.5858764557374848\n",
      "Counter 1 of 5\n",
      "epoch: 118 done, train_loss: 0.6441481047206454\n",
      "save best loss state model & log(epoch 118)\n",
      "epoch: 119 done, train_loss: 0.598087825708919\n",
      "save best loss state model & log(epoch 119)\n",
      "epoch: 120 done, train_loss: 0.5922297090291977\n",
      "Counter 1 of 5\n",
      "epoch: 121 done, train_loss: 0.5958073486884435\n",
      "save best loss state model & log(epoch 121)\n",
      "epoch: 122 done, train_loss: 0.6000811176167594\n",
      "Counter 1 of 5\n",
      "epoch: 123 done, train_loss: 0.5929192958606614\n",
      "Counter 2 of 5\n",
      "epoch: 124 done, train_loss: 0.6524850933088197\n",
      "save best loss state model & log(epoch 124)\n",
      "epoch: 125 done, train_loss: 0.6079423775275549\n",
      "Counter 1 of 5\n",
      "epoch: 126 done, train_loss: 0.575496514638265\n",
      "Counter 2 of 5\n",
      "epoch: 127 done, train_loss: 0.5826625087194972\n",
      "Counter 3 of 5\n",
      "epoch: 128 done, train_loss: 0.5466647255751822\n",
      "save best loss state model & log(epoch 128)\n",
      "epoch: 129 done, train_loss: 0.5615755700402789\n",
      "Counter 1 of 5\n",
      "epoch: 130 done, train_loss: 0.5652126901679568\n",
      "save best loss state model & log(epoch 130)\n",
      "epoch: 131 done, train_loss: 0.5547808955113093\n",
      "save best loss state model & log(epoch 131)\n",
      "epoch: 132 done, train_loss: 0.551796205341816\n",
      "Counter 1 of 5\n",
      "epoch: 133 done, train_loss: 0.5782269636789957\n",
      "save best loss state model & log(epoch 133)\n",
      "epoch: 134 done, train_loss: 0.578894225259622\n",
      "save best loss state model & log(epoch 134)\n",
      "epoch: 135 done, train_loss: 0.5940205181638399\n",
      "Counter 1 of 5\n",
      "epoch: 136 done, train_loss: 0.569989473455482\n",
      "save best loss state model & log(epoch 136)\n",
      "epoch: 137 done, train_loss: 0.5290232780906889\n",
      "save best loss state model & log(epoch 137)\n",
      "epoch: 138 done, train_loss: 0.5704075346390406\n",
      "Counter 1 of 5\n",
      "epoch: 139 done, train_loss: 0.5851035209165679\n",
      "Counter 2 of 5\n",
      "epoch: 140 done, train_loss: 0.5327835919128524\n",
      "Counter 3 of 5\n",
      "epoch: 141 done, train_loss: 0.5654014804297023\n",
      "save best loss state model & log(epoch 141)\n",
      "epoch: 142 done, train_loss: 0.5628974686066309\n",
      "Counter 1 of 5\n",
      "epoch: 143 done, train_loss: 0.5414242934849527\n",
      "save best loss state model & log(epoch 143)\n",
      "epoch: 144 done, train_loss: 0.5124450946847597\n",
      "save best loss state model & log(epoch 144)\n",
      "epoch: 145 done, train_loss: 0.5201540655559964\n",
      "Counter 1 of 5\n",
      "epoch: 146 done, train_loss: 0.5288568826185333\n",
      "save best loss state model & log(epoch 146)\n",
      "epoch: 147 done, train_loss: 0.534243279033237\n",
      "save best loss state model & log(epoch 147)\n",
      "epoch: 148 done, train_loss: 0.5856448320878876\n",
      "save best loss state model & log(epoch 148)\n",
      "epoch: 149 done, train_loss: 0.5059717098871866\n",
      "Counter 1 of 5\n",
      "epoch: 150 done, train_loss: 0.559996147122648\n",
      "save best loss state model & log(epoch 150)\n",
      "epoch: 151 done, train_loss: 0.5303750435511271\n",
      "Counter 1 of 5\n",
      "epoch: 152 done, train_loss: 0.4950246372156673\n",
      "Counter 2 of 5\n",
      "epoch: 153 done, train_loss: 0.5545436027977202\n",
      "save best loss state model & log(epoch 153)\n",
      "epoch: 154 done, train_loss: 0.516363435321384\n",
      "save best loss state model & log(epoch 154)\n",
      "epoch: 155 done, train_loss: 0.5723369982507494\n",
      "save best loss state model & log(epoch 155)\n",
      "epoch: 156 done, train_loss: 0.49104615383678013\n",
      "save best loss state model & log(epoch 156)\n",
      "epoch: 157 done, train_loss: 0.5381638573275672\n",
      "Counter 1 of 5\n",
      "epoch: 158 done, train_loss: 0.5031593806213803\n",
      "Counter 2 of 5\n",
      "epoch: 159 done, train_loss: 0.5353448192278544\n",
      "save best loss state model & log(epoch 159)\n",
      "epoch: 160 done, train_loss: 0.5380833405587409\n",
      "save best loss state model & log(epoch 160)\n",
      "epoch: 161 done, train_loss: 0.5133583247661591\n",
      "Counter 1 of 5\n",
      "epoch: 162 done, train_loss: 0.5113553346859084\n",
      "save best loss state model & log(epoch 162)\n",
      "epoch: 163 done, train_loss: 0.5175817501213815\n",
      "Counter 1 of 5\n",
      "epoch: 164 done, train_loss: 0.5189700689580705\n",
      "Counter 2 of 5\n",
      "epoch: 165 done, train_loss: 0.5318531543016434\n",
      "Counter 3 of 5\n",
      "epoch: 166 done, train_loss: 0.49590512696239686\n",
      "save best loss state model & log(epoch 166)\n",
      "epoch: 167 done, train_loss: 0.5328853378693262\n",
      "Counter 1 of 5\n",
      "epoch: 168 done, train_loss: 0.4795951628022724\n",
      "Counter 2 of 5\n",
      "epoch: 169 done, train_loss: 0.4868960637185309\n",
      "save best loss state model & log(epoch 169)\n",
      "epoch: 170 done, train_loss: 0.49193190617693794\n",
      "save best loss state model & log(epoch 170)\n",
      "epoch: 171 done, train_loss: 0.4857844163974126\n",
      "save best loss state model & log(epoch 171)\n",
      "epoch: 172 done, train_loss: 0.46198636210627025\n",
      "Counter 1 of 5\n",
      "epoch: 173 done, train_loss: 0.5008216400941213\n",
      "save best loss state model & log(epoch 173)\n",
      "epoch: 174 done, train_loss: 0.48585231188270783\n",
      "save best loss state model & log(epoch 174)\n",
      "epoch: 175 done, train_loss: 0.4871538546350267\n",
      "save best loss state model & log(epoch 175)\n",
      "epoch: 176 done, train_loss: 0.48247194704082275\n",
      "save best loss state model & log(epoch 176)\n",
      "epoch: 177 done, train_loss: 0.4758821957641178\n",
      "Counter 1 of 5\n",
      "epoch: 178 done, train_loss: 0.4947074204683304\n",
      "Counter 2 of 5\n",
      "epoch: 179 done, train_loss: 0.496968870361646\n",
      "Counter 3 of 5\n",
      "epoch: 180 done, train_loss: 0.4908686363034778\n",
      "save best loss state model & log(epoch 180)\n",
      "epoch: 181 done, train_loss: 0.5064436710543103\n",
      "Counter 1 of 5\n",
      "epoch: 182 done, train_loss: 0.4797718541489707\n",
      "save best loss state model & log(epoch 182)\n",
      "epoch: 183 done, train_loss: 0.4851042611731423\n",
      "save best loss state model & log(epoch 183)\n",
      "epoch: 184 done, train_loss: 0.48085929205020267\n",
      "save best loss state model & log(epoch 184)\n",
      "epoch: 185 done, train_loss: 0.47159864670700496\n",
      "Counter 1 of 5\n",
      "epoch: 186 done, train_loss: 0.4880084784494506\n",
      "Counter 2 of 5\n",
      "epoch: 187 done, train_loss: 0.4735709743367301\n",
      "save best loss state model & log(epoch 187)\n",
      "epoch: 188 done, train_loss: 0.4963465589616034\n",
      "Counter 1 of 5\n",
      "epoch: 189 done, train_loss: 0.4876607673035728\n",
      "save best loss state model & log(epoch 189)\n",
      "epoch: 190 done, train_loss: 0.4752170749836498\n",
      "Counter 1 of 5\n",
      "epoch: 191 done, train_loss: 0.47016163998179966\n",
      "save best loss state model & log(epoch 191)\n",
      "epoch: 192 done, train_loss: 0.4669373515579436\n",
      "Counter 1 of 5\n",
      "epoch: 193 done, train_loss: 0.46852537327342564\n",
      "save best loss state model & log(epoch 193)\n",
      "epoch: 194 done, train_loss: 0.4419751796457503\n",
      "save best loss state model & log(epoch 194)\n",
      "epoch: 195 done, train_loss: 0.4961048323247168\n",
      "save best loss state model & log(epoch 195)\n",
      "epoch: 196 done, train_loss: 0.47053486274348366\n",
      "Counter 1 of 5\n",
      "epoch: 197 done, train_loss: 0.4687338363793161\n",
      "save best loss state model & log(epoch 197)\n",
      "epoch: 198 done, train_loss: 0.44713328488998944\n",
      "save best loss state model & log(epoch 198)\n",
      "epoch: 199 done, train_loss: 0.43886393308639526\n",
      "Counter 1 of 5\n",
      "epoch: 200 done, train_loss: 0.4820430775483449\n",
      "Counter 2 of 5\n",
      "epoch: 201 done, train_loss: 0.469020856751336\n",
      "Counter 3 of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 202 done, train_loss: 0.457380344470342\n",
      "save best loss state model & log(epoch 202)\n",
      "epoch: 203 done, train_loss: 0.44554927365647423\n",
      "Counter 1 of 5\n",
      "epoch: 204 done, train_loss: 0.4592753541138437\n",
      "Counter 2 of 5\n",
      "epoch: 205 done, train_loss: 0.4511765291293462\n",
      "save best loss state model & log(epoch 205)\n",
      "epoch: 206 done, train_loss: 0.4554980562792884\n",
      "save best loss state model & log(epoch 206)\n",
      "epoch: 207 done, train_loss: 0.4566250716646512\n",
      "save best loss state model & log(epoch 207)\n",
      "epoch: 208 done, train_loss: 0.4588734399941232\n",
      "save best loss state model & log(epoch 208)\n",
      "epoch: 209 done, train_loss: 0.4268709404600991\n",
      "Counter 1 of 5\n",
      "epoch: 210 done, train_loss: 0.4559331031309234\n",
      "save best loss state model & log(epoch 210)\n",
      "epoch: 211 done, train_loss: 0.42838386197884876\n",
      "Counter 1 of 5\n",
      "epoch: 212 done, train_loss: 0.4763445349203216\n",
      "Counter 2 of 5\n",
      "epoch: 213 done, train_loss: 0.4453582829899258\n",
      "save best loss state model & log(epoch 213)\n",
      "epoch: 214 done, train_loss: 0.43203549252616036\n",
      "Counter 1 of 5\n",
      "epoch: 215 done, train_loss: 0.4262330142988099\n",
      "Counter 2 of 5\n",
      "epoch: 216 done, train_loss: 0.44398295051521725\n",
      "Counter 3 of 5\n",
      "epoch: 217 done, train_loss: 0.42630721628665924\n",
      "save best loss state model & log(epoch 217)\n",
      "epoch: 218 done, train_loss: 0.4150235280394554\n",
      "save best loss state model & log(epoch 218)\n",
      "epoch: 219 done, train_loss: 0.45279987156391144\n",
      "save best loss state model & log(epoch 219)\n",
      "epoch: 220 done, train_loss: 0.4263148506482442\n",
      "Counter 1 of 5\n",
      "epoch: 221 done, train_loss: 0.40625421164764297\n",
      "Counter 2 of 5\n",
      "epoch: 222 done, train_loss: 0.43059323231379193\n",
      "Counter 3 of 5\n",
      "epoch: 223 done, train_loss: 0.413414357850949\n",
      "save best loss state model & log(epoch 223)\n",
      "epoch: 224 done, train_loss: 0.4152515795495775\n",
      "save best loss state model & log(epoch 224)\n",
      "epoch: 225 done, train_loss: 0.4462904358903567\n",
      "Counter 1 of 5\n",
      "epoch: 226 done, train_loss: 0.4217801218231519\n",
      "save best loss state model & log(epoch 226)\n",
      "epoch: 227 done, train_loss: 0.4220862214763959\n",
      "Counter 1 of 5\n",
      "epoch: 228 done, train_loss: 0.42658058057228726\n",
      "save best loss state model & log(epoch 228)\n",
      "epoch: 229 done, train_loss: 0.41686341332064736\n",
      "save best loss state model & log(epoch 229)\n",
      "epoch: 230 done, train_loss: 0.4021156852444013\n",
      "save best loss state model & log(epoch 230)\n",
      "epoch: 231 done, train_loss: 0.4127124059531424\n",
      "save best loss state model & log(epoch 231)\n",
      "epoch: 232 done, train_loss: 0.43481801119115615\n",
      "save best loss state model & log(epoch 232)\n",
      "epoch: 233 done, train_loss: 0.41981514501902795\n",
      "save best loss state model & log(epoch 233)\n",
      "epoch: 234 done, train_loss: 0.41608787328004837\n",
      "Counter 1 of 5\n",
      "epoch: 235 done, train_loss: 0.41684412956237793\n",
      "Counter 2 of 5\n",
      "epoch: 236 done, train_loss: 0.44070270160833996\n",
      "save best loss state model & log(epoch 236)\n",
      "epoch: 237 done, train_loss: 0.44681188960870105\n",
      "save best loss state model & log(epoch 237)\n",
      "epoch: 238 done, train_loss: 0.3855852025250594\n",
      "Counter 1 of 5\n",
      "epoch: 239 done, train_loss: 0.40094360295269227\n",
      "Counter 2 of 5\n",
      "epoch: 240 done, train_loss: 0.39418735603491467\n",
      "save best loss state model & log(epoch 240)\n",
      "epoch: 241 done, train_loss: 0.4612601904405488\n",
      "save best loss state model & log(epoch 241)\n",
      "epoch: 242 done, train_loss: 0.401627575357755\n",
      "Counter 1 of 5\n",
      "epoch: 243 done, train_loss: 0.39749787582291496\n",
      "Counter 2 of 5\n",
      "epoch: 244 done, train_loss: 0.40975049055284923\n",
      "save best loss state model & log(epoch 244)\n",
      "epoch: 245 done, train_loss: 0.421914230618212\n",
      "Counter 1 of 5\n",
      "epoch: 246 done, train_loss: 0.4004180481036504\n",
      "Counter 2 of 5\n",
      "epoch: 247 done, train_loss: 0.40367935804857147\n",
      "Counter 3 of 5\n",
      "epoch: 248 done, train_loss: 0.381211433145735\n",
      "Counter 4 of 5\n",
      "epoch: 249 done, train_loss: 0.3923797528776858\n",
      "save best loss state model & log(epoch 249)\n",
      "epoch: 250 done, train_loss: 0.4051384888589382\n",
      "Counter 1 of 5\n",
      "epoch: 251 done, train_loss: 0.3778770731555091\n",
      "Counter 2 of 5\n",
      "epoch: 252 done, train_loss: 0.41543986317184234\n",
      "save best loss state model & log(epoch 252)\n",
      "epoch: 253 done, train_loss: 0.3967982563707564\n",
      "Counter 1 of 5\n",
      "epoch: 254 done, train_loss: 0.3872770696050591\n",
      "Counter 2 of 5\n",
      "epoch: 255 done, train_loss: 0.3746196606920825\n",
      "save best loss state model & log(epoch 255)\n",
      "epoch: 256 done, train_loss: 0.3737946276863416\n",
      "save best loss state model & log(epoch 256)\n",
      "epoch: 257 done, train_loss: 0.3913752105500963\n",
      "Counter 1 of 5\n",
      "epoch: 258 done, train_loss: 0.3835066859092977\n",
      "Counter 2 of 5\n",
      "epoch: 259 done, train_loss: 0.3959372184342808\n",
      "save best loss state model & log(epoch 259)\n",
      "epoch: 260 done, train_loss: 0.4027007731298606\n",
      "save best loss state model & log(epoch 260)\n",
      "epoch: 261 done, train_loss: 0.3815988269117143\n",
      "save best loss state model & log(epoch 261)\n",
      "epoch: 262 done, train_loss: 0.3785129984219869\n",
      "save best loss state model & log(epoch 262)\n",
      "epoch: 263 done, train_loss: 0.3780040095249812\n",
      "Counter 1 of 5\n",
      "epoch: 264 done, train_loss: 0.4138510897755623\n",
      "Counter 2 of 5\n",
      "epoch: 265 done, train_loss: 0.3927331864833832\n",
      "save best loss state model & log(epoch 265)\n",
      "epoch: 266 done, train_loss: 0.38040560649500954\n",
      "save best loss state model & log(epoch 266)\n",
      "epoch: 267 done, train_loss: 0.40134269785549903\n",
      "save best loss state model & log(epoch 267)\n",
      "epoch: 268 done, train_loss: 0.38553227070305085\n",
      "Counter 1 of 5\n",
      "epoch: 269 done, train_loss: 0.39283931959006524\n",
      "save best loss state model & log(epoch 269)\n",
      "epoch: 270 done, train_loss: 0.37978043945299256\n",
      "Counter 1 of 5\n",
      "epoch: 271 done, train_loss: 0.3638443586726983\n",
      "save best loss state model & log(epoch 271)\n",
      "epoch: 272 done, train_loss: 0.3911755395432313\n",
      "save best loss state model & log(epoch 272)\n",
      "epoch: 273 done, train_loss: 0.3558642471002208\n",
      "Counter 1 of 5\n",
      "epoch: 274 done, train_loss: 0.3932324027021726\n",
      "Counter 2 of 5\n",
      "epoch: 275 done, train_loss: 0.41152044137318927\n",
      "Counter 3 of 5\n",
      "epoch: 276 done, train_loss: 0.39253201170100105\n",
      "save best loss state model & log(epoch 276)\n",
      "epoch: 277 done, train_loss: 0.3756161824696594\n",
      "Counter 1 of 5\n",
      "epoch: 278 done, train_loss: 0.3963660117652681\n",
      "save best loss state model & log(epoch 278)\n",
      "epoch: 279 done, train_loss: 0.3704479622344176\n",
      "save best loss state model & log(epoch 279)\n",
      "epoch: 280 done, train_loss: 0.36560038063261246\n",
      "Counter 1 of 5\n",
      "epoch: 281 done, train_loss: 0.3647535526090198\n",
      "save best loss state model & log(epoch 281)\n",
      "epoch: 282 done, train_loss: 0.38736047099033993\n",
      "save best loss state model & log(epoch 282)\n",
      "epoch: 283 done, train_loss: 0.398454745610555\n",
      "Counter 1 of 5\n",
      "epoch: 284 done, train_loss: 0.368573646992445\n",
      "Counter 2 of 5\n",
      "epoch: 285 done, train_loss: 0.3861934261189567\n",
      "save best loss state model & log(epoch 285)\n",
      "epoch: 286 done, train_loss: 0.3934738122754627\n",
      "Counter 1 of 5\n",
      "epoch: 287 done, train_loss: 0.37961769104003906\n",
      "Counter 2 of 5\n",
      "epoch: 288 done, train_loss: 0.40517985075712204\n",
      "Counter 3 of 5\n",
      "epoch: 289 done, train_loss: 0.3947661452823215\n",
      "Counter 4 of 5\n",
      "epoch: 290 done, train_loss: 0.37924469427929985\n",
      "save best loss state model & log(epoch 290)\n",
      "epoch: 291 done, train_loss: 0.39282925551136333\n",
      "Counter 1 of 5\n",
      "epoch: 292 done, train_loss: 0.3894825726747513\n",
      "Counter 2 of 5\n",
      "epoch: 293 done, train_loss: 0.35615065321326256\n",
      "save best loss state model & log(epoch 293)\n",
      "epoch: 294 done, train_loss: 0.37360504145423573\n",
      "Counter 1 of 5\n",
      "epoch: 295 done, train_loss: 0.36359694972634315\n",
      "save best loss state model & log(epoch 295)\n",
      "epoch: 296 done, train_loss: 0.40569911524653435\n",
      "Counter 1 of 5\n",
      "epoch: 297 done, train_loss: 0.37073345937662655\n",
      "Counter 2 of 5\n",
      "epoch: 298 done, train_loss: 0.3904130454692576\n",
      "Counter 3 of 5\n",
      "epoch: 299 done, train_loss: 0.3632754033638371\n",
      "save best loss state model & log(epoch 299)\n",
      "epoch: 300 done, train_loss: 0.34862903671132195\n",
      "Counter 1 of 5\n",
      "epoch: 301 done, train_loss: 0.3760792323284679\n",
      "save best loss state model & log(epoch 301)\n",
      "epoch: 302 done, train_loss: 0.3866896765927474\n",
      "save best loss state model & log(epoch 302)\n",
      "epoch: 303 done, train_loss: 0.3581124097108841\n",
      "save best loss state model & log(epoch 303)\n",
      "epoch: 304 done, train_loss: 0.3622010201215744\n",
      "save best loss state model & log(epoch 304)\n",
      "epoch: 305 done, train_loss: 0.3637876771390438\n",
      "save best loss state model & log(epoch 305)\n",
      "epoch: 306 done, train_loss: 0.3620835691690445\n",
      "Counter 1 of 5\n",
      "epoch: 307 done, train_loss: 0.37447700566715664\n",
      "Counter 2 of 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 308 done, train_loss: 0.3538840123348766\n",
      "Counter 3 of 5\n",
      "epoch: 309 done, train_loss: 0.3826797923280133\n",
      "save best loss state model & log(epoch 309)\n",
      "epoch: 310 done, train_loss: 0.3336207614176803\n",
      "save best loss state model & log(epoch 310)\n",
      "epoch: 311 done, train_loss: 0.36390185521708596\n",
      "Counter 1 of 5\n",
      "epoch: 312 done, train_loss: 0.34973593594299424\n",
      "save best loss state model & log(epoch 312)\n",
      "epoch: 313 done, train_loss: 0.3774471580982208\n",
      "Counter 1 of 5\n",
      "epoch: 314 done, train_loss: 0.35882802431782085\n",
      "Counter 2 of 5\n",
      "epoch: 315 done, train_loss: 0.35836147020260495\n",
      "Counter 3 of 5\n",
      "epoch: 316 done, train_loss: 0.35151588833994335\n",
      "save best loss state model & log(epoch 316)\n",
      "epoch: 317 done, train_loss: 0.3854202495680915\n",
      "save best loss state model & log(epoch 317)\n",
      "epoch: 318 done, train_loss: 0.3921714992158943\n",
      "Counter 1 of 5\n",
      "epoch: 319 done, train_loss: 0.36496928830941516\n",
      "Counter 2 of 5\n",
      "epoch: 320 done, train_loss: 0.3385783988568518\n",
      "Counter 3 of 5\n",
      "epoch: 321 done, train_loss: 0.355765536841419\n",
      "save best loss state model & log(epoch 321)\n",
      "epoch: 322 done, train_loss: 0.38539449829194283\n",
      "save best loss state model & log(epoch 322)\n",
      "epoch: 323 done, train_loss: 0.3493947597841422\n",
      "Counter 1 of 5\n",
      "epoch: 324 done, train_loss: 0.3534574579033587\n",
      "Counter 2 of 5\n",
      "epoch: 325 done, train_loss: 0.36942439319358933\n",
      "Counter 3 of 5\n",
      "epoch: 326 done, train_loss: 0.3668499357170529\n",
      "save best loss state model & log(epoch 326)\n",
      "epoch: 327 done, train_loss: 0.3686223025951121\n",
      "save best loss state model & log(epoch 327)\n",
      "epoch: 328 done, train_loss: 0.3568677310314443\n",
      "save best loss state model & log(epoch 328)\n",
      "epoch: 329 done, train_loss: 0.35718706167406505\n",
      "save best loss state model & log(epoch 329)\n",
      "epoch: 330 done, train_loss: 0.36636549565527177\n",
      "save best loss state model & log(epoch 330)\n",
      "epoch: 331 done, train_loss: 0.3565231007006433\n",
      "Counter 1 of 5\n",
      "epoch: 332 done, train_loss: 0.3459116402599547\n",
      "Counter 2 of 5\n",
      "epoch: 333 done, train_loss: 0.33197973171869916\n",
      "save best loss state model & log(epoch 333)\n",
      "epoch: 334 done, train_loss: 0.35537326832612354\n",
      "Counter 1 of 5\n",
      "epoch: 335 done, train_loss: 0.35154767168892753\n",
      "save best loss state model & log(epoch 335)\n",
      "epoch: 336 done, train_loss: 0.3587652374472883\n",
      "Counter 1 of 5\n",
      "epoch: 337 done, train_loss: 0.3475327288938893\n",
      "Counter 2 of 5\n",
      "epoch: 338 done, train_loss: 0.3636704720556736\n",
      "save best loss state model & log(epoch 338)\n",
      "epoch: 339 done, train_loss: 0.3748312368988991\n",
      "Counter 1 of 5\n",
      "epoch: 340 done, train_loss: 0.34706182156999904\n",
      "save best loss state model & log(epoch 340)\n",
      "epoch: 341 done, train_loss: 0.36433947334686917\n",
      "Counter 1 of 5\n",
      "epoch: 342 done, train_loss: 0.3500346408949958\n",
      "save best loss state model & log(epoch 342)\n",
      "epoch: 343 done, train_loss: 0.3220890706612004\n",
      "save best loss state model & log(epoch 343)\n",
      "epoch: 344 done, train_loss: 0.3329894029431873\n",
      "save best loss state model & log(epoch 344)\n",
      "epoch: 345 done, train_loss: 0.37646371374527615\n",
      "save best loss state model & log(epoch 345)\n",
      "epoch: 346 done, train_loss: 0.3531513487299283\n",
      "Counter 1 of 5\n",
      "epoch: 347 done, train_loss: 0.33827899561987984\n",
      "save best loss state model & log(epoch 347)\n",
      "epoch: 348 done, train_loss: 0.34561286121606827\n",
      "save best loss state model & log(epoch 348)\n",
      "epoch: 349 done, train_loss: 0.3553440525299973\n",
      "Counter 1 of 5\n",
      "epoch: 350 done, train_loss: 0.3491639950209194\n",
      "Counter 2 of 5\n",
      "epoch: 351 done, train_loss: 0.3285232215291924\n",
      "Counter 3 of 5\n",
      "epoch: 352 done, train_loss: 0.3309308836857478\n",
      "save best loss state model & log(epoch 352)\n",
      "epoch: 353 done, train_loss: 0.34672991144988274\n",
      "Counter 1 of 5\n",
      "epoch: 354 done, train_loss: 0.3301652425693141\n",
      "save best loss state model & log(epoch 354)\n",
      "epoch: 355 done, train_loss: 0.33093514665961266\n",
      "Counter 1 of 5\n",
      "epoch: 356 done, train_loss: 0.33759304881095886\n",
      "save best loss state model & log(epoch 356)\n",
      "epoch: 357 done, train_loss: 0.3400534734957748\n",
      "save best loss state model & log(epoch 357)\n",
      "epoch: 358 done, train_loss: 0.33402229183250004\n",
      "Counter 1 of 5\n",
      "epoch: 359 done, train_loss: 0.3515343430141608\n",
      "save best loss state model & log(epoch 359)\n",
      "epoch: 360 done, train_loss: 0.33796582329604363\n",
      "Counter 1 of 5\n",
      "epoch: 361 done, train_loss: 0.34027548092934823\n",
      "save best loss state model & log(epoch 361)\n",
      "epoch: 362 done, train_loss: 0.3455866157180733\n",
      "save best loss state model & log(epoch 362)\n",
      "epoch: 363 done, train_loss: 0.3298138446278042\n",
      "Counter 1 of 5\n",
      "epoch: 364 done, train_loss: 0.3313904085920917\n",
      "Counter 2 of 5\n",
      "epoch: 365 done, train_loss: 0.3176015106340249\n",
      "Counter 3 of 5\n",
      "epoch: 366 done, train_loss: 0.3162341850499312\n",
      "Counter 4 of 5\n",
      "epoch: 367 done, train_loss: 0.32649844512343407\n",
      "Counter 5 of 5\n",
      "Early stopping with best_loss:  0.006907077784037722 and val_loss for this epoch:  0.0069556858299213795 ...\n"
     ]
    }
   ],
   "source": [
    "train_mse, eval_mse = bert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "868b31cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bert_trainer.save_model(os.path.join(model_path, 'bert_class.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7dfeb754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = list(range(len(train_mse)))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(list(range(len(train_mse))), train_mse, label='train_rmse')\n",
    "plt.plot(list(range(len(eval_mse))), eval_mse, label='val_rmse')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32d280",
   "metadata": {},
   "source": [
    "### 3. Model Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7803e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_kor</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i've tried to ignore my feelings but i really ...</td>\n",
       "      <td>나는 내 감정을 무시하려고 노력했지만 정말 우울하다</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to about ish and my mothers just been getting ...</td>\n",
       "      <td>나는 그녀에게 우울증이 있다고 말했고 그녀는 내가 더 일찍 일어나야 한다고 말했고 ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at the apex we feel something at the trough we...</td>\n",
       "      <td>정점에서 우리는 무언가를 느낀다 우리는 우울하게 느끼는 무언가를 느낀다</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i don't think i am depressed or suicidal but v...</td>\n",
       "      <td>나는 내가 우울하거나 자살했다고 생각하지 않지만 매우 매우 지루하고 혼자라고 생각한다</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>now that my ankle is healed though i feel depr...</td>\n",
       "      <td>다시 우울해지긴 했지만 발목이 다 나았으니</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>upon receiving the news each time i've been un...</td>\n",
       "      <td>매번 그 소식을 듣자마자 나는 울거나 슬퍼할 수 없었다</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Why are you doing this when you're not even dr...</td>\n",
       "      <td>술도 안 취했는데 왜 이러는 거야?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>whenever i feel lonely i go here only to reali...</td>\n",
       "      <td>내가 외로울 때마다 나는 내가 아니라는 것을 깨달았을 뿐이다</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>During the training, I was able to get my grad...</td>\n",
       "      <td>훈련 기간 동안, 나는 5등까지 성적을 올릴 수 있었다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>i just feel lonely and unwanteda little help p...</td>\n",
       "      <td>나는 단지 외롭고 원하지 않는 도움을 조금만 주세요</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    i've tried to ignore my feelings but i really ...   \n",
       "1    to about ish and my mothers just been getting ...   \n",
       "2    at the apex we feel something at the trough we...   \n",
       "3    i don't think i am depressed or suicidal but v...   \n",
       "4    now that my ankle is healed though i feel depr...   \n",
       "..                                                 ...   \n",
       "315  upon receiving the news each time i've been un...   \n",
       "316  Why are you doing this when you're not even dr...   \n",
       "317  whenever i feel lonely i go here only to reali...   \n",
       "318  During the training, I was able to get my grad...   \n",
       "319  i just feel lonely and unwanteda little help p...   \n",
       "\n",
       "                                              text_kor  label  \n",
       "0                         나는 내 감정을 무시하려고 노력했지만 정말 우울하다     14  \n",
       "1    나는 그녀에게 우울증이 있다고 말했고 그녀는 내가 더 일찍 일어나야 한다고 말했고 ...     10  \n",
       "2              정점에서 우리는 무언가를 느낀다 우리는 우울하게 느끼는 무언가를 느낀다      8  \n",
       "3      나는 내가 우울하거나 자살했다고 생각하지 않지만 매우 매우 지루하고 혼자라고 생각한다     11  \n",
       "4                              다시 우울해지긴 했지만 발목이 다 나았으니      9  \n",
       "..                                                 ...    ...  \n",
       "315                     매번 그 소식을 듣자마자 나는 울거나 슬퍼할 수 없었다      8  \n",
       "316                                술도 안 취했는데 왜 이러는 거야?      0  \n",
       "317                  내가 외로울 때마다 나는 내가 아니라는 것을 깨달았을 뿐이다      9  \n",
       "318                     훈련 기간 동안, 나는 5등까지 성적을 올릴 수 있었다      0  \n",
       "319                       나는 단지 외롭고 원하지 않는 도움을 조금만 주세요     10  \n",
       "\n",
       "[320 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(os.path.join(data_path, 'bws_score_test.csv'))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a16f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b46c8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path, 'training_config.json')) as f:\n",
    "    training_config = AttrDict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0086a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.pad = 'max_length'\n",
    "training_config.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "466405f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = data_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.label)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # def clear_text(self)  => 전처리 코드를 여기에 넣을 경우 상당히 느려짐\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return text, label\n",
    "        '''\n",
    "        self.reset_index()\n",
    "        text = self.data.text[idx]\n",
    "        label = self.data.label[idx]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5003bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.model = model\n",
    "        self.linear = nn.Linear(config.hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.linear(logits)\n",
    "        x = self.relu(x)\n",
    "        score = self.out(x)\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3451c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertProcessor():\n",
    "    def __init__(self, config, training_config, tokenizer, truncation=True):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = config.max_position_embeddings\n",
    "        self.pad = training_config.pad\n",
    "        self.batch_size = training_config.train_batch_size\n",
    "        self.truncation = truncation\n",
    "    \n",
    "    def convert_data(self, data_file):\n",
    "        context2 = None    # single sentence classification\n",
    "\n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(data_file[idx][0], context2) for idx in range(len(data_file))],   # text, \n",
    "            max_length = self.max_len,\n",
    "            padding = self.pad,\n",
    "            truncation = self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        for i in range(len(data_file)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "            try:\n",
    "                inputs['label'] = data_file[i][1] \n",
    "            except:\n",
    "                inputs['label'] = 0 \n",
    "            features.append(inputs)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        all_labels = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "        return dataset\n",
    "    \n",
    "    def convert_sentence(self, sent_list):   # 사용자 입력 문장 1개 -> 입력 형태 변환\n",
    "        context2 = None \n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(sent_list, context2)], max_length=self.max_len, padding=self.pad, truncation=self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        inputs = {k: batch_encoding[k][0] for k in batch_encoding}\n",
    "        inputs['label'] = 0 \n",
    "        features.append(inputs)\n",
    "\n",
    "        input_id = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        input_am = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        input_tts = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        input_lb = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "        dataset = TensorDataset(input_id, input_am, input_tts, input_lb)\n",
    "        return dataset\n",
    "    \n",
    "    def shuffle_data(self, dataset, data_type):\n",
    "        if data_type == 'train':\n",
    "            return RandomSampler(dataset)\n",
    "        elif data_type == 'eval' or data_type == 'test':\n",
    "            return SequentialSampler(dataset)\n",
    "        \n",
    "    def load_data(self, dataset, sampler):\n",
    "        return DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66386a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTester():\n",
    "    def __init__(self, training_config, model):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "\n",
    "    def get_label(self, test_dataloader, test_type):\n",
    "        '''\n",
    "        test_type: 0  -> Test dataset \n",
    "        test_type: 1  -> Test sentence\n",
    "        '''\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            self.model.eval()   # self 안 붙이면 이상한 Output (BaseModelOutputWithPoolingAndCrossAttentions) 출력 \n",
    "            batch = tuple(t.to(training_config.device) for t in batch)   # args.device: cuda \n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                if test_type == 0:\n",
    "                    preds.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                elif test_type == 1:\n",
    "                    preds.extend(outputs[0].detach().cpu().numpy())            \n",
    "            label = batch[3].detach().cpu().numpy()\n",
    "            labels.extend(label)\n",
    "        return preds, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "97c5948d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/lamda_00/Depression_paper/model/bert-mini were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(model_path, 'bert-tiny'), model_max_length=32)\n",
    "config = BertConfig.from_pretrained(os.path.join(model_path, 'bert-tiny', 'bert_config.json'))\n",
    "model = BertModel.from_pretrained(os.path.join(model_path, 'bert-tiny'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8dfbe5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processor = BertProcessor(config, training_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb41887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = BertDataset(X_test)\n",
    "test_dataset = test_processor.convert_data(test_file)\n",
    "test_sampler = test_processor.shuffle_data(test_dataset, 'test')\n",
    "test_dataloader = test_processor.load_data(test_dataset, test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbae7b92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_reg \u001b[38;5;241m=\u001b[39m BertRegressor(config, model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "model_reg = BertRegressor(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2430407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.path.join(model_path, 'bert_bws_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d97810a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertRegressor(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reg.load_state_dict(torch.load(model_name))\n",
    "model_reg.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b9cabbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tester = BertTester(training_config, model_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "97399182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i've tried to ignore my feelings but i really am depressed\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1acd9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = bert_tester.get_label(test_dataloader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4528a378",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10, 9, 8, 12, 9, 6, 0, 9, 8, 12], [14, 10, 8, 11, 9, 0, 0, 12, 8, 10])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, y_pred))[:10], y_true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7d143ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 320, 10.0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader), len(X_test), len(X_test) / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "91b60322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 320)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred), len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c790fb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 10, 8, 11, 9], [10.677643, 9.397481, 8.988688, 12.5328245, 9.325927])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:5], y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64a292",
   "metadata": {},
   "source": [
    "with open(os.path.join(data_path, 'conv.pickle'), 'rb') as f:\n",
    "    conv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2cb598b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "conv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07faf86f",
   "metadata": {},
   "source": [
    "conv.text.loc[6] = '오늘 너무 우울하다'\n",
    "conv.text.loc[12] = '일이 너무 많아서 잠도 너무 못 자 '\n",
    "conv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "084b386c",
   "metadata": {},
   "source": [
    "translated = [\"hey\", \"Hello, nice to meet you\", \"what is your name ?\", \"I am a psychological counseling chatbot\", \"Ah-huh\", \\\n",
    "              \"good morning\", \"I'm so depressed today\", \"What's wrong?\", \"I just feel so lethargic these days\", \\\n",
    "              \"When you're lethargic, you have to move your body\", \"I think he's psychologically exhausted\", \\\n",
    "              \"I think you're really tired Is your work very hard?\", \"I can't sleep because I have too much work to do\", \\\n",
    "              \"I think it's good to take a break for a while\", \"I should I'll go for a walk from time to time\", \\\n",
    "              \"Taking a walk is good\", \"Thanks for listening\", \"Yes! See you next time\"]\n",
    "\n",
    "len(translated)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bce24eb",
   "metadata": {},
   "source": [
    "conv['translated'] = translated\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "589b705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'conv_translated.pickle'), 'wb') as f:\n",
    "    pickle.dump(conv, f, pickle.HIGHEST_PROTOCOL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "fdcc1c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn_cnt</th>\n",
       "      <th>speaker_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>User</td>\n",
       "      <td>안녕</td>\n",
       "      <td>hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>안녕하세요 ~ 반가워요</td>\n",
       "      <td>Hello, nice to meet you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>User</td>\n",
       "      <td>이름이 뭐야 ?</td>\n",
       "      <td>what is your name ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>저는 심리상담 챗봇, ~ 에요</td>\n",
       "      <td>I am a psychological counseling chatbot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>User</td>\n",
       "      <td>아하 ㅎㅎ</td>\n",
       "      <td>Ah-huh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>좋은 아침이에요</td>\n",
       "      <td>good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>User</td>\n",
       "      <td>오늘 너무 우울하다</td>\n",
       "      <td>I'm so depressed today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>무슨 일 있어요 ?</td>\n",
       "      <td>What's wrong?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>User</td>\n",
       "      <td>그냥 요즘 너무 무기력한거 같아</td>\n",
       "      <td>I just feel so lethargic these days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>무기력할 때는 몸을 움직여야 해요</td>\n",
       "      <td>When you're lethargic, you have to move your body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>User</td>\n",
       "      <td>심리적으로도 지친 거 같아</td>\n",
       "      <td>I think he's psychologically exhausted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>많이 지치신게 느껴지는거 같아요. 일이 많이 힘든가요 ?</td>\n",
       "      <td>I think you're really tired Is your work very ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>User</td>\n",
       "      <td>일이 너무 많아서 잠도 너무 못 자</td>\n",
       "      <td>I can't sleep because I have too much work to do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>그럴때는 잠시 마음의 여유를 가지고 쉬었다 가는 것도 좋은거 같아요</td>\n",
       "      <td>I think it's good to take a break for a while</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>User</td>\n",
       "      <td>그래야겠어, 종종 산책하러 다녀야지</td>\n",
       "      <td>I should I'll go for a walk from time to time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>산책 좋죠</td>\n",
       "      <td>Taking a walk is good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>User</td>\n",
       "      <td>들어줘서 고마워</td>\n",
       "      <td>Thanks for listening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chatbot</td>\n",
       "      <td>네 ! 다음에 또 봐요</td>\n",
       "      <td>Yes! See you next time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    turn_cnt speaker_idx                                   text  \\\n",
       "0          1        User                                     안녕   \n",
       "1          2     Chatbot                           안녕하세요 ~ 반가워요   \n",
       "2          3        User                               이름이 뭐야 ?   \n",
       "3          4     Chatbot                       저는 심리상담 챗봇, ~ 에요   \n",
       "4          5        User                                  아하 ㅎㅎ   \n",
       "5          6     Chatbot                               좋은 아침이에요   \n",
       "6          7        User                             오늘 너무 우울하다   \n",
       "7          8     Chatbot                             무슨 일 있어요 ?   \n",
       "8          9        User                      그냥 요즘 너무 무기력한거 같아   \n",
       "9         10     Chatbot                     무기력할 때는 몸을 움직여야 해요   \n",
       "10        11        User                         심리적으로도 지친 거 같아   \n",
       "11        12     Chatbot        많이 지치신게 느껴지는거 같아요. 일이 많이 힘든가요 ?   \n",
       "12        13        User                   일이 너무 많아서 잠도 너무 못 자    \n",
       "13        14     Chatbot  그럴때는 잠시 마음의 여유를 가지고 쉬었다 가는 것도 좋은거 같아요   \n",
       "14        15        User                    그래야겠어, 종종 산책하러 다녀야지   \n",
       "15        16     Chatbot                                  산책 좋죠   \n",
       "16        17        User                               들어줘서 고마워   \n",
       "17        18     Chatbot                           네 ! 다음에 또 봐요   \n",
       "\n",
       "                                           translated  \n",
       "0                                                 hey  \n",
       "1                             Hello, nice to meet you  \n",
       "2                                 what is your name ?  \n",
       "3             I am a psychological counseling chatbot  \n",
       "4                                              Ah-huh  \n",
       "5                                        good morning  \n",
       "6                              I'm so depressed today  \n",
       "7                                       What's wrong?  \n",
       "8                 I just feel so lethargic these days  \n",
       "9   When you're lethargic, you have to move your body  \n",
       "10             I think he's psychologically exhausted  \n",
       "11  I think you're really tired Is your work very ...  \n",
       "12   I can't sleep because I have too much work to do  \n",
       "13      I think it's good to take a break for a while  \n",
       "14      I should I'll go for a walk from time to time  \n",
       "15                              Taking a walk is good  \n",
       "16                               Thanks for listening  \n",
       "17                             Yes! See you next time  "
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(data_path, 'conv_translated.pickle'), 'rb') as f:\n",
    "    conv = pickle.load(f)\n",
    "\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "b3b06b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User</td>\n",
       "      <td>hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>Hello, nice to meet you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User</td>\n",
       "      <td>what is your name ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>I am a psychological counseling chatbot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User</td>\n",
       "      <td>Ah-huh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speaker                                     text\n",
       "0     User                                      hey\n",
       "1  Chatbot                  Hello, nice to meet you\n",
       "2     User                      what is your name ?\n",
       "3  Chatbot  I am a psychological counseling chatbot\n",
       "4     User                                   Ah-huh"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = conv[['speaker_idx', 'translated']]\n",
    "conv.columns = ['speaker', 'text']\n",
    "conv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "7797aa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'Hello, nice to meet you',\n",
       " 'what is your name ?',\n",
       " 'I am a psychological counseling chatbot',\n",
       " 'Ah-huh',\n",
       " 'good morning',\n",
       " \"I'm so depressed today\",\n",
       " \"What's wrong?\",\n",
       " 'I just feel so lethargic these days',\n",
       " \"When you're lethargic, you have to move your body\",\n",
       " \"I think he's psychologically exhausted\",\n",
       " \"I think you're really tired Is your work very hard?\",\n",
       " \"I can't sleep because I have too much work to do\",\n",
       " \"I think it's good to take a break for a while\",\n",
       " \"I should I'll go for a walk from time to time\",\n",
       " 'Taking a walk is good',\n",
       " 'Thanks for listening',\n",
       " 'Yes! See you next time']"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_list = conv.text.values.tolist()\n",
    "conv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "71939437",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_list = conv.text.values.tolist()\n",
    "\n",
    "conv_result = [] \n",
    "for text in conv_list: \n",
    "    conv_dataset = test_processor.convert_sentence(text)\n",
    "    conv_sampler = test_processor.shuffle_data(conv_dataset, 'test')\n",
    "    conv_dataloader = test_processor.load_data(conv_dataset, conv_sampler)\n",
    "    y_pred, y_true = bert_tester.get_label(conv_dataloader, 1)\n",
    "    conv_result.append(int(y_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d10f41e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10172/2549851626.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conv['predict'] = conv_result\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User</td>\n",
       "      <td>hey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>Hello, nice to meet you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User</td>\n",
       "      <td>what is your name ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>I am a psychological counseling chatbot</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User</td>\n",
       "      <td>Ah-huh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>good morning</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>User</td>\n",
       "      <td>I'm so depressed today</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>What's wrong?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>User</td>\n",
       "      <td>I just feel so lethargic these days</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>When you're lethargic, you have to move your body</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>User</td>\n",
       "      <td>I think he's psychologically exhausted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>I think you're really tired Is your work very ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>User</td>\n",
       "      <td>I can't sleep because I have too much work to do</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>I think it's good to take a break for a while</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>User</td>\n",
       "      <td>I should I'll go for a walk from time to time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>Taking a walk is good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>User</td>\n",
       "      <td>Thanks for listening</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>Yes! See you next time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    speaker                                               text  predict\n",
       "0      User                                                hey        0\n",
       "1   Chatbot                            Hello, nice to meet you        0\n",
       "2      User                                what is your name ?        0\n",
       "3   Chatbot            I am a psychological counseling chatbot        4\n",
       "4      User                                             Ah-huh        0\n",
       "5   Chatbot                                       good morning        0\n",
       "6      User                             I'm so depressed today       10\n",
       "7   Chatbot                                      What's wrong?        0\n",
       "8      User                I just feel so lethargic these days        7\n",
       "9   Chatbot  When you're lethargic, you have to move your body        0\n",
       "10     User             I think he's psychologically exhausted        0\n",
       "11  Chatbot  I think you're really tired Is your work very ...        0\n",
       "12     User   I can't sleep because I have too much work to do        0\n",
       "13  Chatbot      I think it's good to take a break for a while        0\n",
       "14     User      I should I'll go for a walk from time to time        0\n",
       "15  Chatbot                              Taking a walk is good        0\n",
       "16     User                               Thanks for listening        0\n",
       "17  Chatbot                             Yes! See you next time        0"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv['predict'] = conv_result\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f7426eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sent = \"I'm very ok\" # X_test.text[0]\n",
    "test_data = test_processor.convert_sentence(test_sent)\n",
    "test_sampler = test_processor.shuffle_data(test_data, 'test')\n",
    "test_loader = test_processor.load_data(test_data, test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1ede64ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.026226647]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_true = bert_tester.get_label(test_loader, 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18b783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
