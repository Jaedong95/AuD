{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847e574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 16:51:34.734869: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-25 16:51:34.930647: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-25 16:51:34.974166: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-25 16:51:35.574098: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-25 16:51:35.574236: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-25 16:51:35.574246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import pymysql\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import R2Score\n",
    "from torch import nn\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from attrdict import AttrDict\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score , recall_score , confusion_matrix, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3066743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/lamda_00/Depression_paper/data/\"\n",
    "model_path = \"/home/lamda_00/Depression_paper/model/\"\n",
    "ckpt_path = \"/home/lamda_00/Depression_paper/ckpt/\"\n",
    "config_path = \"/home/lamda_00/Depression_paper/config/\"\n",
    "log_path = \"/home/lamda_00/Depression_paper/log/\"\n",
    "config_file = \"bert-base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4191310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '우울',\n",
       " 1: '무기력',\n",
       " 2: '급격한 체중(식욕)변화',\n",
       " 3: '수면장애',\n",
       " 4: '정서불안',\n",
       " 5: '피로',\n",
       " 6: '과도한 죄책감 및 무가치함',\n",
       " 7: '인지기능저하',\n",
       " 8: '자살충동',\n",
       " 9: '일상'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = dict()\n",
    "\n",
    "label[0] = '우울'\n",
    "label[1] = '무기력'\n",
    "label[2] = '급격한 체중(식욕)변화'\n",
    "label[3] = '수면장애'\n",
    "label[4] = '정서불안'\n",
    "label[5] = '피로'\n",
    "label[6] = '과도한 죄책감 및 무가치함'\n",
    "label[7] = '인지기능저하'\n",
    "label[8] = '자살충동'\n",
    "label[9] = '일상'\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cf4d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User</td>\n",
       "      <td>hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>Hello, nice to meet you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User</td>\n",
       "      <td>what is your name ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chatbot</td>\n",
       "      <td>I am a psychological counseling chatbot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User</td>\n",
       "      <td>Ah-huh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speaker                                     text\n",
       "0     User                                      hey\n",
       "1  Chatbot                  Hello, nice to meet you\n",
       "2     User                      what is your name ?\n",
       "3  Chatbot  I am a psychological counseling chatbot\n",
       "4     User                                   Ah-huh"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(data_path, 'conv_translated.pickle'), 'rb') as f:\n",
    "    conv = pickle.load(f)\n",
    "    \n",
    "conv = conv[['speaker_idx', 'translated']]\n",
    "conv.columns = ['speaker', 'text']\n",
    "conv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3efcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.loc[6].text = 'I am so depressed today'\n",
    "conv.loc[12]. text = 'I can not sleep because I have too much work to do'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc6b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path, 'training_config.json')) as f:\n",
    "    training_config = AttrDict(json.load(f))\n",
    "    \n",
    "training_config.pad = 'max_length'\n",
    "training_config.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8dc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = data_file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.label)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # def clear_text(self)  => 전처리 코드를 여기에 넣을 경우 상당히 느려짐\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return text, label\n",
    "        '''\n",
    "        self.reset_index()\n",
    "        text = self.data.text[idx]\n",
    "        label = self.data.label[idx]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12fdc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, config, model):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.model = model\n",
    "        self.linear = nn.Linear(config.hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.linear(logits)\n",
    "        x = self.relu(x)\n",
    "        score = self.out(x)\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232d2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertProcessor():\n",
    "    def __init__(self, training_config, tokenizer, truncation=True):\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = 32\n",
    "        self.pad = training_config.pad\n",
    "        self.batch_size = training_config.train_batch_size\n",
    "        self.truncation = truncation\n",
    "    \n",
    "    def convert_data(self, data_file):\n",
    "        context2 = None    # single sentence classification\n",
    "\n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(data_file[idx][0], context2) for idx in range(len(data_file))],   # text, \n",
    "            max_length = self.max_len,\n",
    "            padding = self.pad,\n",
    "            truncation = self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        for i in range(len(data_file)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "            try:\n",
    "                inputs['label'] = data_file[i][1] \n",
    "            except:\n",
    "                inputs['label'] = 0 \n",
    "            features.append(inputs)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        all_attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        all_token_type_ids = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        all_labels = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "\n",
    "        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "        return dataset\n",
    "    \n",
    "    def convert_sentence(self, sent_list):   # 사용자 입력 문장 1개 -> 입력 형태 변환\n",
    "        context2 = None \n",
    "        batch_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [(sent_list, context2)], max_length=self.max_len, padding=self.pad, truncation=self.truncation\n",
    "        )\n",
    "        \n",
    "        features = []\n",
    "        inputs = {k: batch_encoding[k][0] for k in batch_encoding}\n",
    "        inputs['label'] = 0 \n",
    "        features.append(inputs)\n",
    "\n",
    "        input_id = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "        input_am = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "        input_tts = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "        input_lb = torch.tensor([f['label'] for f in features], dtype=torch.long)\n",
    "        dataset = TensorDataset(input_id, input_am, input_tts, input_lb)\n",
    "        return dataset\n",
    "    \n",
    "    def shuffle_data(self, dataset, data_type):\n",
    "        if data_type == 'train':\n",
    "            return RandomSampler(dataset)\n",
    "        elif data_type == 'eval' or data_type == 'test':\n",
    "            return SequentialSampler(dataset)\n",
    "        \n",
    "    def load_data(self, dataset, sampler):\n",
    "        return DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b241620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegTester():\n",
    "    def __init__(self, training_config, model):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "\n",
    "    def get_label(self, test_dataloader, test_type):\n",
    "        '''\n",
    "        test_type: 0  -> Test dataset \n",
    "        test_type: 1  -> Test sentence\n",
    "        '''\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            self.model.eval()   # self 안 붙이면 이상한 Output (BaseModelOutputWithPoolingAndCrossAttentions) 출력 \n",
    "            batch = tuple(t.to(training_config.device) for t in batch)   # args.device: cuda \n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                if test_type == 0:\n",
    "                    preds.extend(outputs.squeeze().detach().cpu().numpy())\n",
    "                elif test_type == 1:\n",
    "                    preds.extend(outputs[0].detach().cpu().numpy())            \n",
    "            label = batch[3].detach().cpu().numpy()\n",
    "            labels.extend(label)\n",
    "        return preds, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45714089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClsTester():\n",
    "    def __init__(self, training_config, model):\n",
    "        self.training_config = training_config\n",
    "        self.model = model\n",
    "\n",
    "    def get_label(self, test_dataloader, test_type):\n",
    "        '''\n",
    "        test_type: 0  -> Test dataset \n",
    "        test_type: 1  -> Test sentence\n",
    "        '''\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            self.model.eval()\n",
    "            batch = tuple(t.to(self.training_config.device) for t in batch)   # args.device: cuda \n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"labels\": batch[3]\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                test_loss, logits = outputs[:2] \n",
    "                pred = logits.detach().cpu().numpy()\n",
    "                if test_type == 0:\n",
    "                    preds.extend(np.argmax(pred, axis=1))\n",
    "                elif test_type == 1:\n",
    "                    preds.append(np.argmax(pred))  \n",
    "                            \n",
    "            label = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            labels.extend(label)\n",
    "  \n",
    "        return preds, labels \n",
    "    \n",
    "    def get_f1_score(self, test_dataloader):\n",
    "        y_pred, y_true = self.get_label(test_dataloader)\n",
    "        return round(f1_score(y_true, y_pred, average='micro'), 3) \n",
    "     \n",
    "    def get_cl_report(self, test_dataloader):\n",
    "        y_pred, y_true = self.get_label(test_dataloader, 0)\n",
    "        cr = classification_report(y_true, y_pred).split('\\n')\n",
    "        clr_df = []\n",
    "\n",
    "        for idx, line in enumerate(cr):\n",
    "            clr_df.append([])\n",
    "            if line == '':\n",
    "                continue\n",
    "\n",
    "            word_list = line.strip().split(' ')\n",
    "\n",
    "            for word in word_list:\n",
    "                if word != '':\n",
    "                    clr_df[idx].append(word)\n",
    "\n",
    "        clr_df[-2][0] = ' '.join([clr_df[-2][0], clr_df[-2][1]])\n",
    "        clr_df[-3][0] = ' '.join([clr_df[-3][0], clr_df[-3][1]])\n",
    "        clr_df[-4].insert(1, ' ')\n",
    "        clr_df[-4].insert(2, ' ')\n",
    "        clr_df[0].insert(0, 'index')\n",
    "\n",
    "        clr_df[-2].pop(1)\n",
    "        clr_df[-3].pop(1)\n",
    "        clr_df.pop(1)\n",
    "        clr_df.pop(-1)\n",
    "        clr_df.pop(-4)\n",
    "        clr_df = pd.DataFrame(clr_df[1:], columns=clr_df[0])\n",
    "        clr_df.index = clr_df['index']\n",
    "\n",
    "        del clr_df['index']\n",
    "        return clr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec1066",
   "metadata": {},
   "source": [
    "#### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "3112e4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_kor</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i've tried to ignore my feelings but i really ...</td>\n",
       "      <td>나는 내 감정을 무시하려고 노력했지만 정말 우울하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to about ish and my mothers just been getting ...</td>\n",
       "      <td>나는 그녀에게 우울증이 있다고 말했고 그녀는 내가 더 일찍 일어나야 한다고 말했고 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at the apex we feel something at the trough we...</td>\n",
       "      <td>정점에서 우리는 무언가를 느낀다 우리는 우울하게 느끼는 무언가를 느낀다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  i've tried to ignore my feelings but i really ...   \n",
       "1  to about ish and my mothers just been getting ...   \n",
       "2  at the apex we feel something at the trough we...   \n",
       "\n",
       "                                            text_kor  label  \n",
       "0                       나는 내 감정을 무시하려고 노력했지만 정말 우울하다      0  \n",
       "1  나는 그녀에게 우울증이 있다고 말했고 그녀는 내가 더 일찍 일어나야 한다고 말했고 ...      0  \n",
       "2            정점에서 우리는 무언가를 느낀다 우리는 우울하게 느끼는 무언가를 느낀다      0  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(os.path.join(data_path, 'bws_bin_samp_test.csv'))\n",
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "871f1465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/lamda_00/Depression_paper/model/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(model_path, 'bert-base'), model_max_length=32)\n",
    "config = BertConfig.from_pretrained(os.path.join(model_path, 'bert-base', 'bert_config.json'))\n",
    "model = BertModel.from_pretrained(os.path.join(model_path, 'bert-base'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "90c3eaca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_processor = BertProcessor(training_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "a760b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = BertDataset(X_test)\n",
    "test_dataset = test_processor.convert_data(test_file)\n",
    "test_sampler = test_processor.shuffle_data(test_dataset, 'test')\n",
    "test_dataloader = test_processor.load_data(test_dataset, test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "2d379bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "62a582b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = BertRegressor(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "0b64a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.path.join(model_path, 'bws', 'bert_bin_samp_base.pt')\n",
    "# model_name = os.path.join(model_path, 'bws', 'bert_bws_medium.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "58fcdea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertRegressor(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reg.load_state_dict(torch.load(model_name))\n",
    "model_reg.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "ef60fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tester = BertRegTester(training_config, model_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "9e2912c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소요 시간: 0.11694741249084473\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "y_pred, y_true = bert_tester.get_label(test_dataloader, 0)\n",
    "print(f'소요 시간: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "0307f230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_kor</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i've tried to ignore my feelings but i really ...</td>\n",
       "      <td>나는 내 감정을 무시하려고 노력했지만 정말 우울하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to about ish and my mothers just been getting ...</td>\n",
       "      <td>나는 그녀에게 우울증이 있다고 말했고 그녀는 내가 더 일찍 일어나야 한다고 말했고 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at the apex we feel something at the trough we...</td>\n",
       "      <td>정점에서 우리는 무언가를 느낀다 우리는 우울하게 느끼는 무언가를 느낀다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i don't think i am depressed or suicidal but v...</td>\n",
       "      <td>나는 내가 우울하거나 자살했다고 생각하지 않지만 매우 매우 지루하고 혼자라고 생각한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>now that my ankle is healed though i feel depr...</td>\n",
       "      <td>다시 우울해지긴 했지만 발목이 다 나았으니</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>upon receiving the news each time i've been un...</td>\n",
       "      <td>매번 그 소식을 듣자마자 나는 울거나 슬퍼할 수 없었다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Why are you doing this when you're not even dr...</td>\n",
       "      <td>술도 안 취했는데 왜 이러는 거야?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>whenever i feel lonely i go here only to reali...</td>\n",
       "      <td>내가 외로울 때마다 나는 내가 아니라는 것을 깨달았을 뿐이다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>During the training, I was able to get my grad...</td>\n",
       "      <td>훈련 기간 동안, 나는 5등까지 성적을 올릴 수 있었다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>i just feel lonely and unwanteda little help p...</td>\n",
       "      <td>나는 단지 외롭고 원하지 않는 도움을 조금만 주세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    i've tried to ignore my feelings but i really ...   \n",
       "1    to about ish and my mothers just been getting ...   \n",
       "2    at the apex we feel something at the trough we...   \n",
       "3    i don't think i am depressed or suicidal but v...   \n",
       "4    now that my ankle is healed though i feel depr...   \n",
       "..                                                 ...   \n",
       "315  upon receiving the news each time i've been un...   \n",
       "316  Why are you doing this when you're not even dr...   \n",
       "317  whenever i feel lonely i go here only to reali...   \n",
       "318  During the training, I was able to get my grad...   \n",
       "319  i just feel lonely and unwanteda little help p...   \n",
       "\n",
       "                                              text_kor  label  \n",
       "0                         나는 내 감정을 무시하려고 노력했지만 정말 우울하다      0  \n",
       "1    나는 그녀에게 우울증이 있다고 말했고 그녀는 내가 더 일찍 일어나야 한다고 말했고 ...      0  \n",
       "2              정점에서 우리는 무언가를 느낀다 우리는 우울하게 느끼는 무언가를 느낀다      0  \n",
       "3      나는 내가 우울하거나 자살했다고 생각하지 않지만 매우 매우 지루하고 혼자라고 생각한다      0  \n",
       "4                              다시 우울해지긴 했지만 발목이 다 나았으니      0  \n",
       "..                                                 ...    ...  \n",
       "315                     매번 그 소식을 듣자마자 나는 울거나 슬퍼할 수 없었다      0  \n",
       "316                                술도 안 취했는데 왜 이러는 거야?      1  \n",
       "317                  내가 외로울 때마다 나는 내가 아니라는 것을 깨달았을 뿐이다      0  \n",
       "318                     훈련 기간 동안, 나는 5등까지 성적을 올릴 수 있었다      1  \n",
       "319                       나는 단지 외롭고 원하지 않는 도움을 조금만 주세요      0  \n",
       "\n",
       "[320 rows x 3 columns]"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "460bc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "c7aa06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "f673061c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0568)"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(torch.Tensor(y_pred), torch.Tensor((y_true)))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "da8ed2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9811)"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2score = R2Score()\n",
    "r2score(torch.Tensor(y_pred), torch.Tensor(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "7645c414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at the apex we feel something at the trough we feel depressed'"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "c8dd8aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.004119586,\n",
       "  -0.0010512322,\n",
       "  -0.004600115,\n",
       "  -0.004085593,\n",
       "  -0.004511263,\n",
       "  1.0184597,\n",
       "  1.0187069,\n",
       "  -0.0044231527,\n",
       "  -0.004274998,\n",
       "  -0.004575271],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10], y_true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "ad9677a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = \"I am so depressed today\" # X_test.text[0]\n",
    "test_data = test_processor.convert_sentence(test_sent)\n",
    "test_sampler = test_processor.shuffle_data(test_data, 'test')\n",
    "test_loader = test_processor.load_data(test_data, test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "4f19c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소요 시간: 0.013761758804321289\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "y_pred, y_true = bert_tester.get_label(test_loader, 1)\n",
    "print(f'소요 시간: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "c79112e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0036349222]"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "a3b4f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2132/863694454.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_conv['label'] = 9999\n"
     ]
    }
   ],
   "source": [
    "user_conv = conv[conv.speaker=='User']\n",
    "user_conv['label'] = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "19428cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_file = BertDataset(user_conv)\n",
    "conv_dataset = test_processor.convert_data(conv_file)\n",
    "conv_sampler = test_processor.shuffle_data(conv_dataset, 'test')\n",
    "conv_dataloader = test_processor.load_data(conv_dataset, conv_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "19f71f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = bert_tester.get_label(conv_dataloader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "df3108f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2132/3963311452.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_conv['label'] = y_pred\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User</td>\n",
       "      <td>hey</td>\n",
       "      <td>1.024590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User</td>\n",
       "      <td>what is your name ?</td>\n",
       "      <td>1.021050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User</td>\n",
       "      <td>Ah-huh</td>\n",
       "      <td>1.019895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User</td>\n",
       "      <td>I am so depressed today</td>\n",
       "      <td>-0.003669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User</td>\n",
       "      <td>I just feel so lethargic these days</td>\n",
       "      <td>0.998461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>User</td>\n",
       "      <td>I think he's psychologically exhausted</td>\n",
       "      <td>1.020808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>User</td>\n",
       "      <td>I can not sleep because I have too much work t...</td>\n",
       "      <td>1.014418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>User</td>\n",
       "      <td>I should I'll go for a walk from time to time</td>\n",
       "      <td>1.023432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>User</td>\n",
       "      <td>Thanks for listening</td>\n",
       "      <td>1.022637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker                                               text     label\n",
       "0    User                                                hey  1.024590\n",
       "1    User                                what is your name ?  1.021050\n",
       "2    User                                             Ah-huh  1.019895\n",
       "3    User                            I am so depressed today -0.003669\n",
       "4    User                I just feel so lethargic these days  0.998461\n",
       "5    User             I think he's psychologically exhausted  1.020808\n",
       "6    User  I can not sleep because I have too much work t...  1.014418\n",
       "7    User      I should I'll go for a walk from time to time  1.023432\n",
       "8    User                               Thanks for listening  1.022637"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_conv['label'] = y_pred\n",
    "user_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3be600",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(model_path, 'bert-mini'), model_max_length=32)\n",
    "config = BertConfig.from_pretrained(os.path.join(model_path, 'bert-mini', 'bert_config.json'))\n",
    "model = BertModel.from_pretrained(os.path.join(model_path, 'bert-mini'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c86fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = ['hey', 'I feel very depressed', 'I feed depressed', 'I feel sad', 'I cry everyday', 'I went to school', 'I want to die']\n",
    "dep_test = pd.DataFrame(conv, columns=['text'])\n",
    "dep_test['label'] = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add98898",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = BertDataset(dep_test)\n",
    "test_dataset = test_processor.convert_data(test_file)\n",
    "test_sampler = test_processor.shuffle_data(test_dataset, 'test')\n",
    "test_dataloader = test_processor.load_data(test_dataset, test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa17734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = BertRegressor(config, model)\n",
    "# model_reg2 = BertRegressor(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed59467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.path.join(model_path, 'bert_bws_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg.load_state_dict(torch.load(model_name))\n",
    "model_reg.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tester = BertTester(training_config, model_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = bert_tester.get_label(test_dataloader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_test['label'] = [pred for pred in y_pred]\n",
    "dep_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70831977",
   "metadata": {},
   "source": [
    "#### Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb7569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4fws4y</td>\n",
       "      <td>anyways, the doctor says be careful because yo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_23p0b1</td>\n",
       "      <td>i also have [aka *amerge*] for migraines, but ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26twx8</td>\n",
       "      <td>all in all i feel worthless pretty much the on...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  label\n",
       "0     4fws4y  anyways, the doctor says be careful because yo...      3\n",
       "1  t3_23p0b1  i also have [aka *amerge*] for migraines, but ...      3\n",
       "2     26twx8  all in all i feel worthless pretty much the on...      8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(os.path.join(data_path, 'dsm_samp2_test.csv'))\n",
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6181089b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 0, 6, 1, 2, 7, 5, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06192c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/lamda_00/Depression_paper/model/bert-medium were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/lamda_00/Depression_paper/model/bert-medium and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(os.path.join(model_path, 'bert-medium'), model_max_length=32)\n",
    "config = BertConfig.from_pretrained(os.path.join(model_path, 'bert-medium', 'bert_config.json'), num_labels=9, output_hidden_states=True)\n",
    "model = BertForSequenceClassification.from_pretrained(os.path.join(model_path, 'bert-medium'), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a23aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb8a0936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(config_path, 'training_config.json')) as f:\n",
    "    training_config = AttrDict(json.load(f))\n",
    "\n",
    "training_config.pad = 'max_length'\n",
    "training_config.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7e81577",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processor = BertProcessor(training_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "424aac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = BertDataset(X_test)\n",
    "test_dataset = test_processor.convert_data(test_file)\n",
    "test_sampler = test_processor.shuffle_data(test_dataset, 'test')\n",
    "test_dataloader = test_processor.load_data(test_dataset, test_sampler)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f90d96c",
   "metadata": {},
   "source": [
    "# model_name = os.path.join(model_path, 'label_10', 'bert_tiny', 'bert_dsm_6.pt')\n",
    "# model_name = os.path.join(model_path, 'label_10', 'bert_mini', 'bert_dsm_5.pt')\n",
    "# model_name = os.path.join(model_path, 'label_10', 'bert_small', 'bert_dsm_2.pt')\n",
    "# model_name = os.path.join(model_path, 'label_10', 'bert_medium', 'bert_dsm_2.pt')\n",
    "model_name = os.path.join(model_path, 'label_10', 'bert_base', 'bert_dsm_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14be7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = os.path.join(model_path, 'label_9', 'bert_tiny', 'bert_dsm_4.pt')\n",
    "# model_name = os.path.join(model_path, 'label_9', 'bert_mini', 'bert_dsm_3.pt')\n",
    "# model_name = os.path.join(model_path, 'label_9', 'bert_small', 'bert_dsm_2.pt')\n",
    "model_name = os.path.join(model_path, 'label_9', 'bert_medium', 'bert_dsm_1.pt')\n",
    "# model_name = os.path.join(model_path, 'label_9', 'bert_base', 'bert_dsm_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "552b3ae2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.to(training_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d33c9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_samp = X_test.copy()\n",
    "dsm_samp = dsm_samp.sample(10)\n",
    "dsm_samp.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a0bc945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13550/863001631.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(encoded['input_ids']).to(training_config.device)\n",
      "/tmp/ipykernel_13550/863001631.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attn_mask = torch.tensor(encoded['attention_mask']).to(training_config.device)\n",
      "/tmp/ipykernel_13550/863001631.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  token_type_ids = torch.tensor(encoded['token_type_ids']).to(training_config.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.6444,  0.4148, -1.0298,  ..., -0.2488, -0.1934, -0.2653],\n",
      "         [-1.0660, -1.3672,  1.2306,  ...,  0.3471,  1.2565, -0.5600],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0273,  0.0215, -0.0125,  ..., -0.0173, -0.7817, -0.4272],\n",
      "         [-0.8866, -0.6256, -0.7915,  ...,  0.9680,  0.4882,  0.2822],\n",
      "         [-0.8618, -2.0142,  1.3832,  ...,  0.8132,  0.8005, -0.4759],\n",
      "         ...,\n",
      "         [ 1.7118,  1.1357, -0.5227,  ..., -1.0272,  0.3860,  0.3627],\n",
      "         [ 1.5217,  0.9647, -0.5361,  ..., -0.9806,  0.1686,  0.4496],\n",
      "         [ 1.3314,  1.0795, -0.7452,  ..., -0.9855,  0.1476,  0.6717]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2980,  0.0535,  0.0362,  ...,  0.2940, -0.4399, -0.3472],\n",
      "         [-0.3462, -0.5374, -0.4300,  ...,  1.3548,  0.0689,  0.1897],\n",
      "         [-0.3477, -1.4788,  0.4093,  ...,  1.1611,  1.0304, -0.9574],\n",
      "         ...,\n",
      "         [ 0.8865,  1.2726, -0.0369,  ..., -0.2155,  0.1668,  0.1706],\n",
      "         [ 0.9956,  1.0256,  0.0042,  ...,  0.0918,  0.6477,  0.4291],\n",
      "         [ 0.8124,  1.1929, -0.4240,  ..., -0.1208,  0.4573,  0.8000]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3421, -0.3798,  0.1402,  ...,  0.5140, -0.3002, -0.1938],\n",
      "         [-0.0797, -0.9348,  0.0139,  ...,  1.0752,  0.2480, -0.0516],\n",
      "         [-0.4384, -1.5363,  0.9440,  ...,  1.5061,  0.7790, -0.6474],\n",
      "         ...,\n",
      "         [ 1.0866,  0.1171,  0.0649,  ...,  0.0492, -0.1728, -0.0234],\n",
      "         [ 1.0008, -0.0123,  0.1579,  ...,  0.2378,  0.1685,  0.3397],\n",
      "         [ 0.7828,  0.2837, -0.2719,  ...,  0.0687,  0.0824,  0.5007]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3966, -0.6155,  0.0538,  ...,  0.8690,  0.4278, -0.3368],\n",
      "         [ 0.2023, -0.8126, -0.2996,  ...,  0.9808,  0.8208, -0.1319],\n",
      "         [ 0.1808, -1.3128,  0.2041,  ...,  1.6156,  0.8659, -0.5496],\n",
      "         ...,\n",
      "         [ 1.0543, -0.1681,  0.0542,  ...,  0.2889,  0.5670, -0.0441],\n",
      "         [ 0.9002, -0.3612, -0.1159,  ...,  0.3628,  0.7042,  0.2413],\n",
      "         [ 0.8007, -0.1659, -0.3588,  ...,  0.3053,  0.6956,  0.3562]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.9211, -0.0024,  0.8275,  ...,  1.1737,  0.8186, -0.2118],\n",
      "         [ 0.7763, -0.5934,  0.2018,  ...,  1.6265,  1.6670, -0.0271],\n",
      "         [ 0.6455, -0.5301,  0.7799,  ...,  2.1616,  1.4717, -0.4497],\n",
      "         ...,\n",
      "         [ 1.3371,  0.2274,  0.4576,  ...,  0.8630,  1.0030, -0.0975],\n",
      "         [ 1.2130, -0.0060,  0.2004,  ...,  0.9489,  1.1184,  0.0926],\n",
      "         [ 1.2396,  0.1151, -0.0410,  ...,  0.6992,  1.1101,  0.1113]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3695,  0.2681,  0.9746,  ...,  1.4415,  1.1903, -0.2062],\n",
      "         [ 0.3053, -0.4921,  0.3177,  ...,  1.9914,  2.0171,  0.0306],\n",
      "         [ 0.0515, -0.3366,  0.5138,  ...,  2.5053,  1.9328, -0.1817],\n",
      "         ...,\n",
      "         [ 0.4763,  0.2508,  0.8677,  ...,  1.2887,  1.7699, -0.1115],\n",
      "         [ 0.4538,  0.1687,  0.7311,  ...,  1.3191,  1.8041,  0.0698],\n",
      "         [ 0.4273,  0.1941,  0.4916,  ...,  1.1678,  1.7896,  0.0925]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.4439,  1.0011,  1.2005,  ...,  0.9921,  0.3674, -0.7525],\n",
      "         [-0.2426,  0.0370,  0.7953,  ...,  1.5317,  1.4490, -0.0382],\n",
      "         [-0.1311,  0.2035,  0.8864,  ...,  2.2946,  1.2595, -0.4546],\n",
      "         ...,\n",
      "         [-0.3956,  0.8776,  1.0720,  ...,  1.0181,  0.8312, -0.8930],\n",
      "         [-0.3743,  0.8476,  1.0787,  ...,  1.0395,  0.8171, -0.7668],\n",
      "         [-0.3747,  0.7512,  0.9131,  ...,  0.9971,  0.8199, -0.7333]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0712,  0.5363,  0.8418,  ...,  0.7045,  0.6411, -0.5894],\n",
      "         [-0.0938,  0.1436,  1.0261,  ...,  0.9658,  0.9144, -0.5510],\n",
      "         [-0.1143,  0.2194,  1.1378,  ...,  1.2367,  0.8051, -0.7174],\n",
      "         ...,\n",
      "         [-0.0452,  0.4493,  0.9008,  ...,  0.7144,  0.6864, -0.6306],\n",
      "         [-0.0405,  0.4402,  0.8955,  ...,  0.7136,  0.6964, -0.6028],\n",
      "         [-0.0244,  0.4104,  0.8712,  ...,  0.7171,  0.6890, -0.5900]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-3.4882e-01, -7.6914e-02,  2.9806e-01,  ...,  1.1860e-01,\n",
      "          -8.8648e-01, -2.5647e-01],\n",
      "         [-3.3077e-01, -5.3189e-01, -1.4962e-01,  ..., -6.5283e-01,\n",
      "          -7.1838e-01,  7.5094e-01],\n",
      "         [-4.6020e-01,  9.9113e-01,  1.0127e+00,  ..., -7.0199e-02,\n",
      "           1.1084e-04, -3.3134e-01],\n",
      "         ...,\n",
      "         [ 1.2903e+00,  1.2293e+00, -6.7505e-01,  ..., -7.2491e-01,\n",
      "           5.9795e-01,  8.2136e-01],\n",
      "         [ 1.1868e+00,  1.0863e+00, -7.6372e-01,  ..., -6.5045e-01,\n",
      "           4.2206e-01,  9.2617e-01],\n",
      "         [ 1.0921e+00,  1.3333e+00, -9.0595e-01,  ..., -5.5951e-01,\n",
      "           3.5066e-01,  1.0133e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0348, -0.1298,  0.0550,  ..., -0.0604, -0.8443, -0.5622],\n",
      "         [-0.1838, -0.6918, -0.2870,  ..., -0.1604, -0.6098,  0.4861],\n",
      "         [-0.9030,  0.6939,  1.1888,  ...,  0.2878,  0.0702, -0.8274],\n",
      "         ...,\n",
      "         [ 1.3633,  1.0809, -0.4315,  ..., -0.7147,  0.5402,  0.3511],\n",
      "         [ 1.2492,  1.0739, -0.5747,  ..., -0.6772,  0.3603,  0.5139],\n",
      "         [ 0.9916,  1.3429, -0.8434,  ..., -0.7070,  0.3428,  0.6992]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3024, -0.0654,  0.0338,  ..., -0.2578, -0.6595, -0.4904],\n",
      "         [ 0.2112,  0.1218,  0.0272,  ..., -0.2745, -0.5619,  0.6175],\n",
      "         [-0.5093,  0.4874,  0.7676,  ...,  0.2865, -0.0017, -1.4087],\n",
      "         ...,\n",
      "         [ 1.1193,  1.1887, -0.5715,  ..., -0.7025,  0.2495,  0.0984],\n",
      "         [ 0.9791,  1.3018, -0.8170,  ..., -0.6334,  0.1043,  0.0485],\n",
      "         [ 0.6845,  1.1898, -0.9277,  ..., -0.4338,  0.4078,  0.3779]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0807, -0.4249,  0.2763,  ..., -0.2687, -0.4942, -0.3849],\n",
      "         [-0.0418, -0.2674,  0.2179,  ..., -0.2808, -0.2016,  0.9097],\n",
      "         [-0.5621, -0.0773,  0.8710,  ...,  0.4237,  0.3845, -1.6468],\n",
      "         ...,\n",
      "         [ 0.9241,  0.1708,  0.0065,  ..., -0.6931,  0.0962, -0.0091],\n",
      "         [ 0.7128,  0.2126, -0.2281,  ..., -0.5049, -0.0717,  0.0856],\n",
      "         [ 0.4671,  0.1352, -0.5825,  ..., -0.3959,  0.2400,  0.3527]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0940, -0.3974,  0.3720,  ...,  0.2802,  0.0477, -0.1850],\n",
      "         [-0.2016,  0.0365, -0.0661,  ...,  0.2927,  0.0511,  1.0677],\n",
      "         [-0.9196,  0.4673,  0.1658,  ...,  1.1509,  0.3930, -1.3055],\n",
      "         ...,\n",
      "         [ 0.7019,  0.1288,  0.1605,  ..., -0.3642,  0.6269,  0.1564],\n",
      "         [ 0.6146,  0.0207,  0.0395,  ..., -0.2040,  0.5101,  0.2938],\n",
      "         [ 0.4986, -0.0906, -0.3329,  ..., -0.0844,  0.5299,  0.4021]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6538,  0.4525,  1.0471,  ...,  0.7154,  0.1975,  0.0921],\n",
      "         [ 0.2099,  0.1201,  0.2172,  ...,  0.3431,  0.7302,  1.2398],\n",
      "         [-0.5178,  0.9304,  0.2478,  ...,  1.4703,  1.5990, -1.0644],\n",
      "         ...,\n",
      "         [ 0.7753,  0.6137,  0.3271,  ...,  0.1838,  0.7248,  0.4861],\n",
      "         [ 0.8314,  0.5056,  0.1570,  ...,  0.4188,  0.7837,  0.5012],\n",
      "         [ 0.5790,  0.3830, -0.2275,  ...,  0.2382,  0.7253,  0.6143]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3359,  0.4331,  1.0017,  ...,  0.9816,  0.7251, -0.0727],\n",
      "         [-0.0456,  0.2846,  0.5582,  ...,  0.9018,  1.3841,  0.7132],\n",
      "         [-0.2366,  0.7619,  0.3457,  ...,  1.3131,  2.3240, -0.7277],\n",
      "         ...,\n",
      "         [ 0.3603,  0.3791,  0.7035,  ...,  0.8156,  1.4462,  0.2411],\n",
      "         [ 0.3438,  0.2577,  0.6173,  ...,  0.9419,  1.5563,  0.2236],\n",
      "         [ 0.3144,  0.2313,  0.5521,  ...,  0.9235,  1.6135,  0.2609]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6967,  1.0618,  1.4160,  ...,  0.7664,  0.4851, -0.7743],\n",
      "         [-0.3045,  0.5810,  1.1191,  ...,  0.8092,  0.7970,  0.2889],\n",
      "         [-0.3118,  0.5704,  0.4379,  ...,  1.6460,  2.0360, -0.2381],\n",
      "         ...,\n",
      "         [-0.3517,  1.0641,  1.3127,  ...,  0.5449,  0.8099, -0.7751],\n",
      "         [-0.2923,  0.8651,  1.2685,  ...,  0.7102,  0.7946, -0.7487],\n",
      "         [-0.3451,  0.9038,  1.0583,  ...,  0.6376,  0.8344, -0.7314]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2131,  0.4862,  1.0694,  ...,  0.7077,  0.4825, -0.7651],\n",
      "         [-0.1337,  0.2330,  1.3237,  ...,  0.7421,  0.5630, -0.6387],\n",
      "         [ 0.0413,  0.5165,  0.7922,  ...,  1.4937,  0.7891, -0.8799],\n",
      "         ...,\n",
      "         [-0.1853,  0.4716,  1.1494,  ...,  0.5918,  0.5808, -0.7745],\n",
      "         [-0.1605,  0.4179,  1.1327,  ...,  0.6443,  0.5793, -0.7670],\n",
      "         [-0.1649,  0.4315,  1.0478,  ...,  0.6630,  0.5278, -0.8058]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.6552, -1.0135, -0.7807,  ...,  0.1678, -0.9383,  0.7642],\n",
      "         [-0.6730, -0.4841,  0.2378,  ...,  0.6414, -0.5319, -0.2697],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0471,  0.0229, -0.0757,  ..., -0.2312, -0.6938, -0.3849],\n",
      "         [-0.5516, -0.7237, -0.1323,  ..., -0.2980,  0.1280,  0.9637],\n",
      "         [-1.1732, -0.6915,  0.0969,  ..., -0.0274, -0.0378, -0.6139],\n",
      "         ...,\n",
      "         [ 1.1278,  0.9565, -0.2051,  ..., -0.8329,  0.3652,  0.6270],\n",
      "         [ 0.9809,  0.8910, -0.2910,  ..., -0.7702,  0.2557,  0.6684],\n",
      "         [ 0.7965,  0.9976, -0.3739,  ..., -0.8166,  0.2438,  0.7425]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4217,  0.0508, -0.1234,  ..., -0.1775, -0.5277, -0.4866],\n",
      "         [-0.1705, -0.6165,  0.1286,  ..., -0.1335,  0.0876,  0.5977],\n",
      "         [-0.6229, -0.2453, -0.2342,  ...,  0.5761, -0.3471, -1.3694],\n",
      "         ...,\n",
      "         [ 1.1435,  0.6314, -0.2517,  ..., -0.3105,  0.2555,  0.5038],\n",
      "         [ 0.9401,  0.7754, -0.4396,  ..., -0.4533,  0.1266,  0.4349],\n",
      "         [ 0.7575,  0.9782, -0.8814,  ..., -0.5213,  0.3156,  0.5596]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4698, -0.3567,  0.1923,  ...,  0.3437, -0.3753, -0.2328],\n",
      "         [ 0.0546, -1.0106, -0.0812,  ..., -0.0764,  0.2811,  0.4616],\n",
      "         [ 0.2529, -0.2085, -0.0146,  ...,  0.6440, -0.2404, -1.7143],\n",
      "         ...,\n",
      "         [ 1.1711, -0.2917, -0.2319,  ...,  0.0809,  0.0729,  0.6401],\n",
      "         [ 0.8828, -0.0085, -0.4607,  ...,  0.0651,  0.0168,  0.6505],\n",
      "         [ 0.6897,  0.2507, -0.7129,  ...,  0.0351,  0.1658,  0.7160]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7404, -0.6387, -0.0767,  ...,  0.3268, -0.1438, -0.4853],\n",
      "         [ 0.6732, -1.1113, -0.2300,  ...,  0.0451,  1.0428,  0.0773],\n",
      "         [ 0.7037,  0.3127,  0.0805,  ...,  0.5488,  0.7144, -1.4377],\n",
      "         ...,\n",
      "         [ 1.0251, -0.5063, -0.4880,  ..., -0.0476,  0.3771,  0.3114],\n",
      "         [ 0.9081, -0.4025, -0.5233,  ...,  0.0669,  0.3818,  0.3044],\n",
      "         [ 0.7759, -0.2054, -0.6596,  ...,  0.0982,  0.4871,  0.3701]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.7720,  0.6415,  0.2430,  ..., -0.5510, -0.5894, -1.3755],\n",
      "         [ 1.4385,  0.1611, -0.2275,  ...,  0.1751,  1.3261, -0.2466],\n",
      "         [ 0.4814,  1.1219,  0.1482,  ...,  0.2874,  1.1209, -1.8982],\n",
      "         ...,\n",
      "         [ 1.4857,  0.4666, -0.5279,  ..., -0.5368,  0.3516,  0.0577],\n",
      "         [ 1.4124,  0.7072, -0.7176,  ..., -0.5869,  0.2733,  0.0105],\n",
      "         [ 1.2282,  0.7677, -0.9756,  ..., -0.4924,  0.2911,  0.0666]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.3165,  0.6033, -0.2947,  ..., -0.3007, -1.3085, -1.3703],\n",
      "         [ 1.4079,  0.3178, -0.6034,  ...,  0.4823, -0.0070, -1.1837],\n",
      "         [ 0.6687,  0.9905, -0.1179,  ...,  0.1881,  0.0763, -1.9236],\n",
      "         ...,\n",
      "         [ 1.4099,  0.5685, -0.4834,  ...,  0.0145, -0.7649, -0.7307],\n",
      "         [ 1.3409,  0.8239, -0.6384,  ..., -0.1031, -0.7092, -0.7475],\n",
      "         [ 1.2559,  0.8749, -0.6619,  ...,  0.0603, -0.7912, -0.7294]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4256,  0.2532,  0.4361,  ..., -0.1031, -0.4612, -1.2831],\n",
      "         [ 1.0351, -0.2821,  0.0743,  ...,  0.4321, -0.0021, -1.3876],\n",
      "         [ 0.3850,  0.3700,  0.0408,  ...,  0.3627, -0.1973, -1.8360],\n",
      "         ...,\n",
      "         [ 0.7191,  0.0333,  0.1472,  ..., -0.0110, -0.3958, -0.9558],\n",
      "         [ 0.6636,  0.1605,  0.1236,  ..., -0.0289, -0.3846, -0.9561],\n",
      "         [ 0.5589,  0.1881,  0.0672,  ...,  0.0166, -0.3911, -0.9426]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0307, -0.3931,  0.6203,  ..., -1.0092, -0.6393, -0.5862],\n",
      "         [ 0.0013, -0.5710,  0.5964,  ..., -0.8616, -0.6126, -0.7110],\n",
      "         [ 0.0869, -0.4978,  0.5266,  ..., -0.9005, -0.7586, -0.9193],\n",
      "         ...,\n",
      "         [-0.0082, -0.3910,  0.6390,  ..., -1.0009, -0.7315, -0.5151],\n",
      "         [ 0.0037, -0.3916,  0.6035,  ..., -1.0025, -0.7185, -0.4994],\n",
      "         [-0.0095, -0.3876,  0.5920,  ..., -1.0027, -0.7206, -0.4975]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.1287, -0.2173,  0.9759,  ..., -0.3207, -0.6351,  0.3443],\n",
      "         [ 1.7385, -1.4668, -1.0770,  ..., -0.4549, -0.6464,  0.2023],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0339, -0.0157,  0.0703,  ..., -0.1486, -0.7367, -0.5286],\n",
      "         [-0.5055,  0.4211,  1.3925,  ..., -0.8838,  0.2995, -0.3386],\n",
      "         [ 2.0813, -0.5027, -0.8433,  ..., -0.9058, -0.6340, -0.4552],\n",
      "         ...,\n",
      "         [ 0.6269,  1.3096, -0.4133,  ..., -0.9684,  0.6789,  0.4194],\n",
      "         [ 0.5672,  1.2046, -0.5452,  ..., -0.9617,  0.4931,  0.6066],\n",
      "         [ 0.6879,  1.2427, -0.5972,  ..., -0.9436,  0.2421,  0.7551]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2850, -0.0765, -0.0592,  ..., -0.4735, -0.7357, -0.5884],\n",
      "         [-0.1244,  0.3191,  1.1229,  ..., -0.9867, -0.1013, -1.0013],\n",
      "         [ 1.8703, -0.4192, -0.6985,  ..., -1.1671, -1.0406, -1.0118],\n",
      "         ...,\n",
      "         [ 0.6682,  0.9640, -0.4775,  ..., -0.5791,  0.3894,  0.0211],\n",
      "         [ 0.7468,  0.9000, -0.7130,  ..., -0.7149,  0.1528,  0.1431],\n",
      "         [ 1.2784,  0.8841, -0.6508,  ..., -0.6949,  0.1246,  0.4572]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5150, -0.5218,  0.0966,  ..., -0.2080, -0.4935, -0.4447],\n",
      "         [ 0.3277,  0.0220,  0.9118,  ..., -0.9127,  0.2695, -0.5558],\n",
      "         [ 2.3130, -0.6928, -1.1149,  ..., -1.5341, -1.0303, -0.8073],\n",
      "         ...,\n",
      "         [ 0.6069, -0.1240, -0.1364,  ..., -0.2683,  0.3666,  0.0060],\n",
      "         [ 0.6728, -0.1053, -0.4675,  ..., -0.3139,  0.1803,  0.2140],\n",
      "         [ 1.1832, -0.1377, -0.4274,  ..., -0.3954,  0.1215,  0.6408]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.0154, -0.4979, -0.2627,  ..., -0.0919, -0.3437, -0.5493],\n",
      "         [ 0.6894,  0.0586,  0.2622,  ..., -0.6718,  0.4617, -0.7873],\n",
      "         [ 3.0921, -0.7916, -0.9413,  ..., -1.5749, -0.5239, -0.8090],\n",
      "         ...,\n",
      "         [ 1.0967, -0.2782, -0.3958,  ...,  0.0170,  0.3821, -0.3390],\n",
      "         [ 1.1477, -0.1932, -0.6086,  ...,  0.0742,  0.2835, -0.1218],\n",
      "         [ 1.3045, -0.2084, -0.8594,  ..., -0.0849,  0.1916,  0.3172]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.8327,  0.7205,  0.4673,  ..., -1.3435, -0.6051, -1.4640],\n",
      "         [ 1.2897,  1.6981, -0.4793,  ..., -0.9288,  0.5586, -0.4716],\n",
      "         [ 2.9793,  0.7290, -1.0662,  ..., -1.7063, -0.3615, -1.2618],\n",
      "         ...,\n",
      "         [ 1.1477,  0.5582, -0.5390,  ..., -0.9301,  0.5277, -0.6670],\n",
      "         [ 1.1502,  0.7127, -0.6050,  ..., -0.7982,  0.3590, -0.3142],\n",
      "         [ 1.4570,  0.8183, -0.8829,  ..., -0.6673,  0.1548,  0.0206]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.5521,  0.5693, -0.4082,  ..., -0.8361, -1.7683, -1.3763],\n",
      "         [ 1.2249,  1.0735, -0.9615,  ..., -0.3098, -1.0078, -1.0655],\n",
      "         [ 2.4070,  0.8299, -1.0848,  ..., -0.7306, -1.4698, -1.7144],\n",
      "         ...,\n",
      "         [ 1.2693,  0.8289, -0.7052,  ..., -0.4542, -0.8944, -0.9793],\n",
      "         [ 1.2190,  0.9791, -0.5962,  ..., -0.3904, -0.9394, -0.6670],\n",
      "         [ 1.3943,  0.9208, -0.8086,  ..., -0.2795, -1.0370, -0.3568]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2779,  0.2020,  0.3473,  ..., -0.3792, -0.5717, -1.2025],\n",
      "         [ 1.1178, -0.0657, -0.0991,  ...,  0.0159, -0.8318, -1.1422],\n",
      "         [ 1.7412, -0.0532, -0.5011,  ..., -0.2175, -1.1602, -1.6078],\n",
      "         ...,\n",
      "         [ 0.6340, -0.0151,  0.1343,  ..., -0.3150, -0.7379, -0.9351],\n",
      "         [ 0.5819, -0.0479,  0.1461,  ..., -0.2493, -0.7778, -0.7681],\n",
      "         [ 0.6238, -0.0992, -0.0018,  ..., -0.1494, -0.7566, -0.5825]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1097, -0.4822,  0.4594,  ..., -1.0231, -0.5503, -0.3749],\n",
      "         [ 0.0867, -0.5396,  0.4025,  ..., -1.0451, -0.5823, -0.5277],\n",
      "         [ 0.2430, -0.5423,  0.2928,  ..., -1.1930, -0.7416, -0.6495],\n",
      "         ...,\n",
      "         [-0.0935, -0.5389,  0.4400,  ..., -1.0266, -0.6412, -0.3914],\n",
      "         [-0.0722, -0.5290,  0.4429,  ..., -1.0409, -0.6872, -0.3822],\n",
      "         [-0.0432, -0.5471,  0.4140,  ..., -1.0275, -0.7043, -0.3566]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.3308, -0.5319, -0.1496,  ..., -0.6528, -0.7184,  0.7509],\n",
      "         [ 0.3779, -0.1668, -0.1592,  ...,  1.3547,  1.0473,  0.5138],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0615, -0.1721,  0.0395,  ..., -0.1784, -0.6773, -0.5861],\n",
      "         [-0.6137, -0.9078, -0.5530,  ..., -0.6112, -0.3425,  0.4581],\n",
      "         [ 0.0716, -1.2748,  0.5150,  ...,  1.0114,  1.4871, -0.0247],\n",
      "         ...,\n",
      "         [ 1.0667,  0.7996, -0.0177,  ..., -0.6598,  0.2769,  0.3347],\n",
      "         [ 0.9593,  0.7101, -0.0502,  ..., -0.6263,  0.1367,  0.4275],\n",
      "         [ 0.8926,  0.8860, -0.1417,  ..., -0.6423,  0.0714,  0.5484]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1834, -0.1418,  0.0341,  ..., -0.2312, -0.6791, -0.6084],\n",
      "         [-0.1400,  0.1104, -0.3793,  ..., -0.3651, -0.4962,  0.7156],\n",
      "         [-0.0606, -0.8124, -0.0295,  ...,  0.6315,  1.2580,  0.1035],\n",
      "         ...,\n",
      "         [ 0.8646,  0.7697, -0.3257,  ..., -0.2578,  0.0578,  0.2520],\n",
      "         [ 0.7568,  0.9035, -0.3181,  ..., -0.2360,  0.0391,  0.3487],\n",
      "         [ 0.4487,  0.7503, -0.4993,  ..., -0.0965,  0.0571,  0.3991]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1340, -0.4724,  0.5902,  ..., -0.2377, -0.2285, -0.5575],\n",
      "         [ 0.1340, -0.4600, -0.3734,  ..., -0.2788, -0.2497,  0.6639],\n",
      "         [-0.3693, -0.8441, -0.0923,  ...,  1.1037,  1.2517, -0.4523],\n",
      "         ...,\n",
      "         [ 1.0020, -0.1875,  0.0691,  ..., -0.2269,  0.1099,  0.2247],\n",
      "         [ 0.8594, -0.0174,  0.1563,  ..., -0.1230,  0.1123,  0.2398],\n",
      "         [ 0.5158, -0.0473, -0.0597,  ..., -0.0617,  0.1124,  0.3120]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4539, -0.5905,  0.2912,  ...,  0.4340,  0.4496, -0.4423],\n",
      "         [ 0.6085, -0.1274, -0.3761,  ...,  0.3013,  0.6599,  1.0206],\n",
      "         [ 0.1204, -0.4056,  0.3875,  ...,  0.7340,  1.8006, -0.3920],\n",
      "         ...,\n",
      "         [ 0.7516, -0.3320, -0.2704,  ..., -0.0823,  0.6053,  0.2006],\n",
      "         [ 0.6623, -0.2137, -0.1988,  ...,  0.0594,  0.6910,  0.2518],\n",
      "         [ 0.4765, -0.0877, -0.1651,  ...,  0.1079,  0.6148,  0.2930]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7594,  0.2729,  0.8678,  ...,  0.5748,  0.7119, -0.1916],\n",
      "         [ 0.8611,  0.1370, -0.1770,  ...,  0.4637,  1.1310,  1.0647],\n",
      "         [ 0.5981,  0.0252,  0.0441,  ...,  1.0379,  2.5918, -0.0379],\n",
      "         ...,\n",
      "         [ 0.8893,  0.3293, -0.0966,  ...,  0.1182,  0.9686,  0.4439],\n",
      "         [ 0.8137,  0.4814, -0.1270,  ...,  0.2051,  1.0324,  0.4577],\n",
      "         [ 0.6124,  0.6279, -0.1573,  ...,  0.2458,  0.9139,  0.4685]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 3.3098e-01,  2.8386e-01,  1.0302e+00,  ...,  1.1281e+00,\n",
      "           1.1278e+00, -3.6177e-01],\n",
      "         [ 3.0358e-01,  7.4173e-04,  3.5031e-01,  ...,  1.2700e+00,\n",
      "           1.7074e+00,  6.1128e-01],\n",
      "         [ 2.3762e-01, -1.6599e-01,  5.5008e-01,  ...,  1.4435e+00,\n",
      "           2.9396e+00,  1.8081e-01],\n",
      "         ...,\n",
      "         [ 4.8678e-01,  1.6244e-01,  4.8181e-01,  ...,  9.6640e-01,\n",
      "           1.6489e+00,  3.8247e-02],\n",
      "         [ 4.3878e-01,  2.3327e-01,  4.7174e-01,  ...,  1.0109e+00,\n",
      "           1.6746e+00,  5.6396e-02],\n",
      "         [ 2.6777e-01,  2.4643e-01,  5.0928e-01,  ...,  1.0731e+00,\n",
      "           1.6518e+00,  1.2723e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2914,  0.8227,  1.1732,  ...,  0.9332,  0.5235, -0.9827],\n",
      "         [ 0.0112,  0.3290,  1.0392,  ...,  1.0693,  1.1358,  0.0949],\n",
      "         [ 0.2573,  0.2306,  1.0717,  ...,  1.0396,  2.4013, -0.1500],\n",
      "         ...,\n",
      "         [-0.1527,  0.8341,  1.0645,  ...,  0.8755,  0.8677, -0.8404],\n",
      "         [-0.1310,  0.8193,  1.0543,  ...,  0.9070,  0.8649, -0.7947],\n",
      "         [-0.1295,  0.7796,  1.0829,  ...,  0.9245,  0.8453, -0.6276]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0271,  0.4292,  0.9906,  ...,  0.6724,  0.6116, -0.8087],\n",
      "         [ 0.0772,  0.2299,  1.1164,  ...,  0.6960,  0.7881, -0.6856],\n",
      "         [ 0.1393,  0.1462,  1.2970,  ...,  0.7053,  1.3081, -0.8604],\n",
      "         ...,\n",
      "         [ 0.0453,  0.4559,  1.0216,  ...,  0.6234,  0.6645, -0.8017],\n",
      "         [ 0.0492,  0.4458,  1.0176,  ...,  0.6248,  0.6658, -0.8050],\n",
      "         [ 0.0758,  0.4367,  1.0370,  ...,  0.6241,  0.6837, -0.8001]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.4337,  0.2721, -0.5040,  ..., -1.1350, -0.5361,  1.4405],\n",
      "         [-0.8949,  0.5669, -1.1375,  ..., -0.4614,  0.2211, -0.4145],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0643, -0.0778,  0.1658,  ..., -0.1914, -0.6527, -0.4436],\n",
      "         [-0.2509,  0.4247, -0.4074,  ..., -1.2900, -0.3426,  1.0731],\n",
      "         [-0.8894,  1.4105, -0.7471,  ..., -0.3513,  0.4534, -0.5438],\n",
      "         ...,\n",
      "         [ 1.4327,  1.2000, -0.3862,  ..., -1.2399,  0.7810,  0.5462],\n",
      "         [ 1.2531,  1.0853, -0.4671,  ..., -1.0462,  0.5669,  0.6598],\n",
      "         [ 0.9748,  1.2373, -0.6092,  ..., -0.8093,  0.5262,  0.8362]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3267, -0.3928, -0.0137,  ..., -0.2414, -0.4180, -0.2582],\n",
      "         [-0.4102,  0.4152, -0.4556,  ..., -0.8502, -0.2704,  0.6039],\n",
      "         [-1.1590,  0.7843, -1.0981,  ...,  0.0662,  0.7241, -0.5035],\n",
      "         ...,\n",
      "         [ 1.1601,  0.8525, -0.3123,  ..., -1.4165,  0.6687,  0.1651],\n",
      "         [ 0.8491,  0.6038, -0.3421,  ..., -0.6200,  0.6129,  0.1642],\n",
      "         [ 0.4990,  0.6100, -0.4459,  ..., -0.3776,  0.7418,  0.3781]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.3708, -0.1972,  0.2893,  ...,  0.3509, -0.0389, -0.3298],\n",
      "         [-1.5028,  0.5006, -0.0614,  ..., -0.1549, -0.0177, -0.1271],\n",
      "         [-2.5001,  0.9087, -0.4509,  ...,  0.6165,  0.7138, -0.8088],\n",
      "         ...,\n",
      "         [ 0.1997,  0.4329,  0.3707,  ..., -0.6438,  0.3042,  0.1678],\n",
      "         [-0.1951,  0.1609,  0.2666,  ..., -0.0077,  0.2602, -0.0468],\n",
      "         [-0.6015,  0.1471,  0.0931,  ...,  0.2075,  0.3229,  0.1152]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.6642, -0.7302,  0.4001,  ...,  0.0586, -0.2046, -0.4478],\n",
      "         [-1.7206,  0.0435, -0.2346,  ..., -0.1075,  0.0210,  0.0362],\n",
      "         [-2.6950,  0.2294,  0.0280,  ...,  0.6186,  0.1103, -1.1106],\n",
      "         ...,\n",
      "         [-0.4136,  0.0185,  0.2047,  ..., -0.5574,  0.1071,  0.3346],\n",
      "         [-0.7407, -0.0256,  0.0897,  ..., -0.1773,  0.0230,  0.2188],\n",
      "         [-1.0269, -0.0320,  0.0044,  ...,  0.1065,  0.0437,  0.3724]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.8353,  0.1194,  0.5680,  ..., -0.5423, -0.5975, -0.8857],\n",
      "         [-1.8708, -0.3079,  0.1320,  ..., -0.7706, -0.0835, -0.1752],\n",
      "         [-2.1995,  0.4351,  0.4453,  ...,  0.3514,  0.1933, -1.2897],\n",
      "         ...,\n",
      "         [-1.1937, -0.0918,  0.5782,  ..., -0.5756,  0.2682,  0.4193],\n",
      "         [-1.5575,  0.1127,  0.3605,  ..., -0.2192,  0.3706,  0.2273],\n",
      "         [-2.0231,  0.1409,  0.4340,  ...,  0.1146,  0.1580,  0.4227]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.7627e+00, -5.8204e-01,  1.5142e+00,  ..., -6.9744e-04,\n",
      "          -5.1442e-01, -4.0634e-02],\n",
      "         [-2.7236e+00, -9.8663e-01,  1.0186e+00,  ..., -4.5583e-02,\n",
      "           1.9585e-01, -3.1873e-02],\n",
      "         [-3.2358e+00, -5.4617e-01,  1.1861e+00,  ...,  4.9684e-01,\n",
      "           1.1551e-01, -6.0290e-01],\n",
      "         ...,\n",
      "         [-1.9895e+00, -5.0930e-01,  1.1717e+00,  ..., -2.6888e-01,\n",
      "           9.3422e-02,  7.7959e-01],\n",
      "         [-2.4200e+00, -6.1018e-01,  1.2640e+00,  ..., -4.8455e-02,\n",
      "           3.3023e-01,  7.7204e-01],\n",
      "         [-2.8292e+00, -4.8350e-01,  1.3768e+00,  ...,  2.2712e-01,\n",
      "           1.6089e-01,  7.8273e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.3212, -0.5839,  0.4041,  ..., -1.3651,  0.2144,  0.4117],\n",
      "         [-2.3292, -1.0109,  0.2089,  ..., -1.3611,  0.2950,  0.5591],\n",
      "         [-2.2077, -1.0552,  0.4433,  ..., -1.1071,  0.3982,  0.4017],\n",
      "         ...,\n",
      "         [-2.1221, -0.5621,  0.2628,  ..., -1.4792,  0.3801,  0.6717],\n",
      "         [-2.2403, -0.6888,  0.2690,  ..., -1.4379,  0.4380,  0.6789],\n",
      "         [-2.3386, -0.6901,  0.3478,  ..., -1.3609,  0.3930,  0.6534]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5483,  1.2147,  1.0509,  ..., -0.3951, -0.6166,  0.7417],\n",
      "         [-0.5729,  1.1323,  1.0258,  ..., -0.3699, -0.5655,  0.8347],\n",
      "         [-0.5270,  1.1465,  1.1043,  ..., -0.3313, -0.5075,  0.7455],\n",
      "         ...,\n",
      "         [-0.4731,  1.2590,  1.0087,  ..., -0.4166, -0.5837,  0.7837],\n",
      "         [-0.4924,  1.2279,  1.0128,  ..., -0.4028, -0.5785,  0.8056],\n",
      "         [-0.5163,  1.2275,  1.0392,  ..., -0.3782, -0.5881,  0.7973]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.3308, -0.5319, -0.1496,  ..., -0.6528, -0.7184,  0.7509],\n",
      "         [ 1.5590,  1.0236, -0.3210,  ...,  1.6943,  0.1022, -0.5630],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0546, -0.1909,  0.0200,  ...,  0.0535, -0.8629, -0.3870],\n",
      "         [-0.2551, -0.2996,  0.0925,  ..., -0.2415, -0.1851,  0.5067],\n",
      "         [ 1.5043,  0.2569, -0.4857,  ...,  1.5584, -0.0247, -1.1996],\n",
      "         ...,\n",
      "         [ 1.4310,  1.0161, -0.4346,  ..., -0.7528,  0.6804,  0.5742],\n",
      "         [ 1.2078,  1.1490, -0.5767,  ..., -0.6649,  0.5811,  0.6519],\n",
      "         [ 1.0256,  1.5756, -0.6824,  ..., -0.6878,  0.4799,  0.7808]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4040, -0.0320,  0.0050,  ..., -0.1847, -0.7623, -0.3522],\n",
      "         [ 0.5231,  0.1396,  0.3958,  ...,  0.4862, -0.4311,  0.5732],\n",
      "         [ 1.4783, -0.4893, -0.2829,  ...,  1.0550, -0.4821, -1.1147],\n",
      "         ...,\n",
      "         [ 1.5647,  0.7808, -0.6461,  ..., -0.6139,  0.5425,  0.0988],\n",
      "         [ 1.3751,  1.5860, -0.6980,  ..., -0.3965,  0.2752, -0.1123],\n",
      "         [ 1.2249,  1.9535, -0.8627,  ..., -0.4240,  0.2102, -0.0156]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6937, -0.2075, -0.1853,  ..., -0.1492, -1.1802, -0.1005],\n",
      "         [ 0.5758, -0.0065, -0.0729,  ...,  0.0113, -0.6507,  0.3759],\n",
      "         [ 1.6944, -0.5133, -0.3396,  ...,  0.1605, -0.2803, -0.9001],\n",
      "         ...,\n",
      "         [ 1.3996,  0.2051, -0.2536,  ..., -0.5244, -0.0701, -0.1279],\n",
      "         [ 0.9596,  0.7525, -0.3941,  ..., -0.3034, -0.3724, -0.2174],\n",
      "         [ 0.7348,  1.0861, -0.6499,  ..., -0.2930, -0.3474, -0.0791]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.8383, -0.2145, -0.4212,  ...,  0.0036, -0.7910, -0.9608],\n",
      "         [ 0.4659, -0.0470, -0.3133,  ..., -0.1447, -1.1563,  0.0600],\n",
      "         [ 1.5908, -0.1549, -0.6525,  ..., -0.0501, -0.6507, -0.8324],\n",
      "         ...,\n",
      "         [ 0.9839, -0.1083, -0.5071,  ..., -0.2404, -0.3602, -0.6477],\n",
      "         [ 0.6697,  0.1198, -0.6193,  ..., -0.1594, -0.4703, -0.9089],\n",
      "         [ 0.5252,  0.3233, -0.8756,  ..., -0.1931, -0.4308, -0.8138]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.3464,  0.5868, -0.5510,  ..., -0.7722, -0.9155, -1.3417],\n",
      "         [ 1.2426,  0.2834, -0.7525,  ..., -0.4273, -1.2929, -0.3230],\n",
      "         [ 2.1813,  0.5315, -0.6978,  ..., -0.8455, -0.5488, -1.0801],\n",
      "         ...,\n",
      "         [ 1.0012,  0.6109, -0.9826,  ..., -0.3344, -0.4961, -0.5511],\n",
      "         [ 0.9367,  0.8892, -0.9831,  ..., -0.3572, -0.9141, -1.0976],\n",
      "         [ 0.9163,  1.2599, -1.4998,  ..., -0.3655, -0.8444, -1.0532]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.7215,  0.4102, -1.2128,  ..., -0.9531, -2.0744, -0.8138],\n",
      "         [ 2.0316,  0.2069, -0.9563,  ..., -0.8269, -2.5677, -0.4296],\n",
      "         [ 2.5751,  0.6464, -0.9481,  ..., -0.7273, -2.2974, -1.0989],\n",
      "         ...,\n",
      "         [ 1.6791,  0.5989, -1.3542,  ..., -0.4337, -1.8746, -0.2154],\n",
      "         [ 1.5979,  0.6780, -1.2690,  ..., -0.3348, -2.2891, -0.8058],\n",
      "         [ 1.5009,  0.8194, -1.7111,  ..., -0.2289, -2.1138, -0.7034]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.9685, -0.1034, -0.2113,  ..., -0.6813, -1.5431, -0.8071],\n",
      "         [ 1.0043, -0.2154, -0.0858,  ..., -0.4739, -1.5231, -0.4026],\n",
      "         [ 0.9073,  0.0053,  0.0385,  ..., -0.4262, -1.3423, -0.7476],\n",
      "         ...,\n",
      "         [ 1.0580, -0.2531, -0.4050,  ..., -0.6951, -1.4168, -0.1838],\n",
      "         [ 0.9741, -0.1249, -0.2970,  ..., -0.5301, -1.5299, -0.4040],\n",
      "         [ 0.9899, -0.1646, -0.4707,  ..., -0.4794, -1.4310, -0.3540]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1680, -0.8602,  0.0824,  ..., -1.0936, -0.9741, -0.3740],\n",
      "         [ 0.1652, -0.8491,  0.1308,  ..., -1.1016, -0.9273, -0.2022],\n",
      "         [ 0.1485, -0.7300,  0.1496,  ..., -1.0813, -0.8728, -0.2251],\n",
      "         ...,\n",
      "         [ 0.1684, -0.8195,  0.0621,  ..., -1.1333, -0.9940, -0.3383],\n",
      "         [ 0.1035, -0.8036,  0.0752,  ..., -1.0764, -1.0052, -0.3337],\n",
      "         [ 0.0914, -0.8049,  0.0386,  ..., -1.0555, -0.9474, -0.3132]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.4337,  0.2721, -0.5040,  ..., -1.1350, -0.5361,  1.4405],\n",
      "         [ 0.6364,  0.6177, -0.5003,  ..., -0.4119, -1.7444, -1.8347],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0843,  0.1343,  0.0608,  ..., -0.1428, -0.6841, -0.4704],\n",
      "         [-0.6077,  0.6181, -0.7735,  ..., -1.1231, -0.4526,  1.0390],\n",
      "         [-0.2598,  0.5517, -0.8748,  ...,  0.4436, -1.8799, -2.6681],\n",
      "         ...,\n",
      "         [ 0.9259,  1.1711, -0.2629,  ..., -0.8594,  0.4188,  0.4684],\n",
      "         [ 0.8783,  1.0815, -0.4473,  ..., -0.7122,  0.1367,  0.5422],\n",
      "         [ 0.8267,  1.2525, -0.5081,  ..., -0.5873,  0.0557,  0.5868]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1293, -0.1884,  0.2181,  ..., -0.4107, -0.7340, -0.2646],\n",
      "         [-0.4379,  0.7977, -0.5550,  ..., -0.8851, -0.6816,  0.5664],\n",
      "         [-0.5822,  0.3397, -1.2235,  ...,  0.5775, -2.0799, -2.6367],\n",
      "         ...,\n",
      "         [ 0.8649,  0.5883, -0.2877,  ..., -1.2109,  0.1884,  0.1777],\n",
      "         [ 0.8609,  0.6275, -0.4594,  ..., -1.0875, -0.0062,  0.1751],\n",
      "         [ 1.0337,  0.9318, -0.2826,  ..., -0.5917, -0.1601, -0.0684]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.4203, -0.0062,  0.2925,  ...,  0.3813, -0.1530, -0.3262],\n",
      "         [-1.8008,  0.6361, -0.1320,  ..., -0.0050, -0.0653,  0.1217],\n",
      "         [-2.3988,  0.3017, -0.9879,  ...,  0.9901, -0.8690, -2.6566],\n",
      "         ...,\n",
      "         [-0.3932,  0.2690,  0.4872,  ..., -0.2469,  0.0343,  0.1991],\n",
      "         [-0.4968,  0.3047,  0.3347,  ..., -0.1775, -0.1820,  0.1705],\n",
      "         [-0.5474,  0.5032,  0.3647,  ...,  0.2491, -0.4317,  0.0229]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.8049e+00, -7.9665e-01,  3.5053e-01,  ...,  2.1674e-01,\n",
      "          -3.5684e-01, -6.6831e-01],\n",
      "         [-2.0322e+00,  1.0045e-01, -1.7892e-01,  ..., -6.8342e-02,\n",
      "          -1.2742e-01,  5.1603e-02],\n",
      "         [-2.7352e+00, -5.0004e-01, -8.4359e-01,  ...,  9.9807e-01,\n",
      "          -9.8572e-01, -2.7366e+00],\n",
      "         ...,\n",
      "         [-7.3349e-01, -2.7969e-01,  1.2896e-01,  ..., -1.4561e-01,\n",
      "          -1.6752e-01, -2.4053e-02],\n",
      "         [-9.2739e-01, -3.0493e-01,  1.4159e-02,  ..., -1.5230e-01,\n",
      "          -3.2721e-01,  2.5456e-02],\n",
      "         [-1.2233e+00, -3.5757e-01,  9.7491e-02,  ...,  2.7360e-01,\n",
      "          -6.0264e-01, -2.1541e-03]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.9391,  0.0970,  0.3429,  ..., -0.8083, -0.1934, -1.0974],\n",
      "         [-1.8952, -0.0337,  0.0152,  ..., -0.8792,  0.0309, -0.1238],\n",
      "         [-2.3370,  0.1482, -0.4237,  ...,  0.3189, -0.6808, -2.3889],\n",
      "         ...,\n",
      "         [-1.3697, -0.2076,  0.5471,  ..., -0.6414,  0.2783, -0.2573],\n",
      "         [-1.5895, -0.1045,  0.4322,  ..., -0.6619,  0.1967, -0.2231],\n",
      "         [-1.7921,  0.0986,  0.3694,  ..., -0.4634, -0.0424, -0.2319]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-3.2867, -0.7028,  1.4213,  ...,  0.3589, -0.1381, -0.2979],\n",
      "         [-2.9461, -0.9745,  1.2002,  ...,  0.1196,  0.0955,  0.0813],\n",
      "         [-3.5781, -0.5777,  1.2202,  ...,  0.7177, -0.6307, -1.0375],\n",
      "         ...,\n",
      "         [-2.7687, -0.8211,  1.4295,  ...,  0.0817, -0.0236,  0.1032],\n",
      "         [-2.8802, -0.8308,  1.4267,  ...,  0.1345, -0.0686,  0.1609],\n",
      "         [-3.0423, -0.7691,  1.4360,  ...,  0.2622, -0.2333,  0.2176]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.1393, -0.9706,  0.1934,  ..., -1.4897,  0.3420,  0.2186],\n",
      "         [-2.1537, -1.1873,  0.0708,  ..., -1.5522,  0.3292,  0.3830],\n",
      "         [-2.0910, -1.2205,  0.1524,  ..., -1.2985,  0.2168,  0.0813],\n",
      "         ...,\n",
      "         [-2.0281, -1.0793,  0.2140,  ..., -1.5777,  0.4651,  0.3346],\n",
      "         [-2.0571, -1.0878,  0.2192,  ..., -1.5701,  0.4495,  0.3648],\n",
      "         [-2.0591, -1.1388,  0.2024,  ..., -1.5548,  0.3985,  0.3850]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5293,  1.1748,  1.0193,  ..., -0.5296, -0.4338,  0.6932],\n",
      "         [-0.5728,  1.1358,  1.0065,  ..., -0.5350, -0.4304,  0.7721],\n",
      "         [-0.5204,  1.1259,  1.0449,  ..., -0.4647, -0.4113,  0.6848],\n",
      "         ...,\n",
      "         [-0.4983,  1.1641,  1.0350,  ..., -0.5532, -0.4215,  0.7021],\n",
      "         [-0.5066,  1.1642,  1.0338,  ..., -0.5465, -0.4299,  0.7088],\n",
      "         [-0.5116,  1.1565,  1.0329,  ..., -0.5338, -0.4431,  0.7083]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.4900,  0.3118,  0.3831,  ..., -0.4010, -0.0392, -0.4723],\n",
      "         [-0.5480,  0.2005,  1.5369,  ..., -0.5107,  0.3743,  2.7617],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1911, -0.2237,  0.2307,  ..., -0.0601, -0.6685, -0.4605],\n",
      "         [-0.1619, -0.2118,  0.3446,  ..., -0.1231,  0.1826, -0.7716],\n",
      "         [-0.6164, -0.2376,  1.1077,  ..., -0.6143,  0.3719,  3.2652],\n",
      "         ...,\n",
      "         [ 1.4984,  1.2217, -0.4533,  ..., -1.0484,  0.5454,  0.4122],\n",
      "         [ 1.3066,  1.2640, -0.4486,  ..., -0.8920,  0.3547,  0.3977],\n",
      "         [ 1.1728,  1.4010, -0.5979,  ..., -0.7829,  0.2251,  0.4782]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3308, -0.4721,  0.1325,  ..., -0.1090, -0.3502, -0.1683],\n",
      "         [ 0.6424, -0.7348,  0.4642,  ..., -0.4063,  0.1992, -1.4597],\n",
      "         [ 0.1552, -0.5079,  0.2462,  ..., -0.5838,  0.5838,  2.7937],\n",
      "         ...,\n",
      "         [ 1.2291,  1.0166, -0.3750,  ..., -0.8281,  0.6272, -0.0717],\n",
      "         [ 1.1771,  1.0516, -0.3970,  ..., -0.7018,  0.5046, -0.0710],\n",
      "         [ 0.8425,  0.8018, -0.6093,  ..., -0.5361,  0.5889,  0.0111]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.4238, -0.1880,  0.1839,  ...,  0.4954,  0.0746, -0.2616],\n",
      "         [-0.2374, -0.3452,  0.3776,  ..., -0.6766,  0.2734, -0.8447],\n",
      "         [-1.1371, -0.6536,  0.6676,  ..., -0.1392,  0.7760,  2.3693],\n",
      "         ...,\n",
      "         [ 0.3167,  0.5067,  0.3247,  ..., -0.3592,  0.3861, -0.0730],\n",
      "         [ 0.1633,  0.5587,  0.2988,  ..., -0.2598,  0.2341, -0.1121],\n",
      "         [-0.3251,  0.4147,  0.2746,  ..., -0.0838,  0.3421, -0.0669]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.8226, -0.6930,  0.3018,  ...,  0.2021, -0.1553, -0.4215],\n",
      "         [-0.8095,  0.2406,  0.2139,  ..., -1.0647, -0.1440, -0.7554],\n",
      "         [-1.4575, -0.2011,  0.3141,  ..., -0.2859,  0.5650,  2.0001],\n",
      "         ...,\n",
      "         [-0.4119,  0.0083,  0.0242,  ..., -0.2263,  0.1004, -0.0213],\n",
      "         [-0.6197, -0.0234, -0.1272,  ..., -0.1735, -0.0065, -0.1106],\n",
      "         [-0.9694, -0.1350, -0.0158,  ..., -0.0359,  0.0618, -0.0371]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.1155,  0.6968,  0.4110,  ..., -0.1993, -0.1761, -1.0405],\n",
      "         [-1.3455, -0.2917,  0.5556,  ..., -1.1200, -0.1190, -1.8756],\n",
      "         [-2.0937, -0.9613, -0.2386,  ..., -0.4528,  0.4177,  1.2110],\n",
      "         ...,\n",
      "         [-1.4349,  0.3975,  0.6543,  ..., -0.2715,  0.3058, -0.1135],\n",
      "         [-1.6626,  0.3632,  0.4804,  ..., -0.1868,  0.1827, -0.1589],\n",
      "         [-1.9352,  0.5337,  0.2638,  ..., -0.2666,  0.2825, -0.0295]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-3.1351e+00, -2.9400e-01,  1.3750e+00,  ...,  2.9691e-01,\n",
      "          -1.5152e-01, -1.2350e-01],\n",
      "         [-2.0812e+00, -9.2108e-01,  1.2520e+00,  ..., -2.9517e-01,\n",
      "          -1.0534e-01, -5.6207e-01],\n",
      "         [-2.6231e+00, -1.0707e+00,  5.9683e-01,  ...,  3.8457e-01,\n",
      "           2.3276e-01,  1.2514e+00],\n",
      "         ...,\n",
      "         [-2.4933e+00, -2.8077e-01,  1.3834e+00,  ..., -1.4555e-01,\n",
      "          -1.7572e-04,  4.0669e-01],\n",
      "         [-2.6949e+00, -2.9026e-01,  1.3478e+00,  ..., -5.0102e-02,\n",
      "          -1.5319e-01,  3.3457e-01],\n",
      "         [-2.9649e+00, -3.4592e-01,  1.4010e+00,  ...,  1.5415e-02,\n",
      "          -4.1530e-02,  5.3975e-01]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.2859, -0.6198,  0.3037,  ..., -1.3261,  0.2188,  0.4233],\n",
      "         [-2.2341, -1.0467,  0.1829,  ..., -1.3621,  0.3379,  0.3611],\n",
      "         [-2.3355, -1.1299,  0.1333,  ..., -1.1713,  0.3174,  0.7606],\n",
      "         ...,\n",
      "         [-2.1461, -0.6057,  0.3024,  ..., -1.4368,  0.3321,  0.5539],\n",
      "         [-2.2026, -0.6567,  0.3021,  ..., -1.4118,  0.2776,  0.5460],\n",
      "         [-2.2636, -0.7800,  0.2936,  ..., -1.4152,  0.2824,  0.6306]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5977,  1.1991,  1.0024,  ..., -0.4331, -0.5780,  0.6920],\n",
      "         [-0.5712,  1.1007,  0.9880,  ..., -0.4047, -0.5556,  0.6973],\n",
      "         [-0.6113,  1.0694,  0.9733,  ..., -0.3435, -0.5594,  0.7848],\n",
      "         ...,\n",
      "         [-0.5616,  1.2115,  0.9965,  ..., -0.4545, -0.5684,  0.7080],\n",
      "         [-0.5769,  1.1989,  0.9983,  ..., -0.4482, -0.5835,  0.7098],\n",
      "         [-0.5920,  1.1763,  0.9974,  ..., -0.4535, -0.5791,  0.7300]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n",
      "(tensor([[[-0.3488, -0.0769,  0.2981,  ...,  0.1186, -0.8865, -0.2565],\n",
      "         [-0.4003, -0.2316,  1.5100,  ..., -0.5603,  0.2797,  0.2265],\n",
      "         [ 0.3641, -0.8787,  0.4554,  ..., -0.5715, -1.9953,  0.6929],\n",
      "         ...,\n",
      "         [ 1.2903,  1.2293, -0.6751,  ..., -0.7249,  0.5980,  0.8214],\n",
      "         [ 1.1868,  1.0863, -0.7637,  ..., -0.6505,  0.4221,  0.9262],\n",
      "         [ 1.0921,  1.3333, -0.9059,  ..., -0.5595,  0.3507,  1.0133]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0540,  0.1375, -0.1544,  ..., -0.1437, -0.7549, -0.4897],\n",
      "         [-0.2230, -0.1830,  0.9859,  ..., -0.8867,  0.4908,  0.0370],\n",
      "         [ 0.2736,  0.0384,  0.2299,  ..., -0.6656, -2.4456,  0.5244],\n",
      "         ...,\n",
      "         [ 0.6830,  1.2796, -0.4922,  ..., -0.7995,  0.4068,  0.8184],\n",
      "         [ 0.6020,  1.2253, -0.5687,  ..., -0.6623,  0.1367,  0.9416],\n",
      "         [ 0.5761,  1.3761, -0.7876,  ..., -0.6808,  0.0295,  1.0160]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2081,  0.1379, -0.2380,  ..., -0.0594, -0.5133, -0.3735],\n",
      "         [ 0.5744,  0.0829,  0.6989,  ..., -0.7382, -0.1673, -0.1152],\n",
      "         [ 0.6549, -0.1759,  0.0664,  ...,  0.0693, -2.0841,  0.3164],\n",
      "         ...,\n",
      "         [ 0.6454,  1.0948, -0.5251,  ..., -0.1540,  0.3313,  0.6064],\n",
      "         [ 0.7241,  1.1319, -0.6859,  ..., -0.2281,  0.0394,  0.5574],\n",
      "         [ 0.6106,  1.0506, -0.7974,  ..., -0.2796, -0.1295,  0.7500]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4190, -0.2007, -0.2430,  ...,  0.1886, -0.3780, -0.2240],\n",
      "         [ 0.2745, -0.3263,  0.1451,  ..., -0.7688, -0.0427,  0.0845],\n",
      "         [ 0.5097, -0.3862, -0.4903,  ..., -0.1045, -1.5444,  0.1176],\n",
      "         ...,\n",
      "         [ 0.5438,  0.1970, -0.1911,  ...,  0.0750,  0.3106,  0.3979],\n",
      "         [ 0.5953,  0.3291, -0.4421,  ...,  0.0505,  0.0602,  0.4037],\n",
      "         [ 0.6345,  0.3022, -0.7392,  ...,  0.0609, -0.1115,  0.5774]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1683, -0.5238, -0.2078,  ...,  0.7793,  0.2616, -0.1792],\n",
      "         [ 0.4230,  0.1740,  0.0939,  ..., -0.0800, -0.1989,  0.3881],\n",
      "         [ 1.0010, -0.6292, -0.5155,  ...,  0.3818, -0.3124,  0.5571],\n",
      "         ...,\n",
      "         [ 0.3368, -0.2473, -0.3328,  ...,  0.2493,  0.7254,  0.2501],\n",
      "         [ 0.3399, -0.1636, -0.6644,  ...,  0.3770,  0.6294,  0.1958],\n",
      "         [ 0.3918,  0.0747, -1.0421,  ...,  0.2196,  0.4725,  0.2188]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7040,  0.1847,  0.5936,  ...,  0.8777,  0.5303,  0.0303],\n",
      "         [ 1.0058,  0.5954,  0.2435,  ...,  0.0387,  0.3577, -0.1671],\n",
      "         [ 1.8232, -0.4417,  0.1027,  ...,  0.6875,  0.1716,  0.7980],\n",
      "         ...,\n",
      "         [ 0.7282,  0.3413,  0.1052,  ...,  0.1625,  1.0563,  0.1694],\n",
      "         [ 0.7407,  0.3934, -0.3530,  ...,  0.6277,  1.0094, -0.0236],\n",
      "         [ 0.6833,  0.6017, -0.8701,  ...,  0.3843,  0.9294,  0.0746]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2844,  0.3318,  0.6607,  ...,  1.0814,  0.9518, -0.0101],\n",
      "         [ 0.6712,  0.6828, -0.0430,  ...,  0.6339,  1.1149, -0.2355],\n",
      "         [ 0.6035, -0.0908,  0.1766,  ...,  1.2673,  1.2798,  0.5999],\n",
      "         ...,\n",
      "         [ 0.4115, -0.0095,  0.4521,  ...,  0.8965,  1.6889,  0.0447],\n",
      "         [ 0.3735,  0.1403,  0.0699,  ...,  1.2263,  1.6849, -0.1329],\n",
      "         [ 0.3681,  0.3658, -0.3641,  ...,  1.0131,  1.5757, -0.0810]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.4631,  0.9441,  1.1951,  ...,  0.7894,  0.3363, -0.5022],\n",
      "         [ 0.1014,  0.5022,  0.4944,  ...,  0.4237,  0.6134, -0.4572],\n",
      "         [ 0.1263,  0.0746,  0.6878,  ...,  1.2097,  0.9520,  0.3527],\n",
      "         ...,\n",
      "         [-0.1730,  0.8634,  0.9664,  ...,  0.5837,  0.6592, -0.4174],\n",
      "         [-0.2201,  0.8662,  0.8707,  ...,  0.7432,  0.7237, -0.5813],\n",
      "         [-0.1951,  0.9811,  0.6693,  ...,  0.6132,  0.6191, -0.4922]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0076,  0.3657,  0.8632,  ...,  0.6431,  0.5693, -0.5788],\n",
      "         [ 0.1542,  0.1023,  0.9039,  ...,  0.5515,  0.6754, -0.6940],\n",
      "         [ 0.2660, -0.1711,  0.9733,  ...,  0.8531,  0.7564, -0.4301],\n",
      "         ...,\n",
      "         [ 0.0591,  0.3300,  0.9186,  ...,  0.5665,  0.5733, -0.5899],\n",
      "         [ 0.0429,  0.3200,  0.9052,  ...,  0.6101,  0.5903, -0.6100],\n",
      "         [ 0.0506,  0.3452,  0.8334,  ...,  0.5812,  0.5854, -0.5858]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>))\n"
     ]
    }
   ],
   "source": [
    "dsm_emb = []\n",
    "\n",
    "for idx in range(len(dsm_samp)):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text=dsm_samp.text[idx],  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        max_length = 64,  # maximum length of a sentence\n",
    "        pad_to_max_length=True,  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "    input_ids = torch.tensor(encoded['input_ids']).to(training_config.device)\n",
    "    attn_mask = torch.tensor(encoded['attention_mask']).to(training_config.device)\n",
    "    token_type_ids = torch.tensor(encoded['token_type_ids']).to(training_config.device)\n",
    "    \n",
    "    outputs = model(input_ids, attn_mask, token_type_ids)\n",
    "    print(outputs[1])\n",
    "   # hidden_states = outputs[2]\n",
    "   # embedding_output = hidden_states[0]\n",
    "   # dsm_emb.append(list(embedding_output[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb5d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "81cf3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tester = BertClsTester(training_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "da91ceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소요 시간: 5.086172103881836\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "y_pred, y_true = bert_tester.get_label(test_dataloader, 0)\n",
    "print(f'소요 시간: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "92049810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import F1Score\n",
    "from torchmetrics.classification import MulticlassPrecision\n",
    "from torchmetrics.classification import MulticlassRecall\n",
    "from torchmetrics.classification import MulticlassSpecificity\n",
    "\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=len(X_test.label.unique()))\n",
    "precision = MulticlassPrecision(num_classes=len(X_test.label.unique()))\n",
    "recall = MulticlassRecall(num_classes=len(X_test.label.unique()))\n",
    "specificity = MulticlassSpecificity(num_classes=len(X_test.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "7cc2adcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9923352003097534\n",
      "recall: 0.9836284518241882\n",
      "specificity: 0.9985994100570679\n",
      "f1-score: 0.991443932056427\n"
     ]
    }
   ],
   "source": [
    "print(f'precision: {precision(torch.Tensor(y_pred), torch.Tensor(y_true))}')\n",
    "print(f'recall: {recall(torch.Tensor(y_pred), torch.Tensor(y_true))}')\n",
    "print(f'specificity: {specificity(torch.Tensor(y_pred), torch.Tensor(y_true))}')\n",
    "print(f'f1-score: {f1(torch.Tensor(y_pred), torch.Tensor(y_true))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "0adb3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = \"I can't sleep because I have too much work to do\" # X_test.text[0]\n",
    "test_data = test_processor.convert_sentence(test_sent)\n",
    "test_sampler = test_processor.shuffle_data(test_data, 'test')\n",
    "test_loader = test_processor.load_data(test_data, test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "04076293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소요 시간: 0.012841463088989258\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "y_pred2, y_true2 = bert_tester.get_label(test_loader, 1)\n",
    "print(f'소요 시간: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_file = BertDataset(user_conv)\n",
    "conv_dataset = test_processor.convert_data(conv_file)\n",
    "conv_sampler = test_processor.shuffle_data(conv_dataset, 'test')\n",
    "conv_dataloader = test_processor.load_data(conv_dataset, conv_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd381b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = bert_tester.get_label(conv_dataloader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "d8dc1020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7979</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>408</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1433</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3942</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1     2     3    4    5     6    7     8\n",
       "0  7979    3     1     2    1    0     5    0     9\n",
       "1     4  408     0     1    0    0     2    1     0\n",
       "2     3    3  1017     0    0    0     1    0     2\n",
       "3    12    0     1  1433    0    0     0    0     0\n",
       "4     3    0     1     2  136    0     0    0     1\n",
       "5     2    0     2     3    0  339     1    0     1\n",
       "6    48    0     1     5    0    0  3942    0     4\n",
       "7     2    0     0     1    0    0     1  486     0\n",
       "8    31    1     1     2    0    0     4    2  3959"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score , recall_score , confusion_matrix, f1_score, classification_report\n",
    "\n",
    "confusion_mt = pd.DataFrame(confusion_matrix(y_true, y_pred))\n",
    "confusion_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "bba4e796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19869"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "30869636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize \n",
    "\n",
    "y_true = label_binarize(y_true, classes=list(range(len(label))))\n",
    "y_pred = label_binarize(y_pred, classes=list(range(len(label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "2717c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/lamda_base/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1018: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "n_classes = 10\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict() \n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:,i], y_pred[:,i]) \n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "d2bd923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "val = 0\n",
    "\n",
    "for idx, i in enumerate(range(n_classes)):\n",
    "    if idx == 0 or idx == 8: \n",
    "        plt.subplot(131+val) \n",
    "        plt.plot(fpr[i], tpr[i], label = f'ROC curve(area = {round(roc_auc[i], 2)}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate') \n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'Class {idx}')\n",
    "        plt.legend(loc='lower right')\n",
    "        val += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc695c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
